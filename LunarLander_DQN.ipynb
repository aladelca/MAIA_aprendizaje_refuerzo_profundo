{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LunarLander con Stable-Baselines3 (DQN)\n",
        "\n",
        "Este notebook entrena y evalúa un agente DQN de Stable-Baselines3 para el ambiente discreto `LunarLander-v3` de Gymnasium.\n",
        "\n",
        "- Método: DQN de SB3 con red MLP, experiencia repetida, target updates y exploración epsilon.\n",
        "- Ambiente: `LunarLander-v3` (acciones: 0=No-op, 1=Motor Izq., 2=Motor Principal, 3=Motor Der.).\n",
        "- Objetivo: Aterrizar en la zona plana entre banderas (centro), robusto a variaciones del terreno.\n",
        "- Hardware: CPU.\n",
        "- Evita logs con `verbose = False`\n",
        "\n",
        "Incluye entrenamiento con `EvalCallback` y checkpoints, evaluación con y sin vientos, para mayor robustez, y gráficos de aprendizaje y grabación de video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalación de dependencias. Descomentar:\n",
        "# %pip install -q gymnasium[box2d]==0.29.1 stable-baselines3==2.3.2 shimmy==1.3.0 swig==4.2.1 tensorboard\n",
        "# %pip install -q matplotlib==3.8.4 tqdm==4.66.4 imageio==2.34.1 imageio-ffmpeg==0.4.9\n",
        "\n",
        "import sys, platform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print('Usando Stable-Baselines3 DQN')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir el ambiente\n",
        "def make_env(seed: int = SEED, render_mode=None, enable_wind: bool = False, **kwargs):\n",
        "    env = gym.make('LunarLander-v3', render_mode=render_mode, enable_wind=enable_wind, **kwargs)\n",
        "    env = Monitor(env)\n",
        "    env.reset(seed=seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    return env\n",
        "\n",
        "# Verificar espacios\n",
        "_env = make_env()\n",
        "print('obs_shape', _env.observation_space.shape, 'n_actions', _env.action_space.n)\n",
        "_env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hiperparámetros\n",
        "TIMESTEPS = 300_000  # Ajustado según el tiempo disponible\n",
        "POLICY_KWARGS = dict(net_arch=[256, 256])\n",
        "LEARNING_RATE = 1e-3\n",
        "GAMMA = 0.99\n",
        "BUFFER_SIZE = 100_000\n",
        "LEARNING_STARTS = 5_000\n",
        "TRAIN_FREQ = 4\n",
        "TARGET_UPDATE_INTERVAL = 1_000\n",
        "TAU = 1.0  \n",
        "EXPLORATION_FRACTION = 0.5\n",
        "EXPLORATION_FINAL_EPS = 0.05\n",
        "BATCH_SIZE = 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utilidades SB3: callbacks de evaluación y checkpoints\n",
        "log_dir = 'logs'\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "eval_env = make_env(seed=SEED + 100, enable_wind=False)\n",
        "\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=os.path.join(log_dir, 'best_model'),\n",
        "    log_path=log_dir,\n",
        "    eval_freq=10_000,\n",
        "    deterministic=True,\n",
        "    render=False,\n",
        "    n_eval_episodes=10,\n",
        ")\n",
        "\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=50_000,\n",
        "    save_path=os.path.join(log_dir, 'checkpoints'),\n",
        "    name_prefix='dqn_lunar',\n",
        "    save_replay_buffer=True,\n",
        "    save_vecnormalize=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construir y entrenar el modelo DQN de SB3\n",
        "train_env = make_env(seed=SEED, enable_wind=False)\n",
        "\n",
        "model = DQN(\n",
        "    policy='MlpPolicy',\n",
        "    env=train_env,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    buffer_size=BUFFER_SIZE,\n",
        "    learning_starts=LEARNING_STARTS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    gamma=GAMMA,\n",
        "    train_freq=TRAIN_FREQ,\n",
        "    target_update_interval=TARGET_UPDATE_INTERVAL,\n",
        "    tau=TAU,\n",
        "    exploration_fraction=EXPLORATION_FRACTION,\n",
        "    exploration_final_eps=EXPLORATION_FINAL_EPS,\n",
        "    policy_kwargs=POLICY_KWARGS,\n",
        "    tensorboard_log=log_dir,\n",
        "    verbose=0,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "# Entrenar con callbacks\n",
        "model.learn(total_timesteps=TIMESTEPS, callback=[eval_callback, checkpoint_callback], progress_bar=False)\n",
        "\n",
        "# Guardar el modelo final\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model_path = 'models/dqn_lunarlander_sb3'\n",
        "model.save(model_path)\n",
        "print('Modelo guardado en', model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construir y entrenar el modelo DQN de SB3\n",
        "train_env = make_env(seed=SEED, enable_wind=False)\n",
        "\n",
        "model = DQN(\n",
        "    policy='MlpPolicy',\n",
        "    env=train_env,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    buffer_size=BUFFER_SIZE,\n",
        "    learning_starts=LEARNING_STARTS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    gamma=GAMMA,\n",
        "    train_freq=TRAIN_FREQ,\n",
        "    target_update_interval=TARGET_UPDATE_INTERVAL,\n",
        "    tau=TAU,\n",
        "    exploration_fraction=EXPLORATION_FRACTION,\n",
        "    exploration_final_eps=EXPLORATION_FINAL_EPS,\n",
        "    policy_kwargs=POLICY_KWARGS,\n",
        "    tensorboard_log=log_dir,\n",
        "    verbose=1,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "# Entrenar con callbacks\n",
        "model.learn(total_timesteps=TIMESTEPS, callback=[eval_callback, checkpoint_callback], progress_bar=False)\n",
        "\n",
        "# Guardar el modelo final\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model_path = 'models/dqn_lunarlander_sb3'\n",
        "model.save(model_path)\n",
        "print('Modelo guardado en', model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el mejor modelo (si existe) o usar el modelo recién entrenado\n",
        "best_model_dir = os.path.join(log_dir, 'best_model')\n",
        "best_model_path = os.path.join(best_model_dir, 'best_model.zip')\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    print('Cargando mejor modelo desde', best_model_path)\n",
        "    best_model = DQN.load(best_model_path)\n",
        "else:\n",
        "    print('No hay mejor modelo aún; se usará el modelo actual')\n",
        "    best_model = model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TensorBoard: puedes lanzar un servidor para ver curvas (en Colab se integra)\n",
        "# En local: desde terminal, corre: tensorboard --logdir logs | cat\n",
        "print('Logs en:', log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluación con SB3\n",
        "\n",
        "# Evaluación sin viento\n",
        "eval_env_no_wind = make_env(seed=SEED + 1000, enable_wind=False)\n",
        "mean_reward, std_reward = evaluate_policy(best_model, eval_env_no_wind, n_eval_episodes=100, deterministic=True)\n",
        "print(f\"Evaluación sin viento (100 eps): media={mean_reward:.2f} ± {std_reward:.2f}\")\n",
        "\n",
        "eval_env_no_wind.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabación de video opcional (SB3)\n",
        "# Si estás en Colab, el video se guardará en /content. Aquí se guarda en ./videos\n",
        "\n",
        "import imageio\n",
        "\n",
        "def record_video_sb3(model: DQN, filename: str = 'videos/lunar_dqn_episode.mp4', fps: int = 30, seed: int = SEED+999):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    env = make_env(seed=seed, render_mode='rgb_array')\n",
        "    frames = []\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    for _ in range(1000):\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminated, truncated, info = env.step(int(action))\n",
        "        if terminated or truncated:\n",
        "            frame = env.render()\n",
        "            frames.append(frame)\n",
        "            break\n",
        "    env.close()\n",
        "    imageio.mimwrite(filename, frames, fps=fps, quality=8)\n",
        "    print('Video guardado en:', filename)\n",
        "\n",
        "# Descomenta para grabar video con el mejor modelo o el actual\n",
        "record_video_sb3(best_model, filename='videos/lunar_dqn_episode.mp4', fps=30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluación con viento activado\n",
        "\n",
        "eval_env_wind = make_env(seed=SEED + 2000, enable_wind=True, wind_power=10.0, turbulence_power=1.5)\n",
        "mean_reward_wind, std_reward_wind = evaluate_policy(best_model, eval_env_wind, n_eval_episodes=100, deterministic=True)\n",
        "print(f\"Evaluación con viento (100 eps): media={mean_reward_wind:.2f} ± {std_reward_wind:.2f}\")\n",
        "\n",
        "eval_env_wind.close()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
