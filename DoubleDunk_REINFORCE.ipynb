{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DoubleDunk - REINFORCE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resumen:\n",
        "\n",
        "- Algoritmo: REINFORCE con baseline (Actor-Cr√≠tico), entrop√≠a y gradient clipping.\n",
        "- Ambiente: `ALE/DoubleDunk-v5`\n",
        "  - Espacio de Acciones Discreto: `18` acciones\n",
        "  - Espacio de Observaciones: `Box(0, 255, (210, 160, 3), uint8)`\n",
        "- Objetivo: Anotar en el juego de baloncesto, defender y superar al oponente.\n",
        "- Puntaje baseline: `-14.0` (resultado t√≠pico)\n",
        "- Hardware: Compatible con Google Colab GPU, detecta autom√°ticamente CUDA/MPS/CPU\n",
        "- Tiempo de entrenamiento estimado: ~6-8 horas (8000 episodios)\n",
        "- Caracter√≠sticas: Sistema de checkpoints, evaluaci√≥n peri√≥dica, generaci√≥n de videos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instalaci√≥n e importaci√≥n de librer√≠as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Todas las librer√≠as han sido instaladas correctamente.\n"
          ]
        }
      ],
      "source": [
        "# Instalaci√≥n de dependencias para Google Colab\n",
        "!pip install gymnasium[atari] ale-py torch torchvision imageio -q\n",
        "!pip install \"gymnasium[atari,accept-rom-license]\" -q\n",
        "!AutoROM --accept-license\n",
        "\n",
        "# Librer√≠as b√°sicas\n",
        "import sys, platform\n",
        "print({'python': sys.version.split()[0], 'platform': platform.platform()})\n",
        "\n",
        "#Limpia los registros generados\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Todas las librer√≠as han sido instaladas correctamente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecci√≥n de Hardware\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üñ•Ô∏è  DETECCI√ìN DE HARDWARE\n",
            "============================================================\n",
            "Sistema: Darwin 24.5.0\n",
            "PyTorch: 2.8.0\n",
            "‚úÖ MPS DETECTADO (Apple Silicon)\n",
            "   üçé Aceleraci√≥n optimizada para Apple Silicon\n",
            "------------------------------------------------------------\n",
            "DISPOSITIVO FINAL: mps\n",
            "============================================================\n",
            "\n",
            "üéØ Sistema configurado para: mps\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# IMPORTACIONES Y DETECCI√ìN DE HARDWARE\n",
        "# ========================================\n",
        "\n",
        "import gymnasium as gym\n",
        "import gymnasium\n",
        "import ale_py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch.optim import Adam\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from collections import deque\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import imageio\n",
        "import warnings\n",
        "import platform\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Registrar entornos ALE\n",
        "gymnasium.register_envs(ale_py)\n",
        "\n",
        "SEED = 123\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "def detect_hardware():\n",
        "    \"\"\"Detecta y configura el hardware disponible (Google Colab compatible)\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üñ•Ô∏è  DETECCI√ìN DE HARDWARE\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Informaci√≥n del sistema\n",
        "    print(f\"Sistema: {platform.system()} {platform.release()}\")\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    \n",
        "    # Verificar CUDA (GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        \n",
        "        print(f\"‚úÖ GPU DETECTADA: {gpu_name}\")\n",
        "        print(f\"   üíæ Memoria GPU: {gpu_memory:.1f} GB\")\n",
        "        print(f\"   üîß CUDA Version: {torch.version.cuda}\")\n",
        "        \n",
        "        # Configuraciones de PyTorch para GPU\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    # Verificar MPS (Apple Silicon)\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device('mps')\n",
        "        print(f\"‚úÖ MPS DETECTADO (Apple Silicon)\")\n",
        "        print(f\"   üçé Aceleraci√≥n optimizada para Apple Silicon\")\n",
        "        \n",
        "    else:\n",
        "        # Fallback a CPU\n",
        "        device = torch.device('cpu')\n",
        "        print(f\"‚ö†Ô∏è  SOLO CPU DISPONIBLE\")\n",
        "        print(f\"   ‚ùå No se detect√≥ GPU - El entrenamiento ser√° m√°s lento\")\n",
        "        print(f\"   üí° En Colab: Runtime > Change runtime type > GPU\")\n",
        "        torch.set_num_threads(4)  # Limitar threads en CPU\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "    print(f\"DISPOSITIVO FINAL: {device}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return device\n",
        "\n",
        "# Detectar hardware\n",
        "device = detect_hardware()\n",
        "\n",
        "# Configuraciones determin√≠sticas\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(f\"\\nüéØ Sistema configurado para: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizaremos el algoritmo REINFORCE con baseline para el entrenamiento\n",
        "\n",
        "## Definici√≥n del Algoritmo REINFORCE\n",
        "\n",
        "REINFORCE (REward Increment = Nonnegative Factor √ó Offset Reinforcement √ó Characteristic Eligibility) es un algoritmo de gradiente de pol√≠tica que optimiza directamente la pol√≠tica para maximizar la recompensa esperada.\n",
        "\n",
        "**Caracter√≠sticas principales:**\n",
        "- **Algoritmo de pol√≠tica**: Optimiza directamente la funci√≥n de pol√≠tica œÄ(a|s)\n",
        "- **Baseline con Critic**: Reduce la varianza usando un estimador de valor V(s)\n",
        "- **Monte Carlo**: Usa retornos completos de episodios para actualizar\n",
        "- **Gradient ascent**: Maximiza la recompensa esperada usando gradientes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wrappers: preprocesamiento y frame stacking\n",
        "class SimpleFrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k: int = 4):\n",
        "        super().__init__(env)\n",
        "        self.k = k\n",
        "        self.frames = deque(maxlen=k)\n",
        "        obs_space = env.observation_space\n",
        "        h, w = obs_space.shape[0], obs_space.shape[1]\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(h, w, k), dtype=np.uint8)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.frames.clear()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(obs)\n",
        "        return self._get_ob(), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.frames.append(obs)\n",
        "        return self._get_ob(), reward, terminated, truncated, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        return np.stack(list(self.frames), axis=-1)\n",
        "\n",
        "\n",
        "def make_env(seed: int = SEED, render_mode=None):\n",
        "    env = gym.make('ALE/DoubleDunk-v5', render_mode=render_mode)\n",
        "    class GrayResizeWrapper(gym.ObservationWrapper):\n",
        "        def __init__(self, env):\n",
        "            super().__init__(env)\n",
        "            h, w = 84, 84\n",
        "            self.observation_space = gym.spaces.Box(low=0, high=255, shape=(h, w), dtype=np.uint8)\n",
        "        def observation(self, obs):\n",
        "            gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "            resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "            return resized\n",
        "    env = GrayResizeWrapper(env)\n",
        "    env = SimpleFrameStack(env, 4)\n",
        "    env.reset(seed=seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    return env\n",
        "\n",
        "# to tensor CHW [0,1]\n",
        "def obs_to_tensor(obs) -> torch.Tensor:\n",
        "    arr = obs if isinstance(obs, np.ndarray) else np.array(obs)\n",
        "    if arr.ndim == 3 and arr.shape[-1] == 4:\n",
        "        arr = np.transpose(arr, (2, 0, 1))\n",
        "    elif arr.ndim == 2:\n",
        "        arr = np.stack([arr]*4, axis=0)\n",
        "    tensor = torch.from_numpy(arr).float() / 255.0\n",
        "    return tensor.unsqueeze(0).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenamiento\n",
        "\n",
        "Configuraci√≥n de hiperpar√°metros y ejecuci√≥n del entrenamiento REINFORCE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo CNN actor-cr√≠tico\n",
        "class AtariActorCritic(nn.Module):\n",
        "    def __init__(self, in_channels: int, n_actions: int):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 512), nn.ReLU(),\n",
        "        )\n",
        "        self.policy_head = nn.Linear(512, n_actions)\n",
        "        self.value_head = nn.Linear(512, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        z = self.features(x)\n",
        "        z = self.flatten(z)\n",
        "        z = self.fc(z)\n",
        "        logits = self.policy_head(z)\n",
        "        value = self.value_head(z).squeeze(-1)\n",
        "        return logits, value\n",
        "\n",
        "    def act(self, x: torch.Tensor):\n",
        "        logits, value = self.forward(x)\n",
        "        dist = Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy = dist.entropy()\n",
        "        return action.item(), log_prob, entropy, value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config(total_episodes=8000, max_steps_per_episode=8000, gamma=0.99, learning_rate=0.0003, entropy_coef=0.01, value_coef=0.5, grad_clip_norm=0.5, checkpoint_dir='checkpoints_doubledunk', checkpoint_every_episodes=50, eval_every_episodes=100, eval_episodes=10)\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    total_episodes: int = 8000\n",
        "    max_steps_per_episode: int = 8000\n",
        "    gamma: float = 0.99\n",
        "    learning_rate: float = 3e-4\n",
        "    entropy_coef: float = 0.01\n",
        "    value_coef: float = 0.5\n",
        "    grad_clip_norm: float = 0.5\n",
        "    checkpoint_dir: str = 'checkpoints_doubledunk'\n",
        "    checkpoint_every_episodes: int = 50\n",
        "    eval_every_episodes: int = 100\n",
        "    eval_episodes: int = 10\n",
        "\n",
        "cfg = Config()\n",
        "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
        "print(cfg)\n",
        "\n",
        "# Utils\n",
        "\n",
        "def compute_returns(rewards: List[float], gamma: float) -> torch.Tensor:\n",
        "    G = 0.0\n",
        "    returns = []\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.append(G)\n",
        "    returns.reverse()\n",
        "    ret = torch.tensor(returns, dtype=torch.float32, device=device)\n",
        "    return ret.view(-1)\n",
        "\n",
        "\n",
        "def save_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer, episode: int, path: str):\n",
        "    torch.save({'episode': episode,\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict()}, path)\n",
        "\n",
        "\n",
        "def load_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer, path: str):\n",
        "    ckpt = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    return ckpt.get('episode', 0)\n",
        "\n",
        "\n",
        "def evaluate(agent: nn.Module, episodes: int = 10, render: bool = False) -> float:\n",
        "    env = make_env(seed=SEED + 999, render_mode='human' if render else None)\n",
        "    agent.eval()\n",
        "    rewards = []\n",
        "    with torch.no_grad():\n",
        "        for ep in range(episodes):\n",
        "            obs, info = env.reset(seed=SEED + 999 + ep)\n",
        "            total_r = 0.0\n",
        "            for t in range(cfg.max_steps_per_episode):\n",
        "                x = obs_to_tensor(obs)\n",
        "                logits, _ = agent(x)\n",
        "                action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
        "                obs, r, terminated, truncated, info = env.step(action)\n",
        "                total_r += float(r)\n",
        "                if terminated or truncated:\n",
        "                    break\n",
        "            rewards.append(total_r)\n",
        "    env.close()\n",
        "    agent.train()\n",
        "    return float(np.mean(rewards))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Gr√°ficos disponibles despu√©s del entrenamiento\n",
            "üí° Los gr√°ficos se generar√°n autom√°ticamente al completar el entrenamiento\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# GR√ÅFICOS DE PROGRESO DEL ENTRENAMIENTO\n",
        "# ========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plot_training_progress():\n",
        "    \"\"\"Genera gr√°ficos del progreso del entrenamiento REINFORCE\"\"\"\n",
        "    \n",
        "    # Verificar si hay datos de entrenamiento\n",
        "    if not hasattr(agent, 'episode_rewards') or len(episode_rewards) == 0:\n",
        "        print(\"‚ö†Ô∏è  No hay datos de entrenamiento para graficar\")\n",
        "        print(\"üí° Ejecuta primero la celda de entrenamiento\")\n",
        "        return\n",
        "    \n",
        "    print(\"üìà GENERANDO GR√ÅFICOS DE PROGRESO DEL ENTRENAMIENTO\")\n",
        "    print(\"=\" * 55)\n",
        "    \n",
        "    # Crear figura con subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('REINFORCE DoubleDunk - Progreso del Entrenamiento', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Recompensas por episodio (raw + media m√≥vil)\n",
        "    ax1 = axes[0, 0]\n",
        "    episodes_range = range(1, len(episode_rewards) + 1)\n",
        "    ax1.plot(episodes_range, episode_rewards, alpha=0.6, color='blue', linewidth=0.8, label='Recompensa por episodio')\n",
        "    \n",
        "    # Media m√≥vil\n",
        "    if len(episode_rewards) > 50:\n",
        "        window = 50\n",
        "        moving_avg = pd.Series(episode_rewards).rolling(window=window, center=True).mean()\n",
        "        ax1.plot(episodes_range, moving_avg, color='red', linewidth=2, label=f'Media m√≥vil ({window})')\n",
        "    \n",
        "    # L√≠nea de baseline (-14.0)\n",
        "    ax1.axhline(y=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline REINFORCE (-14.0)')\n",
        "    ax1.set_xlabel('Episodios')\n",
        "    ax1.set_ylabel('Recompensa')\n",
        "    ax1.set_title('Evoluci√≥n de Recompensas por Episodio')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Recompensas de evaluaci√≥n\n",
        "    ax2 = axes[0, 1]\n",
        "    if len(eval_rewards) > 0:\n",
        "        ax2.plot(eval_episodes_list, eval_rewards, 'o-', color='green', linewidth=2, markersize=6, label='Evaluaci√≥n (10 eps)')\n",
        "        ax2.axhline(y=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline (-14.0)')\n",
        "        ax2.set_xlabel('Episodios')\n",
        "        ax2.set_ylabel('Recompensa Media')\n",
        "        ax2.set_title('Progreso de Evaluaciones Peri√≥dicas')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax2.text(0.5, 0.5, 'No hay datos de evaluaci√≥n\\ndisponibles', \n",
        "                ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
        "        ax2.set_title('Evaluaciones Peri√≥dicas')\n",
        "    \n",
        "    # 3. Histograma de recompensas\n",
        "    ax3 = axes[1, 0]\n",
        "    ax3.hist(episode_rewards, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "    ax3.axvline(x=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline (-14.0)')\n",
        "    ax3.axvline(x=np.mean(episode_rewards), color='red', linestyle='-', linewidth=2, label=f'Media: {np.mean(episode_rewards):.1f}')\n",
        "    ax3.set_xlabel('Recompensa')\n",
        "    ax3.set_ylabel('Frecuencia')\n",
        "    ax3.set_title('Distribuci√≥n de Recompensas')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Estad√≠sticas del entrenamiento\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.axis('off')\n",
        "    \n",
        "    # Calcular estad√≠sticas\n",
        "    total_episodes = len(episode_rewards)\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "    min_reward = np.min(episode_rewards)\n",
        "    max_reward = np.max(episode_rewards)\n",
        "    \n",
        "    # √öltimos 100 episodios\n",
        "    last_100 = episode_rewards[-100:] if len(episode_rewards) >= 100 else episode_rewards\n",
        "    mean_last_100 = np.mean(last_100)\n",
        "    \n",
        "    # Mejora vs baseline\n",
        "    improvement = mean_reward - (-14.0)\n",
        "    improvement_pct = (improvement / abs(-14.0)) * 100\n",
        "    \n",
        "    stats_text = f\"\"\"\n",
        "üìä ESTAD√çSTICAS DEL ENTRENAMIENTO\n",
        "\n",
        "üéØ Episodios totales: {total_episodes:,}\n",
        "üìà Recompensa media: {mean_reward:.2f} ¬± {std_reward:.2f}\n",
        "üèÜ Mejor episodio: {max_reward:.2f}\n",
        "üìâ Peor episodio: {min_reward:.2f}\n",
        "\n",
        "üìä √öltimos 100 episodios: {mean_last_100:.2f}\n",
        "üì∂ Mejora vs baseline: {improvement:+.2f} ({improvement_pct:+.1f}%)\n",
        "\n",
        "üé≤ Baseline REINFORCE: -14.0\n",
        "üî• Estado: {\"‚úÖ Mejorando\" if improvement > 0 else \"‚ö†Ô∏è Por debajo del baseline\"}\n",
        "\"\"\"\n",
        "    \n",
        "    ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=11,\n",
        "             verticalalignment='top', fontfamily='monospace',\n",
        "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Imprimir resumen\n",
        "    print(f\"\\nüìã RESUMEN DEL PROGRESO:\")\n",
        "    print(f\"   üéØ Episodios entrenados: {total_episodes:,}\")\n",
        "    print(f\"   üìä Recompensa promedio: {mean_reward:.2f}\")\n",
        "    print(f\"   üèÜ Mejor resultado: {max_reward:.2f}\")\n",
        "    print(f\"   üìà √öltimos 100 eps: {mean_last_100:.2f}\")\n",
        "    print(f\"   {'‚úÖ' if improvement > 0 else '‚ùå'} Mejora vs baseline: {improvement:+.2f}\")\n",
        "\n",
        "# Ejecutar visualizaci√≥n si hay datos\n",
        "try:\n",
        "    if 'episode_rewards' in locals() and len(episode_rewards) > 10:\n",
        "        plot_training_progress()\n",
        "    else:\n",
        "        print(\"üìä Gr√°ficos disponibles despu√©s del entrenamiento\")\n",
        "        print(\"üí° Los gr√°ficos se generar√°n autom√°ticamente al completar el entrenamiento\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error generando gr√°ficos: {e}\")\n",
        "    print(\"üí° Aseg√∫rate de ejecutar primero la celda de entrenamiento\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando checkpoint desde checkpoints_doubledunk/reinforce_doubledunk.pt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg.max_steps_per_episode):\n\u001b[32m     29\u001b[39m     x = obs_to_tensor(obs)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     action, log_prob, entropy, value = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     obs, reward, terminated, truncated, info = env.step(action)\n\u001b[32m     32\u001b[39m     log_probs.append(log_prob)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mAtariActorCritic.act\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     logits, value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     dist = Categorical(logits=logits)\n\u001b[32m     33\u001b[39m     action = dist.sample()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mAtariActorCritic.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m z = \u001b[38;5;28mself\u001b[39m.features(x)\n\u001b[32m     24\u001b[39m z = \u001b[38;5;28mself\u001b[39m.flatten(z)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.policy_head(z)\n\u001b[32m     27\u001b[39m value = \u001b[38;5;28mself\u001b[39m.value_head(z).squeeze(-\u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Entrenamiento REINFORCE con baseline\n",
        "\n",
        "env = make_env(seed=SEED, render_mode=None)\n",
        "n_actions = env.action_space.n\n",
        "in_channels = 4\n",
        "\n",
        "agent = AtariActorCritic(in_channels=in_channels, n_actions=n_actions).to(device)\n",
        "optimizer = Adam(agent.parameters(), lr=cfg.learning_rate)\n",
        "\n",
        "start_episode = 0\n",
        "ckpt_path = os.path.join(cfg.checkpoint_dir, 'reinforce_doubledunk.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "    print('Cargando checkpoint desde', ckpt_path)\n",
        "    start_episode = load_checkpoint(agent, optimizer, ckpt_path)\n",
        "\n",
        "best_eval = -float('inf')\n",
        "\n",
        "# Listas para almacenar progreso del entrenamiento\n",
        "episode_rewards = []\n",
        "eval_rewards = []\n",
        "eval_episodes_list = []\n",
        "\n",
        "for ep in range(start_episode, cfg.total_episodes):\n",
        "    obs, info = env.reset(seed=SEED + ep)\n",
        "    log_probs, entropies, values, rewards = [], [], [], []\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for t in range(cfg.max_steps_per_episode):\n",
        "        x = obs_to_tensor(obs)\n",
        "        action, log_prob, entropy, value = agent.act(x)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        log_probs.append(log_prob)\n",
        "        entropies.append(entropy)\n",
        "        values.append(value)\n",
        "        rewards.append(float(reward))\n",
        "        total_reward += float(reward)\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    # Almacenar reward del episodio para gr√°ficos\n",
        "    episode_rewards.append(total_reward)\n",
        "\n",
        "    returns = compute_returns(rewards, cfg.gamma)\n",
        "    values_t = torch.stack(values)\n",
        "    log_probs_t = torch.stack(log_probs)\n",
        "    entropies_t = torch.stack(entropies)\n",
        "\n",
        "    # Asegurar que values_t y returns tengan la misma forma\n",
        "    if values_t.dim() > 1:\n",
        "        values_t = values_t.squeeze(-1)\n",
        "    \n",
        "    advantages = returns - values_t\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    # Entropy decay: m√°s exploraci√≥n al inicio\n",
        "    entropy_coef = cfg.entropy_coef * (0.5 + 0.5 * (1 - (ep / max(1, cfg.total_episodes))))\n",
        "\n",
        "    policy_loss = -(log_probs_t * advantages.detach()).mean()\n",
        "    value_loss = F.mse_loss(values_t, returns)\n",
        "    entropy_bonus = entropies_t.mean()\n",
        "\n",
        "    loss = policy_loss + cfg.value_coef * value_loss - entropy_coef * entropy_bonus\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(agent.parameters(), cfg.grad_clip_norm)\n",
        "    optimizer.step()\n",
        "\n",
        "    if (ep + 1) % cfg.checkpoint_every_episodes == 0:\n",
        "        save_checkpoint(agent, optimizer, ep + 1, ckpt_path)\n",
        "\n",
        "    if (ep + 1) % cfg.eval_every_episodes == 0:\n",
        "        avg_eval = evaluate(agent, episodes=cfg.eval_episodes, render=False)\n",
        "        print(f'Ep {ep+1} | Reward entrenamiento: {total_reward:.1f} | Eval-10 media: {avg_eval:.1f}')\n",
        "        \n",
        "        # Almacenar progreso de evaluaci√≥n para gr√°ficos\n",
        "        eval_rewards.append(avg_eval)\n",
        "        eval_episodes_list.append(ep + 1)\n",
        "        \n",
        "        if avg_eval > best_eval:\n",
        "            best_eval = avg_eval\n",
        "            torch.save({'model': agent.state_dict(), 'avg_eval': best_eval}, os.path.join(cfg.checkpoint_dir, 'best.pt'))\n",
        "\n",
        "env.close()\n",
        "print('Entrenamiento finalizado.')\n",
        "\n",
        "# Generar gr√°ficos autom√°ticamente al completar el entrenamiento\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà GENERANDO GR√ÅFICOS DEL PROGRESO DE ENTRENAMIENTO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    # Ejecutar funci√≥n de gr√°ficos directamente aqu√≠\n",
        "    if len(episode_rewards) > 10:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import pandas as pd\n",
        "        \n",
        "        # Crear figura con subplots\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('REINFORCE DoubleDunk - Progreso del Entrenamiento', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # 1. Recompensas por episodio (raw + media m√≥vil)\n",
        "        ax1 = axes[0, 0]\n",
        "        episodes_range = range(1, len(episode_rewards) + 1)\n",
        "        ax1.plot(episodes_range, episode_rewards, alpha=0.6, color='blue', linewidth=0.8, label='Recompensa por episodio')\n",
        "        \n",
        "        # Media m√≥vil\n",
        "        if len(episode_rewards) > 50:\n",
        "            window = 50\n",
        "            moving_avg = pd.Series(episode_rewards).rolling(window=window, center=True).mean()\n",
        "            ax1.plot(episodes_range, moving_avg, color='red', linewidth=2, label=f'Media m√≥vil ({window})')\n",
        "        \n",
        "        # L√≠nea de baseline (-14.0)\n",
        "        ax1.axhline(y=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline REINFORCE (-14.0)')\n",
        "        ax1.set_xlabel('Episodios')\n",
        "        ax1.set_ylabel('Recompensa')\n",
        "        ax1.set_title('Evoluci√≥n de Recompensas por Episodio')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 2. Recompensas de evaluaci√≥n\n",
        "        ax2 = axes[0, 1]\n",
        "        if len(eval_rewards) > 0:\n",
        "            ax2.plot(eval_episodes_list, eval_rewards, 'o-', color='green', linewidth=2, markersize=6, label='Evaluaci√≥n (10 eps)')\n",
        "            ax2.axhline(y=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline (-14.0)')\n",
        "            ax2.set_xlabel('Episodios')\n",
        "            ax2.set_ylabel('Recompensa Media')\n",
        "            ax2.set_title('Progreso de Evaluaciones Peri√≥dicas')\n",
        "            ax2.legend()\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "        else:\n",
        "            ax2.text(0.5, 0.5, 'No hay datos de evaluaci√≥n\\ndisponibles', \n",
        "                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
        "            ax2.set_title('Evaluaciones Peri√≥dicas')\n",
        "        \n",
        "        # 3. Histograma de recompensas\n",
        "        ax3 = axes[1, 0]\n",
        "        ax3.hist(episode_rewards, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "        ax3.axvline(x=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline (-14.0)')\n",
        "        ax3.axvline(x=np.mean(episode_rewards), color='red', linestyle='-', linewidth=2, label=f'Media: {np.mean(episode_rewards):.1f}')\n",
        "        ax3.set_xlabel('Recompensa')\n",
        "        ax3.set_ylabel('Frecuencia')\n",
        "        ax3.set_title('Distribuci√≥n de Recompensas')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 4. Estad√≠sticas del entrenamiento\n",
        "        ax4 = axes[1, 1]\n",
        "        ax4.axis('off')\n",
        "        \n",
        "        # Calcular estad√≠sticas\n",
        "        total_episodes = len(episode_rewards)\n",
        "        mean_reward = np.mean(episode_rewards)\n",
        "        std_reward = np.std(episode_rewards)\n",
        "        min_reward = np.min(episode_rewards)\n",
        "        max_reward = np.max(episode_rewards)\n",
        "        \n",
        "        # √öltimos 100 episodios\n",
        "        last_100 = episode_rewards[-100:] if len(episode_rewards) >= 100 else episode_rewards\n",
        "        mean_last_100 = np.mean(last_100)\n",
        "        \n",
        "        # Mejora vs baseline\n",
        "        improvement = mean_reward - (-14.0)\n",
        "        improvement_pct = (improvement / abs(-14.0)) * 100\n",
        "        \n",
        "        stats_text = f\"\"\"\n",
        "üìä ESTAD√çSTICAS DEL ENTRENAMIENTO\n",
        "\n",
        "üéØ Episodios totales: {total_episodes:,}\n",
        "üìà Recompensa media: {mean_reward:.2f} ¬± {std_reward:.2f}\n",
        "üèÜ Mejor episodio: {max_reward:.2f}\n",
        "üìâ Peor episodio: {min_reward:.2f}\n",
        "\n",
        "üìä √öltimos 100 episodios: {mean_last_100:.2f}\n",
        "üì∂ Mejora vs baseline: {improvement:+.2f} ({improvement_pct:+.1f}%)\n",
        "\n",
        "üé≤ Baseline REINFORCE: -14.0\n",
        "üî• Estado: {\"‚úÖ Mejorando\" if improvement > 0 else \"‚ö†Ô∏è Por debajo del baseline\"}\n",
        "\"\"\"\n",
        "        \n",
        "        ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=11,\n",
        "                 verticalalignment='top', fontfamily='monospace',\n",
        "                 bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Imprimir resumen\n",
        "        print(f\"\\nüìã RESUMEN DEL PROGRESO:\")\n",
        "        print(f\"   üéØ Episodios entrenados: {total_episodes:,}\")\n",
        "        print(f\"   üìä Recompensa promedio: {mean_reward:.2f}\")\n",
        "        print(f\"   üèÜ Mejor resultado: {max_reward:.2f}\")\n",
        "        print(f\"   üìà √öltimos 100 eps: {mean_last_100:.2f}\")\n",
        "        print(f\"   {'‚úÖ' if improvement > 0 else '‚ùå'} Mejora vs baseline: {improvement:+.2f}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Entrenamiento muy corto para generar gr√°ficos √∫tiles\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error generando gr√°ficos: {e}\")\n",
        "    print(\"üí° Los gr√°ficos se pueden generar manualmente en la siguiente celda\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluaci√≥n del modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä INICIANDO EVALUACI√ìN COMPLETA REINFORCE\n",
            "=======================================================\n",
            "üèÜ Cargando mejor modelo entrenado...\n",
            "‚úÖ Mejor modelo cargado exitosamente\n",
            "\n",
            "üéØ Evaluaci√≥n oficial (10 episodios)...\n",
            "   Episodio 1: Reward=-4.00, Steps=8000\n",
            "   Episodio 2: Reward=0.00, Steps=8000\n",
            "   Episodio 3: Reward=0.00, Steps=8000\n",
            "   Episodio 4: Reward=2.00, Steps=8000\n",
            "   Episodio 5: Reward=0.00, Steps=8000\n",
            "   Episodio 6: Reward=-2.00, Steps=8000\n",
            "   Episodio 7: Reward=0.00, Steps=8000\n",
            "   Episodio 8: Reward=-2.00, Steps=8000\n",
            "   Episodio 9: Reward=-6.00, Steps=8000\n",
            "   Episodio 10: Reward=0.00, Steps=8000\n",
            "\n",
            "======================================================================\n",
            "üìã REPORTE OFICIAL - REINFORCE DOUBLEDUNK\n",
            "======================================================================\n",
            "üìÖ Fecha evaluaci√≥n: 2025-09-09 00:54:57\n",
            "‚è±Ô∏è  Tiempo evaluaci√≥n: 106.06s\n",
            "üñ•Ô∏è  Dispositivo: mps\n",
            "üéØ Algoritmo: REINFORCE con baseline (Actor-Cr√≠tico)\n",
            "\n",
            "üéØ RESULTADOS PRINCIPALES:\n",
            "üìä REINFORCE (10 episodios):    -1.20 ¬± 2.23\n",
            "üìà Rango de recompensas:        [-6.00, 2.00]\n",
            "üìè Duraci√≥n promedio:           8000.0 ¬± 0.0 pasos\n",
            "\n",
            "üìä AN√ÅLISIS DE RENDIMIENTO:\n",
            "‚îú‚îÄ üé≤ Baseline REINFORCE:       -14.0\n",
            "‚îú‚îÄ üì∂ Mejora absoluta:          +12.80 puntos\n",
            "‚îú‚îÄ üìà Mejora porcentual:        +91.4%\n",
            "‚îî‚îÄ üî• Estado: ‚úÖ Superando baseline\n",
            "\n",
            "üíæ ARCHIVOS GENERADOS:\n",
            "‚îî‚îÄ reinforce_doubledunk_results.json\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video guardado en: videos/doubledunk_reinforce.mp4\n",
            "Cargando mejor modelo desde checkpoints_doubledunk/best.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video guardado en: videos/doubledunk_best.mp4\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# EVALUACI√ìN COMPLETA \n",
        "# ========================================\n",
        "\n",
        "def comprehensive_evaluation_reinforce():\n",
        "    \"\"\"Evaluaci√≥n completa del modelo REINFORCE entrenado comparable con DDQN\"\"\"\n",
        "    \n",
        "    print(\"üìä INICIANDO EVALUACI√ìN COMPLETA REINFORCE\")\n",
        "    print(\"=\" * 55)\n",
        "    eval_start = time.time()\n",
        "    eval_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    \n",
        "    # Crear entorno de evaluaci√≥n\n",
        "    eval_env = make_env(seed=SEED + 999, render_mode=None)\n",
        "    \n",
        "    # Cargar mejor modelo si existe\n",
        "    try:\n",
        "        best_model_path = os.path.join(cfg.checkpoint_dir, 'best.pt')\n",
        "        if os.path.exists(best_model_path):\n",
        "            print(\"üèÜ Cargando mejor modelo entrenado...\")\n",
        "            best_agent = AtariActorCritic(in_channels=4, n_actions=eval_env.action_space.n).to(device)\n",
        "            ckpt_best = torch.load(best_model_path, map_location=device)\n",
        "            best_agent.load_state_dict(ckpt_best['model'])\n",
        "            eval_agent = best_agent\n",
        "            print(\"‚úÖ Mejor modelo cargado exitosamente\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Usando modelo actual (no se encontr√≥ best.pt)\")\n",
        "            eval_agent = agent\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error cargando mejor modelo: {e}\")\n",
        "        print(\"‚ö†Ô∏è  Usando modelo actual\")\n",
        "        eval_agent = agent\n",
        "    \n",
        "    # EVALUACI√ìN PRINCIPAL (10 episodios para comparar con DDQN)\n",
        "    print(\"\\nüéØ Evaluaci√≥n oficial (10 episodios)...\")\n",
        "    eval_agent.eval()\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for ep in range(10):\n",
        "            obs, info = eval_env.reset(seed=SEED + 999 + ep)\n",
        "            total_reward = 0.0\n",
        "            episode_length = 0\n",
        "            \n",
        "            for t in range(cfg.max_steps_per_episode):\n",
        "                x = obs_to_tensor(obs)\n",
        "                logits, _ = eval_agent(x)\n",
        "                action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
        "                obs, reward, terminated, truncated, info = eval_env.step(action)\n",
        "                total_reward += float(reward)\n",
        "                episode_length += 1\n",
        "                \n",
        "                if terminated or truncated:\n",
        "                    break\n",
        "            \n",
        "            episode_rewards.append(total_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "            print(f\"   Episodio {ep+1}: Reward={total_reward:.2f}, Steps={episode_length}\")\n",
        "    \n",
        "    eval_agent.train()\n",
        "    eval_env.close()\n",
        "    \n",
        "    eval_end = time.time()\n",
        "    eval_duration = eval_end - eval_start\n",
        "    \n",
        "    # Calcular estad√≠sticas\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "    min_reward = np.min(episode_rewards)\n",
        "    max_reward = np.max(episode_rewards)\n",
        "    mean_length = np.mean(episode_lengths)\n",
        "    std_length = np.std(episode_lengths)\n",
        "    \n",
        "    # Mejora vs baseline REINFORCE t√≠pico\n",
        "    baseline_reinforce = -14.0\n",
        "    improvement = mean_reward - baseline_reinforce\n",
        "    improvement_pct = (improvement / abs(baseline_reinforce)) * 100 if baseline_reinforce != 0 else 0\n",
        "    \n",
        "    # REPORTE OFICIAL COMPARABLE CON DDQN\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìã REPORTE OFICIAL - REINFORCE DOUBLEDUNK\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"üìÖ Fecha evaluaci√≥n: {eval_timestamp}\")\n",
        "    print(f\"‚è±Ô∏è  Tiempo evaluaci√≥n: {eval_duration:.2f}s\")\n",
        "    print(f\"üñ•Ô∏è  Dispositivo: {device}\")\n",
        "    print(f\"üéØ Algoritmo: REINFORCE con baseline (Actor-Cr√≠tico)\")\n",
        "    \n",
        "    print(f\"\\nüéØ RESULTADOS PRINCIPALES:\")\n",
        "    print(f\"üìä REINFORCE (10 episodios):    {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
        "    print(f\"üìà Rango de recompensas:        [{min_reward:.2f}, {max_reward:.2f}]\")\n",
        "    print(f\"üìè Duraci√≥n promedio:           {mean_length:.1f} ¬± {std_length:.1f} pasos\")\n",
        "    \n",
        "    print(f\"\\nüìä AN√ÅLISIS DE RENDIMIENTO:\")\n",
        "    print(f\"‚îú‚îÄ üé≤ Baseline REINFORCE:       {baseline_reinforce:.1f}\")\n",
        "    print(f\"‚îú‚îÄ üì∂ Mejora absoluta:          {improvement:+.2f} puntos\")\n",
        "    print(f\"‚îú‚îÄ üìà Mejora porcentual:        {improvement_pct:+.1f}%\")\n",
        "    print(f\"‚îî‚îÄ üî• Estado: {'‚úÖ Superando baseline' if improvement > 0 else '‚ö†Ô∏è Por debajo del baseline'}\")\n",
        "    \n",
        "    # Guardar resultados para comparaci√≥n\n",
        "    results = {\n",
        "        'experiment_info': {\n",
        "            'timestamp': eval_timestamp,\n",
        "            'algorithm': 'REINFORCE',\n",
        "            'device': str(device),\n",
        "            'evaluation_episodes': 10\n",
        "        },\n",
        "        'evaluation_results': {\n",
        "            'reinforce_baseline': baseline_reinforce,\n",
        "            'reinforce_10_episodes': {\n",
        "                'mean': float(mean_reward),\n",
        "                'std': float(std_reward),\n",
        "                'min': float(min_reward),\n",
        "                'max': float(max_reward)\n",
        "            },\n",
        "            'episode_lengths': {\n",
        "                'mean': float(mean_length),\n",
        "                'std': float(std_length)\n",
        "            },\n",
        "            'improvement_absolute': float(improvement),\n",
        "            'improvement_percentage': float(improvement_pct)\n",
        "        },\n",
        "        'individual_episodes': [float(r) for r in episode_rewards]\n",
        "    }\n",
        "    \n",
        "    # Exportar resultados\n",
        "    with open('reinforce_doubledunk_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nüíæ ARCHIVOS GENERADOS:\")\n",
        "    print(f\"‚îî‚îÄ reinforce_doubledunk_results.json\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Ejecutar evaluaci√≥n comparable\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "evaluation_results = comprehensive_evaluation_reinforce()\n",
        "\n",
        "# Funci√≥n de grabaci√≥n de video\n",
        "\n",
        "def record_video(model: nn.Module, filename: str = 'videos/doubledunk_reinforce.mp4', fps: int = 30, seed: int = SEED+2024):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    env = make_env(seed=seed, render_mode='rgb_array')\n",
        "    frames = []\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    with torch.no_grad():\n",
        "        for t in range(cfg.max_steps_per_episode):\n",
        "            x = obs_to_tensor(obs)\n",
        "            logits, _ = model(x)\n",
        "            action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
        "            frame = env.render()\n",
        "            frames.append(frame)\n",
        "            obs, r, terminated, truncated, info = env.step(action)\n",
        "            if terminated or truncated:\n",
        "                frame = env.render()\n",
        "                frames.append(frame)\n",
        "                break\n",
        "    env.close()\n",
        "    imageio.mimwrite(filename, frames, fps=fps, quality=8)\n",
        "    print('Video guardado en:', filename)\n",
        "\n",
        "# Grabar con agente actual\n",
        "record_video(agent, filename='videos/doubledunk_reinforce.mp4', fps=30)\n",
        "\n",
        "# Grabar con mejor modelo si existe\n",
        "best_path = os.path.join(cfg.checkpoint_dir, 'best.pt')\n",
        "if os.path.exists(best_path):\n",
        "    print('Cargando mejor modelo desde', best_path)\n",
        "    env_tmp = make_env(seed=SEED+3030)\n",
        "    n_actions_best = env_tmp.action_space.n\n",
        "    env_tmp.close()\n",
        "    best_agent = AtariActorCritic(in_channels=4, n_actions=n_actions_best).to(device)\n",
        "    ckpt_best = torch.load(best_path, map_location=device)\n",
        "    best_agent.load_state_dict(ckpt_best['model'])\n",
        "    record_video(best_agent, filename='videos/doubledunk_best.mp4', fps=30, seed=SEED+3030)\n",
        "else:\n",
        "    print('No se encontr√≥ best.pt; a√∫n no hay mejor modelo guardado')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Referencias\n",
        "\n",
        "[1] Gym Docs DoubleDunk https://ale.farama.org/environments/double_dunk/\n",
        "\n",
        "[2] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4), 229-256.\n",
        "\n",
        "[3] Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n",
        "\n",
        "[4] PyTorch Documentation https://pytorch.org/docs/stable/index.html\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
