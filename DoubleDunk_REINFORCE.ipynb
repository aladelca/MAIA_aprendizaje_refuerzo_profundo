{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DoubleDunk con REINFORCE (GPU: CUDA o MPS)\n",
        "\n",
        "Entrenamos un agente REINFORCE con baseline (actor-crítico) para `ALE/DoubleDunk-v5` (Atari) usando Gymnasium + ALE.\n",
        "\n",
        "- Algoritmo: REINFORCE con baseline, entropía y grad clipping.\n",
        "- Observaciones: preprocesamiento a 84x84 en escala de grises + apilado de 4 frames.\n",
        "- Hardware: usa automáticamente CUDA (NVIDIA), MPS (Apple Silicon) o CPU.\n",
        "- Checkpoints: guardado periódico y mejor modelo.\n",
        "- Video: grabación de episodio con el agente y con el mejor modelo.\n",
        "\n",
        "Referencia: [ALE DoubleDunk](https://ale.farama.org/environments/double_dunk/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mejoras para el Rendimiento\n",
        "\n",
        "DoubleDunk es un juego complejo donde inicialmente obtienes recompensas negativas. Esto es **NORMAL** - no significa que haya recompensas 0, sino que el agente está aprendiendo desde un estado donde comete errores.\n",
        "\n",
        "**Correcciones aplicadas:**\n",
        "1. **Shape mismatch corregido**: Los valores ahora tienen forma consistente con los returns \n",
        "2. **Entropía balanceada**: Decae gradualmente para mantener exploración inicial\n",
        "3. **Learning rate ajustado**: 3e-4 para convergencia más estable\n",
        "4. **Más episodios**: 8000 para dar tiempo al agente de aprender el juego\n",
        "\n",
        "El agente mejorará gradualmente de recompensas muy negativas (-20) hacia positivas conforme aprenda a jugar basketball.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'python': '3.13.5', 'platform': 'macOS-15.5-arm64-arm-64bit-Mach-O'}\n"
          ]
        }
      ],
      "source": [
        "# Instalación (si es necesario)\n",
        "# !pip install -q gymnasium[atari] ale-py torch torchvision imageio\n",
        "\n",
        "import sys, platform\n",
        "print({'python': sys.version.split()[0], 'platform': platform.platform()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import gymnasium\n",
        "import ale_py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch.optim import Adam\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from collections import deque\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import imageio\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Registrar entornos ALE\n",
        "gymnasium.register_envs(ale_py)\n",
        "\n",
        "SEED = 123\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Device: CUDA -> MPS -> CPU\n",
        "device = (\n",
        "    torch.device('cuda') if torch.cuda.is_available() else\n",
        "    (torch.device('mps') if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else torch.device('cpu'))\n",
        ")\n",
        "print('Using device:', device)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wrappers: preprocesamiento y frame stacking\n",
        "class SimpleFrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k: int = 4):\n",
        "        super().__init__(env)\n",
        "        self.k = k\n",
        "        self.frames = deque(maxlen=k)\n",
        "        obs_space = env.observation_space\n",
        "        h, w = obs_space.shape[0], obs_space.shape[1]\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(h, w, k), dtype=np.uint8)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.frames.clear()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(obs)\n",
        "        return self._get_ob(), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.frames.append(obs)\n",
        "        return self._get_ob(), reward, terminated, truncated, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        return np.stack(list(self.frames), axis=-1)\n",
        "\n",
        "\n",
        "def make_env(seed: int = SEED, render_mode=None):\n",
        "    env = gym.make('ALE/DoubleDunk-v5', render_mode=render_mode)\n",
        "    class GrayResizeWrapper(gym.ObservationWrapper):\n",
        "        def __init__(self, env):\n",
        "            super().__init__(env)\n",
        "            h, w = 84, 84\n",
        "            self.observation_space = gym.spaces.Box(low=0, high=255, shape=(h, w), dtype=np.uint8)\n",
        "        def observation(self, obs):\n",
        "            gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "            resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "            return resized\n",
        "    env = GrayResizeWrapper(env)\n",
        "    env = SimpleFrameStack(env, 4)\n",
        "    env.reset(seed=seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    return env\n",
        "\n",
        "# to tensor CHW [0,1]\n",
        "def obs_to_tensor(obs) -> torch.Tensor:\n",
        "    arr = obs if isinstance(obs, np.ndarray) else np.array(obs)\n",
        "    if arr.ndim == 3 and arr.shape[-1] == 4:\n",
        "        arr = np.transpose(arr, (2, 0, 1))\n",
        "    elif arr.ndim == 2:\n",
        "        arr = np.stack([arr]*4, axis=0)\n",
        "    tensor = torch.from_numpy(arr).float() / 255.0\n",
        "    return tensor.unsqueeze(0).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo CNN actor-crítico\n",
        "class AtariActorCritic(nn.Module):\n",
        "    def __init__(self, in_channels: int, n_actions: int):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 512), nn.ReLU(),\n",
        "        )\n",
        "        self.policy_head = nn.Linear(512, n_actions)\n",
        "        self.value_head = nn.Linear(512, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        z = self.features(x)\n",
        "        z = self.flatten(z)\n",
        "        z = self.fc(z)\n",
        "        logits = self.policy_head(z)\n",
        "        value = self.value_head(z).squeeze(-1)\n",
        "        return logits, value\n",
        "\n",
        "    def act(self, x: torch.Tensor):\n",
        "        logits, value = self.forward(x)\n",
        "        dist = Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy = dist.entropy()\n",
        "        return action.item(), log_prob, entropy, value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config(total_episodes=8000, max_steps_per_episode=8000, gamma=0.99, learning_rate=0.0003, entropy_coef=0.01, value_coef=0.5, grad_clip_norm=0.5, checkpoint_dir='checkpoints_doubledunk', checkpoint_every_episodes=50, eval_every_episodes=100, eval_episodes=10)\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    total_episodes: int = 8000\n",
        "    max_steps_per_episode: int = 8000\n",
        "    gamma: float = 0.99\n",
        "    learning_rate: float = 3e-4\n",
        "    entropy_coef: float = 0.01\n",
        "    value_coef: float = 0.5\n",
        "    grad_clip_norm: float = 0.5\n",
        "    checkpoint_dir: str = 'checkpoints_doubledunk'\n",
        "    checkpoint_every_episodes: int = 50\n",
        "    eval_every_episodes: int = 100\n",
        "    eval_episodes: int = 10\n",
        "\n",
        "cfg = Config()\n",
        "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
        "print(cfg)\n",
        "\n",
        "# Utils\n",
        "\n",
        "def compute_returns(rewards: List[float], gamma: float) -> torch.Tensor:\n",
        "    G = 0.0\n",
        "    returns = []\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.append(G)\n",
        "    returns.reverse()\n",
        "    ret = torch.tensor(returns, dtype=torch.float32, device=device)\n",
        "    return ret.view(-1)\n",
        "\n",
        "\n",
        "def save_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer, episode: int, path: str):\n",
        "    torch.save({'episode': episode,\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict()}, path)\n",
        "\n",
        "\n",
        "def load_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer, path: str):\n",
        "    ckpt = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    return ckpt.get('episode', 0)\n",
        "\n",
        "\n",
        "def evaluate(agent: nn.Module, episodes: int = 10, render: bool = False) -> float:\n",
        "    env = make_env(seed=SEED + 999, render_mode='human' if render else None)\n",
        "    agent.eval()\n",
        "    rewards = []\n",
        "    with torch.no_grad():\n",
        "        for ep in range(episodes):\n",
        "            obs, info = env.reset(seed=SEED + 999 + ep)\n",
        "            total_r = 0.0\n",
        "            for t in range(cfg.max_steps_per_episode):\n",
        "                x = obs_to_tensor(obs)\n",
        "                logits, _ = agent(x)\n",
        "                action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
        "                obs, r, terminated, truncated, info = env.step(action)\n",
        "                total_r += float(r)\n",
        "                if terminated or truncated:\n",
        "                    break\n",
        "            rewards.append(total_r)\n",
        "    env.close()\n",
        "    agent.train()\n",
        "    return float(np.mean(rewards))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando checkpoint desde checkpoints_doubledunk/reinforce_doubledunk.pt\n",
            "Ep 3100 | Reward entrenamiento: -16.0 | Eval-10 media: -2.4\n",
            "Ep 3200 | Reward entrenamiento: -24.0 | Eval-10 media: -10.0\n",
            "Ep 3300 | Reward entrenamiento: -20.0 | Eval-10 media: -2.4\n",
            "Ep 3400 | Reward entrenamiento: -12.0 | Eval-10 media: -2.8\n",
            "Ep 3500 | Reward entrenamiento: -18.0 | Eval-10 media: -1.4\n",
            "Ep 3600 | Reward entrenamiento: -22.0 | Eval-10 media: -1.2\n",
            "Ep 3700 | Reward entrenamiento: -22.0 | Eval-10 media: -1.8\n",
            "Ep 3800 | Reward entrenamiento: -20.0 | Eval-10 media: -2.8\n",
            "Ep 3900 | Reward entrenamiento: -14.0 | Eval-10 media: -3.6\n",
            "Ep 4000 | Reward entrenamiento: -22.0 | Eval-10 media: -2.8\n",
            "Ep 4100 | Reward entrenamiento: -20.0 | Eval-10 media: -2.6\n",
            "Ep 4200 | Reward entrenamiento: -22.0 | Eval-10 media: -8.8\n",
            "Ep 4300 | Reward entrenamiento: -10.0 | Eval-10 media: -7.0\n",
            "Ep 4400 | Reward entrenamiento: -20.0 | Eval-10 media: -7.2\n",
            "Ep 4500 | Reward entrenamiento: -22.0 | Eval-10 media: -21.8\n",
            "Ep 4600 | Reward entrenamiento: -20.0 | Eval-10 media: -13.2\n",
            "Ep 4700 | Reward entrenamiento: -20.0 | Eval-10 media: -18.8\n",
            "Ep 4800 | Reward entrenamiento: -16.0 | Eval-10 media: -18.8\n",
            "Ep 4900 | Reward entrenamiento: -18.0 | Eval-10 media: -19.0\n",
            "Ep 5000 | Reward entrenamiento: -18.0 | Eval-10 media: -7.4\n",
            "Ep 5100 | Reward entrenamiento: -18.0 | Eval-10 media: -20.4\n",
            "Ep 5200 | Reward entrenamiento: -22.0 | Eval-10 media: -3.4\n",
            "Ep 5300 | Reward entrenamiento: -22.0 | Eval-10 media: -4.0\n",
            "Ep 5400 | Reward entrenamiento: -20.0 | Eval-10 media: -3.4\n",
            "Ep 5500 | Reward entrenamiento: -24.0 | Eval-10 media: -18.2\n",
            "Ep 5600 | Reward entrenamiento: -20.0 | Eval-10 media: -12.0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg.max_steps_per_episode):\n\u001b[32m     24\u001b[39m     x = obs_to_tensor(obs)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     action, log_prob, entropy, value = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     obs, reward, terminated, truncated, info = env.step(action)\n\u001b[32m     27\u001b[39m     log_probs.append(log_prob)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mAtariActorCritic.act\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     31\u001b[39m logits, value = \u001b[38;5;28mself\u001b[39m.forward(x)\n\u001b[32m     32\u001b[39m dist = Categorical(logits=logits)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m action = \u001b[43mdist\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m log_prob = dist.log_prob(action)\n\u001b[32m     35\u001b[39m entropy = dist.entropy()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/distributions/categorical.py:143\u001b[39m, in \u001b[36mCategorical.sample\u001b[39m\u001b[34m(self, sample_shape)\u001b[39m\n\u001b[32m    141\u001b[39m     sample_shape = torch.Size(sample_shape)\n\u001b[32m    142\u001b[39m probs_2d = \u001b[38;5;28mself\u001b[39m.probs.reshape(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m._num_events)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m samples_2d = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.T\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d.reshape(\u001b[38;5;28mself\u001b[39m._extended_shape(sample_shape))\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Entrenamiento REINFORCE con baseline\n",
        "\n",
        "env = make_env(seed=SEED, render_mode=None)\n",
        "n_actions = env.action_space.n\n",
        "in_channels = 4\n",
        "\n",
        "agent = AtariActorCritic(in_channels=in_channels, n_actions=n_actions).to(device)\n",
        "optimizer = Adam(agent.parameters(), lr=cfg.learning_rate)\n",
        "\n",
        "start_episode = 0\n",
        "ckpt_path = os.path.join(cfg.checkpoint_dir, 'reinforce_doubledunk.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "    print('Cargando checkpoint desde', ckpt_path)\n",
        "    start_episode = load_checkpoint(agent, optimizer, ckpt_path)\n",
        "\n",
        "best_eval = -float('inf')\n",
        "\n",
        "for ep in range(start_episode, cfg.total_episodes):\n",
        "    obs, info = env.reset(seed=SEED + ep)\n",
        "    log_probs, entropies, values, rewards = [], [], [], []\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for t in range(cfg.max_steps_per_episode):\n",
        "        x = obs_to_tensor(obs)\n",
        "        action, log_prob, entropy, value = agent.act(x)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        log_probs.append(log_prob)\n",
        "        entropies.append(entropy)\n",
        "        values.append(value)\n",
        "        rewards.append(float(reward))\n",
        "        total_reward += float(reward)\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    returns = compute_returns(rewards, cfg.gamma)\n",
        "    values_t = torch.stack(values)\n",
        "    log_probs_t = torch.stack(log_probs)\n",
        "    entropies_t = torch.stack(entropies)\n",
        "\n",
        "    # Asegurar que values_t y returns tengan la misma forma\n",
        "    if values_t.dim() > 1:\n",
        "        values_t = values_t.squeeze(-1)\n",
        "    \n",
        "    advantages = returns - values_t\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    # Entropy decay: más exploración al inicio\n",
        "    entropy_coef = cfg.entropy_coef * (0.5 + 0.5 * (1 - (ep / max(1, cfg.total_episodes))))\n",
        "\n",
        "    policy_loss = -(log_probs_t * advantages.detach()).mean()\n",
        "    value_loss = F.mse_loss(values_t, returns)\n",
        "    entropy_bonus = entropies_t.mean()\n",
        "\n",
        "    loss = policy_loss + cfg.value_coef * value_loss - entropy_coef * entropy_bonus\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(agent.parameters(), cfg.grad_clip_norm)\n",
        "    optimizer.step()\n",
        "\n",
        "    if (ep + 1) % cfg.checkpoint_every_episodes == 0:\n",
        "        save_checkpoint(agent, optimizer, ep + 1, ckpt_path)\n",
        "\n",
        "    if (ep + 1) % cfg.eval_every_episodes == 0:\n",
        "        avg_eval = evaluate(agent, episodes=cfg.eval_episodes, render=False)\n",
        "        print(f'Ep {ep+1} | Reward entrenamiento: {total_reward:.1f} | Eval-10 media: {avg_eval:.1f}')\n",
        "        if avg_eval > best_eval:\n",
        "            best_eval = avg_eval\n",
        "            torch.save({'model': agent.state_dict(), 'avg_eval': best_eval}, os.path.join(cfg.checkpoint_dir, 'best.pt'))\n",
        "\n",
        "env.close()\n",
        "print('Entrenamiento finalizado.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recompensa promedio en 10 episodios (agente actual): -14.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video guardado en: videos/doubledunk_reinforce.mp4\n",
            "Cargando mejor modelo desde checkpoints_doubledunk/best.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video guardado en: videos/doubledunk_best.mp4\n"
          ]
        }
      ],
      "source": [
        "# Evaluación y video (agente actual y mejor modelo)\n",
        "\n",
        "# Evaluación 10 episodios\n",
        "avg10 = evaluate(agent, episodes=10, render=False)\n",
        "print(f'Recompensa promedio en 10 episodios (agente actual): {avg10:.2f}')\n",
        "\n",
        "# Función de grabación de video\n",
        "\n",
        "def record_video(model: nn.Module, filename: str = 'videos/doubledunk_reinforce.mp4', fps: int = 30, seed: int = SEED+2024):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    env = make_env(seed=seed, render_mode='rgb_array')\n",
        "    frames = []\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    with torch.no_grad():\n",
        "        for t in range(cfg.max_steps_per_episode):\n",
        "            x = obs_to_tensor(obs)\n",
        "            logits, _ = model(x)\n",
        "            action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
        "            frame = env.render()\n",
        "            frames.append(frame)\n",
        "            obs, r, terminated, truncated, info = env.step(action)\n",
        "            if terminated or truncated:\n",
        "                frame = env.render()\n",
        "                frames.append(frame)\n",
        "                break\n",
        "    env.close()\n",
        "    imageio.mimwrite(filename, frames, fps=fps, quality=8)\n",
        "    print('Video guardado en:', filename)\n",
        "\n",
        "# Grabar con agente actual\n",
        "record_video(agent, filename='videos/doubledunk_reinforce.mp4', fps=30)\n",
        "\n",
        "# Grabar con mejor modelo si existe\n",
        "best_path = os.path.join(cfg.checkpoint_dir, 'best.pt')\n",
        "if os.path.exists(best_path):\n",
        "    print('Cargando mejor modelo desde', best_path)\n",
        "    env_tmp = make_env(seed=SEED+3030)\n",
        "    n_actions_best = env_tmp.action_space.n\n",
        "    env_tmp.close()\n",
        "    best_agent = AtariActorCritic(in_channels=4, n_actions=n_actions_best).to(device)\n",
        "    ckpt_best = torch.load(best_path, map_location=device)\n",
        "    best_agent.load_state_dict(ckpt_best['model'])\n",
        "    record_video(best_agent, filename='videos/doubledunk_best.mp4', fps=30, seed=SEED+3030)\n",
        "else:\n",
        "    print('No se encontró best.pt; aún no hay mejor modelo guardado')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
