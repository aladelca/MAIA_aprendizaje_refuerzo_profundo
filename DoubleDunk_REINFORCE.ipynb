{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DoubleDunk - REINFORCE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resumen:\n",
        "\n",
        "- Algoritmo: REINFORCE con baseline (Actor-CrÃ­tico), entropÃ­a y gradient clipping.\n",
        "- Ambiente: `ALE/DoubleDunk-v5`\n",
        "  - Espacio de Acciones Discreto: `18` acciones\n",
        "  - Espacio de Observaciones: `Box(0, 255, (210, 160, 3), uint8)`\n",
        "- Objetivo: Anotar en el juego de baloncesto, defender y superar al oponente.\n",
        "- Puntaje baseline: `-14.0` (resultado tÃ­pico)\n",
        "- Hardware: Compatible con Google Colab GPU, detecta automÃ¡ticamente CUDA/MPS/CPU\n",
        "- Tiempo de entrenamiento estimado: ~6-8 horas (8000 episodios)\n",
        "- CaracterÃ­sticas: Sistema de checkpoints, evaluaciÃ³n periÃ³dica, generaciÃ³n de videos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## InstalaciÃ³n e importaciÃ³n de librerÃ­as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Todas las librerÃ­as han sido instaladas correctamente.\n"
          ]
        }
      ],
      "source": [
        "# InstalaciÃ³n de dependencias para Google Colab\n",
        "!pip install gymnasium[atari] ale-py torch torchvision imageio -q\n",
        "!pip install \"gymnasium[atari,accept-rom-license]\" -q\n",
        "!AutoROM --accept-license\n",
        "\n",
        "# LibrerÃ­as bÃ¡sicas\n",
        "import sys, platform\n",
        "print({'python': sys.version.split()[0], 'platform': platform.platform()})\n",
        "\n",
        "#Limpia los registros generados\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Todas las librerÃ­as han sido instaladas correctamente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DetecciÃ³n de Hardware\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ğŸ–¥ï¸  DETECCIÃ“N DE HARDWARE\n",
            "============================================================\n",
            "Sistema: Darwin 24.5.0\n",
            "PyTorch: 2.8.0\n",
            "âœ… MPS DETECTADO (Apple Silicon)\n",
            "   ğŸ AceleraciÃ³n optimizada para Apple Silicon\n",
            "------------------------------------------------------------\n",
            "DISPOSITIVO FINAL: mps\n",
            "============================================================\n",
            "\n",
            "ğŸ¯ Sistema configurado para: mps\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# IMPORTACIONES Y DETECCIÃ“N DE HARDWARE\n",
        "# ========================================\n",
        "\n",
        "import gymnasium as gym\n",
        "import gymnasium\n",
        "import ale_py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch.optim import Adam\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from collections import deque\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import imageio\n",
        "import warnings\n",
        "import platform\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Registrar entornos ALE\n",
        "gymnasium.register_envs(ale_py)\n",
        "\n",
        "SEED = 123\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "def detect_hardware():\n",
        "    \"\"\"Detecta y configura el hardware disponible (Google Colab compatible)\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ–¥ï¸  DETECCIÃ“N DE HARDWARE\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # InformaciÃ³n del sistema\n",
        "    print(f\"Sistema: {platform.system()} {platform.release()}\")\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    \n",
        "    # Verificar CUDA (GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        \n",
        "        print(f\"âœ… GPU DETECTADA: {gpu_name}\")\n",
        "        print(f\"   ğŸ’¾ Memoria GPU: {gpu_memory:.1f} GB\")\n",
        "        print(f\"   ğŸ”§ CUDA Version: {torch.version.cuda}\")\n",
        "        \n",
        "        # Configuraciones de PyTorch para GPU\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    # Verificar MPS (Apple Silicon)\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device('mps')\n",
        "        print(f\"âœ… MPS DETECTADO (Apple Silicon)\")\n",
        "        print(f\"   ğŸ AceleraciÃ³n optimizada para Apple Silicon\")\n",
        "        \n",
        "    else:\n",
        "        # Fallback a CPU\n",
        "        device = torch.device('cpu')\n",
        "        print(f\"âš ï¸  SOLO CPU DISPONIBLE\")\n",
        "        print(f\"   âŒ No se detectÃ³ GPU - El entrenamiento serÃ¡ mÃ¡s lento\")\n",
        "        print(f\"   ğŸ’¡ En Colab: Runtime > Change runtime type > GPU\")\n",
        "        torch.set_num_threads(4)  # Limitar threads en CPU\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "    print(f\"DISPOSITIVO FINAL: {device}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return device\n",
        "\n",
        "# Detectar hardware\n",
        "device = detect_hardware()\n",
        "\n",
        "# Configuraciones determinÃ­sticas\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(f\"\\nğŸ¯ Sistema configurado para: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizaremos el algoritmo REINFORCE con baseline para el entrenamiento\n",
        "\n",
        "## DefiniciÃ³n del Algoritmo REINFORCE\n",
        "\n",
        "REINFORCE (REward Increment = Nonnegative Factor Ã— Offset Reinforcement Ã— Characteristic Eligibility) es un algoritmo de gradiente de polÃ­tica que optimiza directamente la polÃ­tica para maximizar la recompensa esperada.\n",
        "\n",
        "**CaracterÃ­sticas principales:**\n",
        "- **Algoritmo de polÃ­tica**: Optimiza directamente la funciÃ³n de polÃ­tica Ï€(a|s)\n",
        "- **Baseline con Critic**: Reduce la varianza usando un estimador de valor V(s)\n",
        "- **Monte Carlo**: Usa retornos completos de episodios para actualizar\n",
        "- **Gradient ascent**: Maximiza la recompensa esperada usando gradientes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wrappers: preprocesamiento y frame stacking\n",
        "class SimpleFrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k: int = 4):\n",
        "        super().__init__(env)\n",
        "        self.k = k\n",
        "        self.frames = deque(maxlen=k)\n",
        "        obs_space = env.observation_space\n",
        "        h, w = obs_space.shape[0], obs_space.shape[1]\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(h, w, k), dtype=np.uint8)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.frames.clear()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(obs)\n",
        "        return self._get_ob(), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.frames.append(obs)\n",
        "        return self._get_ob(), reward, terminated, truncated, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        return np.stack(list(self.frames), axis=-1)\n",
        "\n",
        "\n",
        "def make_env(seed: int = SEED, render_mode=None):\n",
        "    env = gym.make('ALE/DoubleDunk-v5', render_mode=render_mode)\n",
        "    class GrayResizeWrapper(gym.ObservationWrapper):\n",
        "        def __init__(self, env):\n",
        "            super().__init__(env)\n",
        "            h, w = 84, 84\n",
        "            self.observation_space = gym.spaces.Box(low=0, high=255, shape=(h, w), dtype=np.uint8)\n",
        "        def observation(self, obs):\n",
        "            gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "            resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "            return resized\n",
        "    env = GrayResizeWrapper(env)\n",
        "    env = SimpleFrameStack(env, 4)\n",
        "    env.reset(seed=seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    return env\n",
        "\n",
        "# to tensor CHW [0,1]\n",
        "def obs_to_tensor(obs) -> torch.Tensor:\n",
        "    arr = obs if isinstance(obs, np.ndarray) else np.array(obs)\n",
        "    if arr.ndim == 3 and arr.shape[-1] == 4:\n",
        "        arr = np.transpose(arr, (2, 0, 1))\n",
        "    elif arr.ndim == 2:\n",
        "        arr = np.stack([arr]*4, axis=0)\n",
        "    tensor = torch.from_numpy(arr).float() / 255.0\n",
        "    return tensor.unsqueeze(0).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenamiento\n",
        "\n",
        "ConfiguraciÃ³n de hiperparÃ¡metros y ejecuciÃ³n del entrenamiento REINFORCE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo CNN actor-crÃ­tico\n",
        "class AtariActorCritic(nn.Module):\n",
        "    def __init__(self, in_channels: int, n_actions: int):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 512), nn.ReLU(),\n",
        "        )\n",
        "        self.policy_head = nn.Linear(512, n_actions)\n",
        "        self.value_head = nn.Linear(512, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        z = self.features(x)\n",
        "        z = self.flatten(z)\n",
        "        z = self.fc(z)\n",
        "        logits = self.policy_head(z)\n",
        "        value = self.value_head(z).squeeze(-1)\n",
        "        return logits, value\n",
        "\n",
        "    def act(self, x: torch.Tensor):\n",
        "        logits, value = self.forward(x)\n",
        "        dist = Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy = dist.entropy()\n",
        "        return action.item(), log_prob, entropy, value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config(total_episodes=8000, max_steps_per_episode=8000, gamma=0.99, learning_rate=0.0003, entropy_coef=0.01, value_coef=0.5, grad_clip_norm=0.5, checkpoint_dir='checkpoints_doubledunk', checkpoint_every_episodes=50, eval_every_episodes=100, eval_episodes=10)\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    total_episodes: int = 8000\n",
        "    max_steps_per_episode: int = 8000\n",
        "    gamma: float = 0.99\n",
        "    learning_rate: float = 3e-4\n",
        "    entropy_coef: float = 0.01\n",
        "    value_coef: float = 0.5\n",
        "    grad_clip_norm: float = 0.5\n",
        "    checkpoint_dir: str = 'checkpoints_doubledunk'\n",
        "    checkpoint_every_episodes: int = 50\n",
        "    eval_every_episodes: int = 100\n",
        "    eval_episodes: int = 10\n",
        "\n",
        "cfg = Config()\n",
        "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
        "print(cfg)\n",
        "\n",
        "# Utils\n",
        "\n",
        "def compute_returns(rewards: List[float], gamma: float) -> torch.Tensor:\n",
        "    G = 0.0\n",
        "    returns = []\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.append(G)\n",
        "    returns.reverse()\n",
        "    ret = torch.tensor(returns, dtype=torch.float32, device=device)\n",
        "    return ret.view(-1)\n",
        "\n",
        "\n",
        "def save_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer, episode: int, path: str):\n",
        "    torch.save({'episode': episode,\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict()}, path)\n",
        "\n",
        "\n",
        "def load_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer, path: str):\n",
        "    ckpt = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    return ckpt.get('episode', 0)\n",
        "\n",
        "\n",
        "def evaluate(agent: nn.Module, episodes: int = 10, render: bool = False) -> float:\n",
        "    env = make_env(seed=SEED + 999, render_mode='human' if render else None)\n",
        "    agent.eval()\n",
        "    rewards = []\n",
        "    with torch.no_grad():\n",
        "        for ep in range(episodes):\n",
        "            obs, info = env.reset(seed=SEED + 999 + ep)\n",
        "            total_r = 0.0\n",
        "            for t in range(cfg.max_steps_per_episode):\n",
        "                x = obs_to_tensor(obs)\n",
        "                logits, _ = agent(x)\n",
        "                action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
        "                obs, r, terminated, truncated, info = env.step(action)\n",
        "                total_r += float(r)\n",
        "                if terminated or truncated:\n",
        "                    break\n",
        "            rewards.append(total_r)\n",
        "    env.close()\n",
        "    agent.train()\n",
        "    return float(np.mean(rewards))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š GrÃ¡ficos disponibles despuÃ©s del entrenamiento\n",
            "ğŸ’¡ Los grÃ¡ficos se generarÃ¡n automÃ¡ticamente al completar el entrenamiento\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# GRÃFICOS DE PROGRESO DEL ENTRENAMIENTO\n",
        "# ========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plot_training_progress():\n",
        "    \"\"\"Genera grÃ¡ficos del progreso del entrenamiento REINFORCE\"\"\"\n",
        "    \n",
        "    # Verificar si hay datos de entrenamiento\n",
        "    if not hasattr(agent, 'episode_rewards') or len(episode_rewards) == 0:\n",
        "        print(\"âš ï¸  No hay datos de entrenamiento para graficar\")\n",
        "        print(\"ğŸ’¡ Ejecuta primero la celda de entrenamiento\")\n",
        "        return\n",
        "    \n",
        "    print(\"ğŸ“ˆ GENERANDO GRÃFICOS DE PROGRESO DEL ENTRENAMIENTO\")\n",
        "    print(\"=\" * 55)\n",
        "    \n",
        "    # Crear figura con subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('REINFORCE DoubleDunk - Progreso del Entrenamiento', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Recompensas por episodio (raw + media mÃ³vil)\n",
        "    ax1 = axes[0, 0]\n",
        "    episodes_range = range(1, len(episode_rewards) + 1)\n",
        "    ax1.plot(episodes_range, episode_rewards, alpha=0.6, color='blue', linewidth=0.8, label='Recompensa por episodio')\n",
        "    \n",
        "    # Media mÃ³vil\n",
        "    if len(episode_rewards) > 50:\n",
        "        window = 50\n",
        "        moving_avg = pd.Series(episode_rewards).rolling(window=window, center=True).mean()\n",
        "        ax1.plot(episodes_range, moving_avg, color='red', linewidth=2, label=f'Media mÃ³vil ({window})')\n",
        "    \n",
        "    # LÃ­nea de baseline (-14.0)\n",
        "    ax1.axhline(y=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline REINFORCE (-14.0)')\n",
        "    ax1.set_xlabel('Episodios')\n",
        "    ax1.set_ylabel('Recompensa')\n",
        "    ax1.set_title('EvoluciÃ³n de Recompensas por Episodio')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Recompensas de evaluaciÃ³n\n",
        "    ax2 = axes[0, 1]\n",
        "    if len(eval_rewards) > 0:\n",
        "        ax2.plot(eval_episodes_list, eval_rewards, 'o-', color='green', linewidth=2, markersize=6, label='EvaluaciÃ³n (10 eps)')\n",
        "        ax2.axhline(y=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline (-14.0)')\n",
        "        ax2.set_xlabel('Episodios')\n",
        "        ax2.set_ylabel('Recompensa Media')\n",
        "        ax2.set_title('Progreso de Evaluaciones PeriÃ³dicas')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax2.text(0.5, 0.5, 'No hay datos de evaluaciÃ³n\\ndisponibles', \n",
        "                ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
        "        ax2.set_title('Evaluaciones PeriÃ³dicas')\n",
        "    \n",
        "    # 3. Histograma de recompensas\n",
        "    ax3 = axes[1, 0]\n",
        "    ax3.hist(episode_rewards, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "    ax3.axvline(x=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline (-14.0)')\n",
        "    ax3.axvline(x=np.mean(episode_rewards), color='red', linestyle='-', linewidth=2, label=f'Media: {np.mean(episode_rewards):.1f}')\n",
        "    ax3.set_xlabel('Recompensa')\n",
        "    ax3.set_ylabel('Frecuencia')\n",
        "    ax3.set_title('DistribuciÃ³n de Recompensas')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. EstadÃ­sticas del entrenamiento\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.axis('off')\n",
        "    \n",
        "    # Calcular estadÃ­sticas\n",
        "    total_episodes = len(episode_rewards)\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "    min_reward = np.min(episode_rewards)\n",
        "    max_reward = np.max(episode_rewards)\n",
        "    \n",
        "    # Ãšltimos 100 episodios\n",
        "    last_100 = episode_rewards[-100:] if len(episode_rewards) >= 100 else episode_rewards\n",
        "    mean_last_100 = np.mean(last_100)\n",
        "    \n",
        "    # Mejora vs baseline\n",
        "    improvement = mean_reward - (-14.0)\n",
        "    improvement_pct = (improvement / abs(-14.0)) * 100\n",
        "    \n",
        "    stats_text = f\"\"\"\n",
        "ğŸ“Š ESTADÃSTICAS DEL ENTRENAMIENTO\n",
        "\n",
        "ğŸ¯ Episodios totales: {total_episodes:,}\n",
        "ğŸ“ˆ Recompensa media: {mean_reward:.2f} Â± {std_reward:.2f}\n",
        "ğŸ† Mejor episodio: {max_reward:.2f}\n",
        "ğŸ“‰ Peor episodio: {min_reward:.2f}\n",
        "\n",
        "ğŸ“Š Ãšltimos 100 episodios: {mean_last_100:.2f}\n",
        "ğŸ“¶ Mejora vs baseline: {improvement:+.2f} ({improvement_pct:+.1f}%)\n",
        "\n",
        "ğŸ² Baseline REINFORCE: -14.0\n",
        "ğŸ”¥ Estado: {\"âœ… Mejorando\" if improvement > 0 else \"âš ï¸ Por debajo del baseline\"}\n",
        "\"\"\"\n",
        "    \n",
        "    ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=11,\n",
        "             verticalalignment='top', fontfamily='monospace',\n",
        "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Imprimir resumen\n",
        "    print(f\"\\nğŸ“‹ RESUMEN DEL PROGRESO:\")\n",
        "    print(f\"   ğŸ¯ Episodios entrenados: {total_episodes:,}\")\n",
        "    print(f\"   ğŸ“Š Recompensa promedio: {mean_reward:.2f}\")\n",
        "    print(f\"   ğŸ† Mejor resultado: {max_reward:.2f}\")\n",
        "    print(f\"   ğŸ“ˆ Ãšltimos 100 eps: {mean_last_100:.2f}\")\n",
        "    print(f\"   {'âœ…' if improvement > 0 else 'âŒ'} Mejora vs baseline: {improvement:+.2f}\")\n",
        "\n",
        "# Ejecutar visualizaciÃ³n si hay datos\n",
        "try:\n",
        "    if 'episode_rewards' in locals() and len(episode_rewards) > 10:\n",
        "        plot_training_progress()\n",
        "    else:\n",
        "        print(\"ğŸ“Š GrÃ¡ficos disponibles despuÃ©s del entrenamiento\")\n",
        "        print(\"ğŸ’¡ Los grÃ¡ficos se generarÃ¡n automÃ¡ticamente al completar el entrenamiento\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Error generando grÃ¡ficos: {e}\")\n",
        "    print(\"ğŸ’¡ AsegÃºrate de ejecutar primero la celda de entrenamiento\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando checkpoint desde checkpoints_doubledunk/reinforce_doubledunk.pt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg.max_steps_per_episode):\n\u001b[32m     29\u001b[39m     x = obs_to_tensor(obs)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     action, log_prob, entropy, value = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     obs, reward, terminated, truncated, info = env.step(action)\n\u001b[32m     32\u001b[39m     log_probs.append(log_prob)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mAtariActorCritic.act\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     logits, value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     dist = Categorical(logits=logits)\n\u001b[32m     33\u001b[39m     action = dist.sample()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mAtariActorCritic.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m z = \u001b[38;5;28mself\u001b[39m.features(x)\n\u001b[32m     24\u001b[39m z = \u001b[38;5;28mself\u001b[39m.flatten(z)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.policy_head(z)\n\u001b[32m     27\u001b[39m value = \u001b[38;5;28mself\u001b[39m.value_head(z).squeeze(-\u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Entrenamiento REINFORCE con baseline\n",
        "\n",
        "env = make_env(seed=SEED, render_mode=None)\n",
        "n_actions = env.action_space.n\n",
        "in_channels = 4\n",
        "\n",
        "agent = AtariActorCritic(in_channels=in_channels, n_actions=n_actions).to(device)\n",
        "optimizer = Adam(agent.parameters(), lr=cfg.learning_rate)\n",
        "\n",
        "start_episode = 0\n",
        "ckpt_path = os.path.join(cfg.checkpoint_dir, 'reinforce_doubledunk.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "    print('Cargando checkpoint desde', ckpt_path)\n",
        "    start_episode = load_checkpoint(agent, optimizer, ckpt_path)\n",
        "\n",
        "best_eval = -float('inf')\n",
        "\n",
        "# Listas para almacenar progreso del entrenamiento\n",
        "episode_rewards = []\n",
        "eval_rewards = []\n",
        "eval_episodes_list = []\n",
        "\n",
        "for ep in range(start_episode, cfg.total_episodes):\n",
        "    obs, info = env.reset(seed=SEED + ep)\n",
        "    log_probs, entropies, values, rewards = [], [], [], []\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for t in range(cfg.max_steps_per_episode):\n",
        "        x = obs_to_tensor(obs)\n",
        "        action, log_prob, entropy, value = agent.act(x)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        log_probs.append(log_prob)\n",
        "        entropies.append(entropy)\n",
        "        values.append(value)\n",
        "        rewards.append(float(reward))\n",
        "        total_reward += float(reward)\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    # Almacenar reward del episodio para grÃ¡ficos\n",
        "    episode_rewards.append(total_reward)\n",
        "\n",
        "    returns = compute_returns(rewards, cfg.gamma)\n",
        "    values_t = torch.stack(values)\n",
        "    log_probs_t = torch.stack(log_probs)\n",
        "    entropies_t = torch.stack(entropies)\n",
        "\n",
        "    # Asegurar que values_t y returns tengan la misma forma\n",
        "    if values_t.dim() > 1:\n",
        "        values_t = values_t.squeeze(-1)\n",
        "    \n",
        "    advantages = returns - values_t\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    # Entropy decay: mÃ¡s exploraciÃ³n al inicio\n",
        "    entropy_coef = cfg.entropy_coef * (0.5 + 0.5 * (1 - (ep / max(1, cfg.total_episodes))))\n",
        "\n",
        "    policy_loss = -(log_probs_t * advantages.detach()).mean()\n",
        "    value_loss = F.mse_loss(values_t, returns)\n",
        "    entropy_bonus = entropies_t.mean()\n",
        "\n",
        "    loss = policy_loss + cfg.value_coef * value_loss - entropy_coef * entropy_bonus\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(agent.parameters(), cfg.grad_clip_norm)\n",
        "    optimizer.step()\n",
        "\n",
        "    if (ep + 1) % cfg.checkpoint_every_episodes == 0:\n",
        "        save_checkpoint(agent, optimizer, ep + 1, ckpt_path)\n",
        "\n",
        "    if (ep + 1) % cfg.eval_every_episodes == 0:\n",
        "        avg_eval = evaluate(agent, episodes=cfg.eval_episodes, render=False)\n",
        "        print(f'Ep {ep+1} | Reward entrenamiento: {total_reward:.1f} | Eval-10 media: {avg_eval:.1f}')\n",
        "        \n",
        "        # Almacenar progreso de evaluaciÃ³n para grÃ¡ficos\n",
        "        eval_rewards.append(avg_eval)\n",
        "        eval_episodes_list.append(ep + 1)\n",
        "        \n",
        "        if avg_eval > best_eval:\n",
        "            best_eval = avg_eval\n",
        "            torch.save({'model': agent.state_dict(), 'avg_eval': best_eval}, os.path.join(cfg.checkpoint_dir, 'best.pt'))\n",
        "\n",
        "env.close()\n",
        "print('Entrenamiento finalizado.')\n",
        "\n",
        "# Generar grÃ¡ficos automÃ¡ticamente al completar el entrenamiento\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“ˆ GENERANDO GRÃFICOS DEL PROGRESO DE ENTRENAMIENTO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    # Ejecutar funciÃ³n de grÃ¡ficos directamente aquÃ­\n",
        "    if len(episode_rewards) > 10:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import pandas as pd\n",
        "        \n",
        "        # Crear figura con subplots\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('REINFORCE DoubleDunk - Progreso del Entrenamiento', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # 1. Recompensas por episodio (raw + media mÃ³vil)\n",
        "        ax1 = axes[0, 0]\n",
        "        episodes_range = range(1, len(episode_rewards) + 1)\n",
        "        ax1.plot(episodes_range, episode_rewards, alpha=0.6, color='blue', linewidth=0.8, label='Recompensa por episodio')\n",
        "        \n",
        "        # Media mÃ³vil\n",
        "        if len(episode_rewards) > 50:\n",
        "            window = 50\n",
        "            moving_avg = pd.Series(episode_rewards).rolling(window=window, center=True).mean()\n",
        "            ax1.plot(episodes_range, moving_avg, color='red', linewidth=2, label=f'Media mÃ³vil ({window})')\n",
        "        \n",
        "        # LÃ­nea de baseline (-14.0)\n",
        "        ax1.axhline(y=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline REINFORCE (-14.0)')\n",
        "        ax1.set_xlabel('Episodios')\n",
        "        ax1.set_ylabel('Recompensa')\n",
        "        ax1.set_title('EvoluciÃ³n de Recompensas por Episodio')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 2. Recompensas de evaluaciÃ³n\n",
        "        ax2 = axes[0, 1]\n",
        "        if len(eval_rewards) > 0:\n",
        "            ax2.plot(eval_episodes_list, eval_rewards, 'o-', color='green', linewidth=2, markersize=6, label='EvaluaciÃ³n (10 eps)')\n",
        "            ax2.axhline(y=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline (-14.0)')\n",
        "            ax2.set_xlabel('Episodios')\n",
        "            ax2.set_ylabel('Recompensa Media')\n",
        "            ax2.set_title('Progreso de Evaluaciones PeriÃ³dicas')\n",
        "            ax2.legend()\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "        else:\n",
        "            ax2.text(0.5, 0.5, 'No hay datos de evaluaciÃ³n\\ndisponibles', \n",
        "                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
        "            ax2.set_title('Evaluaciones PeriÃ³dicas')\n",
        "        \n",
        "        # 3. Histograma de recompensas\n",
        "        ax3 = axes[1, 0]\n",
        "        ax3.hist(episode_rewards, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "        ax3.axvline(x=-14.0, color='orange', linestyle='--', linewidth=2, label='Baseline (-14.0)')\n",
        "        ax3.axvline(x=np.mean(episode_rewards), color='red', linestyle='-', linewidth=2, label=f'Media: {np.mean(episode_rewards):.1f}')\n",
        "        ax3.set_xlabel('Recompensa')\n",
        "        ax3.set_ylabel('Frecuencia')\n",
        "        ax3.set_title('DistribuciÃ³n de Recompensas')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 4. EstadÃ­sticas del entrenamiento\n",
        "        ax4 = axes[1, 1]\n",
        "        ax4.axis('off')\n",
        "        \n",
        "        # Calcular estadÃ­sticas\n",
        "        total_episodes = len(episode_rewards)\n",
        "        mean_reward = np.mean(episode_rewards)\n",
        "        std_reward = np.std(episode_rewards)\n",
        "        min_reward = np.min(episode_rewards)\n",
        "        max_reward = np.max(episode_rewards)\n",
        "        \n",
        "        # Ãšltimos 100 episodios\n",
        "        last_100 = episode_rewards[-100:] if len(episode_rewards) >= 100 else episode_rewards\n",
        "        mean_last_100 = np.mean(last_100)\n",
        "        \n",
        "        # Mejora vs baseline\n",
        "        improvement = mean_reward - (-14.0)\n",
        "        improvement_pct = (improvement / abs(-14.0)) * 100\n",
        "        \n",
        "        stats_text = f\"\"\"\n",
        "ğŸ“Š ESTADÃSTICAS DEL ENTRENAMIENTO\n",
        "\n",
        "ğŸ¯ Episodios totales: {total_episodes:,}\n",
        "ğŸ“ˆ Recompensa media: {mean_reward:.2f} Â± {std_reward:.2f}\n",
        "ğŸ† Mejor episodio: {max_reward:.2f}\n",
        "ğŸ“‰ Peor episodio: {min_reward:.2f}\n",
        "\n",
        "ğŸ“Š Ãšltimos 100 episodios: {mean_last_100:.2f}\n",
        "ğŸ“¶ Mejora vs baseline: {improvement:+.2f} ({improvement_pct:+.1f}%)\n",
        "\n",
        "ğŸ² Baseline REINFORCE: -14.0\n",
        "ğŸ”¥ Estado: {\"âœ… Mejorando\" if improvement > 0 else \"âš ï¸ Por debajo del baseline\"}\n",
        "\"\"\"\n",
        "        \n",
        "        ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=11,\n",
        "                 verticalalignment='top', fontfamily='monospace',\n",
        "                 bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Imprimir resumen\n",
        "        print(f\"\\nğŸ“‹ RESUMEN DEL PROGRESO:\")\n",
        "        print(f\"   ğŸ¯ Episodios entrenados: {total_episodes:,}\")\n",
        "        print(f\"   ğŸ“Š Recompensa promedio: {mean_reward:.2f}\")\n",
        "        print(f\"   ğŸ† Mejor resultado: {max_reward:.2f}\")\n",
        "        print(f\"   ğŸ“ˆ Ãšltimos 100 eps: {mean_last_100:.2f}\")\n",
        "        print(f\"   {'âœ…' if improvement > 0 else 'âŒ'} Mejora vs baseline: {improvement:+.2f}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"âš ï¸  Entrenamiento muy corto para generar grÃ¡ficos Ãºtiles\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Error generando grÃ¡ficos: {e}\")\n",
        "    print(\"ğŸ’¡ Los grÃ¡ficos se pueden generar manualmente en la siguiente celda\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EvaluaciÃ³n del modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š INICIANDO EVALUACIÃ“N COMPLETA REINFORCE\n",
            "=======================================================\n",
            "ğŸ† Cargando mejor modelo entrenado...\n",
            "âœ… Mejor modelo cargado exitosamente\n",
            "\n",
            "ğŸ¯ EvaluaciÃ³n oficial (10 episodios)...\n",
            "   Episodio 1: Reward=-4.00, Steps=8000\n",
            "   Episodio 2: Reward=0.00, Steps=8000\n",
            "   Episodio 3: Reward=0.00, Steps=8000\n",
            "   Episodio 4: Reward=2.00, Steps=8000\n",
            "   Episodio 5: Reward=0.00, Steps=8000\n",
            "   Episodio 6: Reward=-2.00, Steps=8000\n",
            "   Episodio 7: Reward=0.00, Steps=8000\n",
            "   Episodio 8: Reward=-2.00, Steps=8000\n",
            "   Episodio 9: Reward=-6.00, Steps=8000\n",
            "   Episodio 10: Reward=0.00, Steps=8000\n",
            "\n",
            "======================================================================\n",
            "ğŸ“‹ REPORTE OFICIAL - REINFORCE DOUBLEDUNK\n",
            "======================================================================\n",
            "ğŸ“… Fecha evaluaciÃ³n: 2025-09-09 00:54:57\n",
            "â±ï¸  Tiempo evaluaciÃ³n: 106.06s\n",
            "ğŸ–¥ï¸  Dispositivo: mps\n",
            "ğŸ¯ Algoritmo: REINFORCE con baseline (Actor-CrÃ­tico)\n",
            "\n",
            "ğŸ¯ RESULTADOS PRINCIPALES:\n",
            "ğŸ“Š REINFORCE (10 episodios):    -1.20 Â± 2.23\n",
            "ğŸ“ˆ Rango de recompensas:        [-6.00, 2.00]\n",
            "ğŸ“ DuraciÃ³n promedio:           8000.0 Â± 0.0 pasos\n",
            "\n",
            "ğŸ“Š ANÃLISIS DE RENDIMIENTO:\n",
            "â”œâ”€ ğŸ² Baseline REINFORCE:       -14.0\n",
            "â”œâ”€ ğŸ“¶ Mejora absoluta:          +12.80 puntos\n",
            "â”œâ”€ ğŸ“ˆ Mejora porcentual:        +91.4%\n",
            "â””â”€ ğŸ”¥ Estado: âœ… Superando baseline\n",
            "\n",
            "ğŸ’¾ ARCHIVOS GENERADOS:\n",
            "â””â”€ reinforce_doubledunk_results.json\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video guardado en: videos/doubledunk_reinforce.mp4\n",
            "Cargando mejor modelo desde checkpoints_doubledunk/best.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video guardado en: videos/doubledunk_best.mp4\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# EVALUACIÃ“N COMPLETA \n",
        "# ========================================\n",
        "\n",
        "def comprehensive_evaluation_reinforce():\n",
        "    \"\"\"EvaluaciÃ³n completa del modelo REINFORCE entrenado comparable con DDQN\"\"\"\n",
        "    \n",
        "    print(\"ğŸ“Š INICIANDO EVALUACIÃ“N COMPLETA REINFORCE\")\n",
        "    print(\"=\" * 55)\n",
        "    eval_start = time.time()\n",
        "    eval_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    \n",
        "    # Crear entorno de evaluaciÃ³n\n",
        "    eval_env = make_env(seed=SEED + 999, render_mode=None)\n",
        "    \n",
        "    # Cargar mejor modelo si existe\n",
        "    try:\n",
        "        best_model_path = os.path.join(cfg.checkpoint_dir, 'best.pt')\n",
        "        if os.path.exists(best_model_path):\n",
        "            print(\"ğŸ† Cargando mejor modelo entrenado...\")\n",
        "            best_agent = AtariActorCritic(in_channels=4, n_actions=eval_env.action_space.n).to(device)\n",
        "            ckpt_best = torch.load(best_model_path, map_location=device)\n",
        "            best_agent.load_state_dict(ckpt_best['model'])\n",
        "            eval_agent = best_agent\n",
        "            print(\"âœ… Mejor modelo cargado exitosamente\")\n",
        "        else:\n",
        "            print(\"âš ï¸  Usando modelo actual (no se encontrÃ³ best.pt)\")\n",
        "            eval_agent = agent\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Error cargando mejor modelo: {e}\")\n",
        "        print(\"âš ï¸  Usando modelo actual\")\n",
        "        eval_agent = agent\n",
        "    \n",
        "    # EVALUACIÃ“N PRINCIPAL (10 episodios para comparar con DDQN)\n",
        "    print(\"\\nğŸ¯ EvaluaciÃ³n oficial (10 episodios)...\")\n",
        "    eval_agent.eval()\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for ep in range(10):\n",
        "            obs, info = eval_env.reset(seed=SEED + 999 + ep)\n",
        "            total_reward = 0.0\n",
        "            episode_length = 0\n",
        "            \n",
        "            for t in range(cfg.max_steps_per_episode):\n",
        "                x = obs_to_tensor(obs)\n",
        "                logits, _ = eval_agent(x)\n",
        "                action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
        "                obs, reward, terminated, truncated, info = eval_env.step(action)\n",
        "                total_reward += float(reward)\n",
        "                episode_length += 1\n",
        "                \n",
        "                if terminated or truncated:\n",
        "                    break\n",
        "            \n",
        "            episode_rewards.append(total_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "            print(f\"   Episodio {ep+1}: Reward={total_reward:.2f}, Steps={episode_length}\")\n",
        "    \n",
        "    eval_agent.train()\n",
        "    eval_env.close()\n",
        "    \n",
        "    eval_end = time.time()\n",
        "    eval_duration = eval_end - eval_start\n",
        "    \n",
        "    # Calcular estadÃ­sticas\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "    min_reward = np.min(episode_rewards)\n",
        "    max_reward = np.max(episode_rewards)\n",
        "    mean_length = np.mean(episode_lengths)\n",
        "    std_length = np.std(episode_lengths)\n",
        "    \n",
        "    # Mejora vs baseline REINFORCE tÃ­pico\n",
        "    baseline_reinforce = -14.0\n",
        "    improvement = mean_reward - baseline_reinforce\n",
        "    improvement_pct = (improvement / abs(baseline_reinforce)) * 100 if baseline_reinforce != 0 else 0\n",
        "    \n",
        "    # REPORTE OFICIAL COMPARABLE CON DDQN\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ğŸ“‹ REPORTE OFICIAL - REINFORCE DOUBLEDUNK\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"ğŸ“… Fecha evaluaciÃ³n: {eval_timestamp}\")\n",
        "    print(f\"â±ï¸  Tiempo evaluaciÃ³n: {eval_duration:.2f}s\")\n",
        "    print(f\"ğŸ–¥ï¸  Dispositivo: {device}\")\n",
        "    print(f\"ğŸ¯ Algoritmo: REINFORCE con baseline (Actor-CrÃ­tico)\")\n",
        "    \n",
        "    print(f\"\\nğŸ¯ RESULTADOS PRINCIPALES:\")\n",
        "    print(f\"ğŸ“Š REINFORCE (10 episodios):    {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
        "    print(f\"ğŸ“ˆ Rango de recompensas:        [{min_reward:.2f}, {max_reward:.2f}]\")\n",
        "    print(f\"ğŸ“ DuraciÃ³n promedio:           {mean_length:.1f} Â± {std_length:.1f} pasos\")\n",
        "    \n",
        "    print(f\"\\nğŸ“Š ANÃLISIS DE RENDIMIENTO:\")\n",
        "    print(f\"â”œâ”€ ğŸ² Baseline REINFORCE:       {baseline_reinforce:.1f}\")\n",
        "    print(f\"â”œâ”€ ğŸ“¶ Mejora absoluta:          {improvement:+.2f} puntos\")\n",
        "    print(f\"â”œâ”€ ğŸ“ˆ Mejora porcentual:        {improvement_pct:+.1f}%\")\n",
        "    print(f\"â””â”€ ğŸ”¥ Estado: {'âœ… Superando baseline' if improvement > 0 else 'âš ï¸ Por debajo del baseline'}\")\n",
        "    \n",
        "    # Guardar resultados para comparaciÃ³n\n",
        "    results = {\n",
        "        'experiment_info': {\n",
        "            'timestamp': eval_timestamp,\n",
        "            'algorithm': 'REINFORCE',\n",
        "            'device': str(device),\n",
        "            'evaluation_episodes': 10\n",
        "        },\n",
        "        'evaluation_results': {\n",
        "            'reinforce_baseline': baseline_reinforce,\n",
        "            'reinforce_10_episodes': {\n",
        "                'mean': float(mean_reward),\n",
        "                'std': float(std_reward),\n",
        "                'min': float(min_reward),\n",
        "                'max': float(max_reward)\n",
        "            },\n",
        "            'episode_lengths': {\n",
        "                'mean': float(mean_length),\n",
        "                'std': float(std_length)\n",
        "            },\n",
        "            'improvement_absolute': float(improvement),\n",
        "            'improvement_percentage': float(improvement_pct)\n",
        "        },\n",
        "        'individual_episodes': [float(r) for r in episode_rewards]\n",
        "    }\n",
        "    \n",
        "    # Exportar resultados\n",
        "    with open('reinforce_doubledunk_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nğŸ’¾ ARCHIVOS GENERADOS:\")\n",
        "    print(f\"â””â”€ reinforce_doubledunk_results.json\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Ejecutar evaluaciÃ³n comparable\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "evaluation_results = comprehensive_evaluation_reinforce()\n",
        "\n",
        "# FunciÃ³n de grabaciÃ³n de video\n",
        "\n",
        "def record_video(model: nn.Module, filename: str = 'videos/doubledunk_reinforce.mp4', fps: int = 30, seed: int = SEED+2024):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    env = make_env(seed=seed, render_mode='rgb_array')\n",
        "    frames = []\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    with torch.no_grad():\n",
        "        for t in range(cfg.max_steps_per_episode):\n",
        "            x = obs_to_tensor(obs)\n",
        "            logits, _ = model(x)\n",
        "            action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
        "            frame = env.render()\n",
        "            frames.append(frame)\n",
        "            obs, r, terminated, truncated, info = env.step(action)\n",
        "            if terminated or truncated:\n",
        "                frame = env.render()\n",
        "                frames.append(frame)\n",
        "                break\n",
        "    env.close()\n",
        "    imageio.mimwrite(filename, frames, fps=fps, quality=8)\n",
        "    print('Video guardado en:', filename)\n",
        "\n",
        "# Grabar con agente actual\n",
        "record_video(agent, filename='videos/doubledunk_reinforce.mp4', fps=30)\n",
        "\n",
        "# Grabar con mejor modelo si existe\n",
        "best_path = os.path.join(cfg.checkpoint_dir, 'best.pt')\n",
        "if os.path.exists(best_path):\n",
        "    print('Cargando mejor modelo desde', best_path)\n",
        "    env_tmp = make_env(seed=SEED+3030)\n",
        "    n_actions_best = env_tmp.action_space.n\n",
        "    env_tmp.close()\n",
        "    best_agent = AtariActorCritic(in_channels=4, n_actions=n_actions_best).to(device)\n",
        "    ckpt_best = torch.load(best_path, map_location=device)\n",
        "    best_agent.load_state_dict(ckpt_best['model'])\n",
        "    record_video(best_agent, filename='videos/doubledunk_best.mp4', fps=30, seed=SEED+3030)\n",
        "else:\n",
        "    print('No se encontrÃ³ best.pt; aÃºn no hay mejor modelo guardado')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Referencias\n",
        "\n",
        "[1] Gym Docs DoubleDunk https://ale.farama.org/environments/double_dunk/\n",
        "\n",
        "[2] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4), 229-256.\n",
        "\n",
        "[3] Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n",
        "\n",
        "[4] PyTorch Documentation https://pytorch.org/docs/stable/index.html\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
