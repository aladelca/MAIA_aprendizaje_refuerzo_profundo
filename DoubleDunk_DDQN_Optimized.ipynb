{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🏀 DoubleDunk - DDQN Optimizado para MAIA\n",
        "\n",
        "**Reto de Aprendizaje por Refuerzo Profundo**  \n",
        "**Algoritmo:** Double Deep Q-Network (DDQN) con Epsilon Scheduler Cíclico  \n",
        "**Entorno:** ALE/DoubleDunk-v5 (Atari Basketball)  \n",
        "**Plataforma:** Google Colab con GPU optimizado para entrenamiento largo  \n",
        "\n",
        "\n",
        "\n",
        "## 📋 Información del Proyecto\n",
        "\n",
        "- **Curso:** Aprendizaje por Refuerzo Profundo - MAIA\n",
        "- **Problema:** Optimización de agente para juego DoubleDunk\n",
        "- **Método:** DDQN con mejoras específicas vs REINFORCE baseline\n",
        "- **Tiempo estimado:** 6-12 horas en GPU Colab (con interrupciones)\n",
        "\n",
        "\n",
        "⚠️ **IMPORTANTE:** Este notebook está diseñado para entrenamientos largos en GPU. Utiliza checkpoints automáticos para manejar desconexiones de Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📦 Instalación de Dependencias (Google Colab GPU)\n",
        "\n",
        "**Optimizado para sesiones GPU largas con gestión de memoria**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# INSTALACIÓN OPTIMIZADA PARA GPU COLAB\n",
        "# ========================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "print(\"🚀 Configurando entorno para DDQN DoubleDunk en GPU...\")\n",
        "\n",
        "# Verificar que estamos en Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"✅ Google Colab detectado\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"⚠️  Ejecutándose fuera de Colab\")\n",
        "\n",
        "# Instalaciones principales con verificación de errores\n",
        "packages = [\n",
        "    'stable-baselines3[extra]',\n",
        "    'ale-py',\n",
        "    'gymnasium[atari,accept-rom-license]',\n",
        "    'autorom',\n",
        "    'tensorboard',\n",
        "    'opencv-python',\n",
        "    'imageio[ffmpeg]',\n",
        "    'pandas',\n",
        "    'matplotlib',\n",
        "    'seaborn',\n",
        "    'tqdm'\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
        "        print(f\"✅ {package}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"❌ Error instalando {package}: {e}\")\n",
        "\n",
        "# Configurar ROMs de Atari\n",
        "try:\n",
        "    subprocess.run(['AutoROM', '--accept-license'], check=True, \n",
        "                  capture_output=True, text=True)\n",
        "    print(\"✅ ROMs de Atari configuradas\")\n",
        "except:\n",
        "    print(\"⚠️  ROMs ya configuradas o error menor\")\n",
        "\n",
        "# Configurar directorio de trabajo\n",
        "if IN_COLAB:\n",
        "    os.makedirs('/content/ddqn_doubledunk', exist_ok=True)\n",
        "    os.chdir('/content/ddqn_doubledunk')\n",
        "    print(\"📁 Directorio de trabajo: /content/ddqn_doubledunk\")\n",
        "\n",
        "print(\"✅ Configuración completa - Listo para GPU\")\n",
        "print(\"🎯 Iniciando importaciones...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# IMPORTACIONES Y CONFIGURACIÓN GPU\n",
        "# ========================================\n",
        "\n",
        "# Librerías RL y utilidades\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.logger import configure\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
        "from stable_baselines3.common.vec_env import VecMonitor, VecFrameStack, VecVideoRecorder\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "\n",
        "import gymnasium\n",
        "import ale_py\n",
        "from collections import deque\n",
        "import cv2\n",
        "\n",
        "# Registrar entornos ALE\n",
        "gymnasium.register_envs(ale_py)\n",
        "\n",
        "# Librerías básicas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "import glob\n",
        "import base64\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML, display, clear_output\n",
        "\n",
        "# PyTorch para GPU\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "print(\"📚 Importaciones completadas\")\n",
        "print(f\"🔢 Stable Baselines3: {stable_baselines3.__version__}\")\n",
        "print(f\"🔥 PyTorch: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🖥️ Detección y Configuración de GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# DETECCIÓN AUTOMÁTICA DE GPU COLAB\n",
        "# ========================================\n",
        "\n",
        "import platform\n",
        "import psutil\n",
        "\n",
        "def detect_colab_hardware():\n",
        "    \"\"\"\n",
        "    Detecta y configura el hardware disponible en Google Colab\n",
        "    Optimizado para GPU T4, P100, V100, A100\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"🖥️  DETECCIÓN DE HARDWARE GOOGLE COLAB\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Información del sistema\n",
        "    print(f\"Sistema: {platform.system()} {platform.release()}\")\n",
        "    print(f\"CPU: {platform.processor()}\")\n",
        "    print(f\"RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    \n",
        "    device_info = {}\n",
        "    \n",
        "    # Verificar CUDA (GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        gpu_compute = torch.cuda.get_device_properties(0).major\n",
        "        \n",
        "        device_info = {\n",
        "            'device': device,\n",
        "            'name': gpu_name,\n",
        "            'memory_gb': gpu_memory,\n",
        "            'compute_capability': gpu_compute,\n",
        "            'type': 'GPU_CUDA',\n",
        "            'recommended': True\n",
        "        }\n",
        "        \n",
        "        print(f\"✅ GPU DETECTADA: {gpu_name}\")\n",
        "        print(f\"   💾 Memoria GPU: {gpu_memory:.1f} GB\")\n",
        "        print(f\"   🔧 Compute Capability: {gpu_compute}.x\")\n",
        "        \n",
        "        # Configuraciones específicas por GPU\n",
        "        if 'T4' in gpu_name:\n",
        "            print(f\"   🎯 Tesla T4 detectada - Configuración optimizada\")\n",
        "            batch_size_factor = 1.0\n",
        "        elif 'P100' in gpu_name:\n",
        "            print(f\"   🚀 Tesla P100 detectada - Configuración de alto rendimiento\")\n",
        "            batch_size_factor = 1.2\n",
        "        elif 'V100' in gpu_name:\n",
        "            print(f\"   💎 Tesla V100 detectada - Configuración premium\")\n",
        "            batch_size_factor = 1.5\n",
        "        elif 'A100' in gpu_name:\n",
        "            print(f\"   🌟 Tesla A100 detectada - Configuración máxima\")\n",
        "            batch_size_factor = 2.0\n",
        "        else:\n",
        "            print(f\"   🔧 GPU genérica detectada - Configuración estándar\")\n",
        "            batch_size_factor = 1.0\n",
        "            \n",
        "        device_info['batch_size_factor'] = batch_size_factor\n",
        "        \n",
        "    else:\n",
        "        # Fallback a CPU\n",
        "        device = torch.device('cpu')\n",
        "        device_info = {\n",
        "            'device': device,\n",
        "            'name': 'CPU',\n",
        "            'type': 'CPU',\n",
        "            'recommended': False,\n",
        "            'batch_size_factor': 0.5\n",
        "        }\n",
        "        print(f\"⚠️  SOLO CPU DISPONIBLE\")\n",
        "        print(f\"   ❌ No se detectó GPU - El entrenamiento será MUY lento\")\n",
        "        print(f\"   💡 Asegúrate de activar GPU en Colab: Runtime > Change runtime type > GPU\")\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "    print(f\"DISPOSITIVO FINAL: {device_info['name']} ({device_info['device']})\")\n",
        "    \n",
        "    if device_info['recommended']:\n",
        "        print(f\"✅ Configuración óptima para entrenamiento largo\")\n",
        "    else:\n",
        "        print(f\"⚠️  ADVERTENCIA: Sin GPU el entrenamiento puede tomar días\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return device_info['device'], device_info\n",
        "\n",
        "# Detectar hardware\n",
        "DEVICE, DEVICE_INFO = detect_colab_hardware()\n",
        "\n",
        "# Configuraciones de PyTorch para GPU\n",
        "if DEVICE.type == 'cuda':\n",
        "    print(\"🚀 Aplicando optimizaciones CUDA...\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    # Limpiar caché de GPU\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"   📊 Memoria GPU inicial: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"💻 Configurando para CPU...\")\n",
        "    torch.set_num_threads(4)  # Limitar threads en Colab\n",
        "\n",
        "print(f\"\\n🎯 Sistema configurado para: {DEVICE_INFO['name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 💾 Sistema de Checkpoints Inteligente\n",
        "\n",
        "**Gestión automática de checkpoints para entrenamientos largos con interrupciones**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# SISTEMA DE CHECKPOINTS INTELIGENTE\n",
        "# ========================================\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"\n",
        "    Gestor de checkpoints optimizado para entrenamientos largos en Colab\n",
        "    Maneja desconexiones automáticamente y preserva todo el estado\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, checkpoint_dir=\"./checkpoints\", backup_every=50000):\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.backup_every = backup_every\n",
        "        self.session_start = datetime.now()\n",
        "        \n",
        "        # Crear directorios\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "        os.makedirs(f\"{checkpoint_dir}/models\", exist_ok=True)\n",
        "        os.makedirs(f\"{checkpoint_dir}/training_state\", exist_ok=True)\n",
        "        \n",
        "        print(f\"💾 Checkpoint Manager inicializado\")\n",
        "        print(f\"   📁 Directorio: {checkpoint_dir}\")\n",
        "        print(f\"   ⏰ Backup cada: {backup_every:,} timesteps\")\n",
        "    \n",
        "    def _convert_to_json_serializable(self, obj):\n",
        "        \"\"\"\n",
        "        Convierte objetos a tipos serializables por JSON\n",
        "        \"\"\"\n",
        "        if isinstance(obj, dict):\n",
        "            return {key: self._convert_to_json_serializable(value) for key, value in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [self._convert_to_json_serializable(item) for item in obj]\n",
        "        elif hasattr(obj, 'item'):  # numpy scalars\n",
        "            return obj.item()\n",
        "        elif hasattr(obj, 'tolist'):  # numpy arrays\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, (np.int32, np.int64, np.float32, np.float64)):\n",
        "            return float(obj) if 'float' in str(type(obj)) else int(obj)\n",
        "        else:\n",
        "            return obj\n",
        "    \n",
        "    def save_checkpoint(self, model, timestep, episode_rewards, metadata=None):\n",
        "        \"\"\"\n",
        "        Guarda checkpoint completo del estado del entrenamiento\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convertir episode_rewards a float estándar para JSON serialization\n",
        "            safe_episode_rewards = [float(reward) for reward in episode_rewards]\n",
        "            \n",
        "            # Convertir metadata recursivamente\n",
        "            safe_metadata = self._convert_to_json_serializable(metadata or {})\n",
        "            \n",
        "            checkpoint_data = {\n",
        "                'timestep': int(timestep),\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'session_duration': str(datetime.now() - self.session_start),\n",
        "                'episode_rewards': safe_episode_rewards,\n",
        "                'device': str(DEVICE),\n",
        "                'metadata': safe_metadata\n",
        "            }\n",
        "            \n",
        "            # Guardar modelo\n",
        "            model_path = f\"{self.checkpoint_dir}/models/ddqn_checkpoint_{timestep}.zip\"\n",
        "            model.save(model_path)\n",
        "            \n",
        "            # Guardar estado de entrenamiento\n",
        "            state_path = f\"{self.checkpoint_dir}/training_state/state_{timestep}.json\"\n",
        "            with open(state_path, 'w') as f:\n",
        "                json.dump(checkpoint_data, f, indent=2)\n",
        "            \n",
        "            print(f\"💾 Checkpoint guardado: timestep {timestep:,}\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error guardando checkpoint: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def list_checkpoints(self):\n",
        "        \"\"\"Lista todos los checkpoints disponibles\"\"\"\n",
        "        checkpoints = []\n",
        "        model_files = glob.glob(f\"{self.checkpoint_dir}/models/ddqn_checkpoint_*.zip\")\n",
        "        \n",
        "        for model_file in model_files:\n",
        "            try:\n",
        "                timestep = int(model_file.split('_')[-1].split('.')[0])\n",
        "                state_file = f\"{self.checkpoint_dir}/training_state/state_{timestep}.json\"\n",
        "                \n",
        "                if os.path.exists(state_file):\n",
        "                    with open(state_file, 'r') as f:\n",
        "                        state_data = json.load(f)\n",
        "                    \n",
        "                    checkpoints.append({\n",
        "                        'timestep': timestep,\n",
        "                        'model_path': model_file,\n",
        "                        'state_path': state_file,\n",
        "                        'timestamp': state_data.get('timestamp', 'Unknown'),\n",
        "                        'episodes': len(state_data.get('episode_rewards', [])),\n",
        "                        'last_reward': state_data.get('episode_rewards', [0])[-1] if state_data.get('episode_rewards') else 0\n",
        "                    })\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        return sorted(checkpoints, key=lambda x: x['timestep'])\n",
        "    \n",
        "    def get_latest_checkpoint(self):\n",
        "        \"\"\"Obtiene el checkpoint más reciente\"\"\"\n",
        "        checkpoints = self.list_checkpoints()\n",
        "        return checkpoints[-1] if checkpoints else None\n",
        "    \n",
        "    def load_checkpoint(self, timestep=None):\n",
        "        \"\"\"Carga un checkpoint específico o el más reciente\"\"\"\n",
        "        if timestep is None:\n",
        "            checkpoint = self.get_latest_checkpoint()\n",
        "        else:\n",
        "            checkpoints = self.list_checkpoints()\n",
        "            checkpoint = next((c for c in checkpoints if c['timestep'] == timestep), None)\n",
        "        \n",
        "        if checkpoint is None:\n",
        "            return None, None\n",
        "        \n",
        "        try:\n",
        "            # Cargar estado\n",
        "            with open(checkpoint['state_path'], 'r') as f:\n",
        "                state_data = json.load(f)\n",
        "            \n",
        "            print(f\"📂 Cargando checkpoint: timestep {checkpoint['timestep']:,}\")\n",
        "            print(f\"   📅 Fecha: {checkpoint['timestamp']}\")\n",
        "            print(f\"   📊 Episodios: {checkpoint['episodes']}\")\n",
        "            print(f\"   🎯 Última recompensa: {checkpoint['last_reward']:.2f}\")\n",
        "            \n",
        "            return checkpoint['model_path'], state_data\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error cargando checkpoint: {e}\")\n",
        "            return None, None\n",
        "\n",
        "# Inicializar gestor de checkpoints\n",
        "checkpoint_manager = CheckpointManager(\n",
        "    checkpoint_dir=\"./checkpoints_doubledunk\",\n",
        "    backup_every=50000\n",
        ")\n",
        "\n",
        "# Verificar checkpoints existentes\n",
        "existing_checkpoints = checkpoint_manager.list_checkpoints()\n",
        "if existing_checkpoints:\n",
        "    print(f\"\\n📋 Checkpoints existentes encontrados: {len(existing_checkpoints)}\")\n",
        "    for cp in existing_checkpoints[-3:]:  # Mostrar los 3 más recientes\n",
        "        print(f\"   ⏰ {cp['timestep']:,} steps - {cp['timestamp'][:19]} - Reward: {cp['last_reward']:.2f}\")\n",
        "else:\n",
        "    print(f\"\\n📋 No se encontraron checkpoints - Entrenamiento desde cero\")\n",
        "\n",
        "print(f\"\\n✅ Sistema de checkpoints configurado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 Algoritmo DDQN Optimizado + Callbacks + Trainer\n",
        "\n",
        "**Implementación completa con sistema de checkpoints integrado**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# IMPLEMENTACIÓN COMPLETA DDQN + SISTEMA CHECKPOINT\n",
        "# ========================================\n",
        "\n",
        "# Callbacks optimizados\n",
        "class RewardLoggerCallback(BaseCallback):\n",
        "    def __init__(self, path_backup: str = None, checkpoint_manager=None, usar_media: bool = True, ventana_para_media: int = 30):\n",
        "        super().__init__(verbose=True)\n",
        "        self.episode_rewards = []\n",
        "        self.best_score = -np.inf\n",
        "        self.path_backup = path_backup\n",
        "        self.checkpoint_manager = checkpoint_manager\n",
        "        self.usar_media = usar_media\n",
        "        self.ventana_para_media = ventana_para_media\n",
        "        self.last_checkpoint_step = 0\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        infos = self.locals.get(\"infos\", [])\n",
        "        if not infos:\n",
        "            return True\n",
        "\n",
        "        for info in infos:\n",
        "            ep = info.get(\"episode\")\n",
        "            if ep is None:\n",
        "                continue\n",
        "            r = ep.get(\"r\")\n",
        "            self.episode_rewards.append(r)\n",
        "\n",
        "            # Checkpoint automático cada 50k steps\n",
        "            if (self.checkpoint_manager and \n",
        "                self.num_timesteps - self.last_checkpoint_step >= self.checkpoint_manager.backup_every):\n",
        "                self.checkpoint_manager.save_checkpoint(\n",
        "                    self.model, self.num_timesteps, self.episode_rewards,\n",
        "                    {'best_score': self.best_score, 'total_episodes': len(self.episode_rewards)}\n",
        "                )\n",
        "                self.last_checkpoint_step = self.num_timesteps\n",
        "\n",
        "            # Evaluar mejor modelo\n",
        "            if self.usar_media:\n",
        "                if len(self.episode_rewards) >= self.ventana_para_media:\n",
        "                    score = float(np.mean(self.episode_rewards[-self.ventana_para_media:]))\n",
        "                else:\n",
        "                    score = float(np.mean(self.episode_rewards))\n",
        "            else:\n",
        "                score = float(r)\n",
        "\n",
        "            if score > self.best_score:\n",
        "                self.best_score = score\n",
        "                if self.path_backup:\n",
        "                    dirname = os.path.dirname(self.path_backup)\n",
        "                    if dirname and not os.path.exists(dirname):\n",
        "                        os.makedirs(dirname, exist_ok=True)\n",
        "                    try:\n",
        "                        self.model.save(self.path_backup)\n",
        "                        print(f\"🏆 Nuevo mejor score {score:.2f} -> Guardado en: {self.path_backup}.zip\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"❌ Error guardando mejor modelo: {e}\")\n",
        "        return True\n",
        "\n",
        "class EpsilonSchedulerCallback(BaseCallback):\n",
        "    def __init__(self, schedule_fn, total_timesteps: int, verbose: int = 1):\n",
        "        super().__init__(verbose)\n",
        "        self.schedule_fn = schedule_fn\n",
        "        self.total_timesteps = int(total_timesteps)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        t = min(self.num_timesteps, self.total_timesteps)\n",
        "        progress = t / float(self.total_timesteps)\n",
        "        new_eps = float(self.schedule_fn(progress))\n",
        "        self.model.exploration_rate = new_eps\n",
        "\n",
        "        if ((self.num_timesteps - 1) % 10_000 == 0):\n",
        "            try:\n",
        "                self.logger.record(\"train/epsilon\", new_eps)\n",
        "                lr = float(self.model.policy.optimizer.param_groups[0][\"lr\"])\n",
        "                self.logger.record(\"train/learning_rate\", lr)\n",
        "                if self.verbose:\n",
        "                    print(f\"📊 Step {self.num_timesteps:,} | ε={new_eps:.4f} | LR={lr:.6f}\")\n",
        "            except Exception:\n",
        "                pass\n",
        "        return True\n",
        "\n",
        "# Epsilon scheduler\n",
        "def crear_eps_scheduler(val_inicial: float, val_min: float, n_ciclos: int, degree: int):\n",
        "    def scheduler(progress: float) -> float:\n",
        "        envelope = (1.0 - progress**degree)\n",
        "        cos_term = 0.5 * (1.0 + np.cos(2 * np.pi * n_ciclos * progress))\n",
        "        val = val_inicial * envelope * cos_term\n",
        "        return float(max(val, val_min))\n",
        "    return scheduler\n",
        "\n",
        "# DDQN implementación\n",
        "class OptimizedDoubleDQN(DQN):\n",
        "    def train(self, gradient_steps: int, batch_size: int = 32) -> None:\n",
        "        self.policy.set_training_mode(True)\n",
        "        self._update_learning_rate(self.policy.optimizer)\n",
        "        \n",
        "        losses = []\n",
        "        for _ in range(gradient_steps):\n",
        "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n",
        "            obs = replay_data.observations\n",
        "            next_obs = replay_data.next_observations\n",
        "            actions = replay_data.actions\n",
        "            rewards = replay_data.rewards\n",
        "            dones = replay_data.dones\n",
        "\n",
        "            if rewards.dim() == 1:\n",
        "                rewards = rewards.unsqueeze(1)\n",
        "            if dones.dim() == 1:\n",
        "                dones = dones.unsqueeze(1)\n",
        "            if actions.dim() == 1:\n",
        "                actions = actions.unsqueeze(1)\n",
        "\n",
        "            device = self.device\n",
        "            obs = obs.to(device)\n",
        "            next_obs = next_obs.to(device)\n",
        "            actions = actions.to(device)\n",
        "            rewards = rewards.to(device).float()\n",
        "            dones = dones.to(device).float()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                q_next_online = self.q_net(next_obs)\n",
        "                next_actions_online = q_next_online.argmax(dim=1, keepdim=True)\n",
        "                q_next_target = self.q_net_target(next_obs)\n",
        "                q_next_target_selected = torch.gather(q_next_target, dim=1, index=next_actions_online)\n",
        "                target_q_values = rewards + (1.0 - dones) * (self.gamma * q_next_target_selected)\n",
        "\n",
        "            q_values_all = self.q_net(obs)\n",
        "            current_q_values = torch.gather(q_values_all, dim=1, index=actions.long())\n",
        "            \n",
        "            loss = F.huber_loss(current_q_values, target_q_values, delta=1.0)\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "            self.policy.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm * 0.5)\n",
        "            self.policy.optimizer.step()\n",
        "\n",
        "        self._n_updates += gradient_steps\n",
        "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
        "        self.logger.record(\"train/loss\", np.mean(losses))\n",
        "\n",
        "# Configuración optimizada para GPU\n",
        "DOUBLEDUNK_GPU_CONFIG = {\n",
        "    'total_timesteps': 5_000_000,\n",
        "    'n_ciclos_eps': 12,\n",
        "    'eps_inicial': 0.95,\n",
        "    'eps_min': 0.01,\n",
        "    'scheduler_degree': 1.5,\n",
        "    'ventana_media': 30,\n",
        "    'learning_rate': 2.5e-4 if DEVICE_INFO['recommended'] else 1e-4,\n",
        "    'buffer_size': 200_000,\n",
        "    'batch_size': int(32 * DEVICE_INFO.get('batch_size_factor', 1.0)),\n",
        "    'target_update': 8_000,\n",
        "    'train_freq': 4,\n",
        "    'learning_starts': 20_000,\n",
        "    'gamma': 0.995,\n",
        "}\n",
        "\n",
        "print(\"⚙️ Configuración GPU optimizada:\")\n",
        "for key, value in DOUBLEDUNK_GPU_CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "def create_optimized_gpu_model(env):\n",
        "    return OptimizedDoubleDQN(\n",
        "        \"CnnPolicy\",\n",
        "        env,\n",
        "        learning_rate=DOUBLEDUNK_GPU_CONFIG['learning_rate'],\n",
        "        buffer_size=DOUBLEDUNK_GPU_CONFIG['buffer_size'],\n",
        "        learning_starts=DOUBLEDUNK_GPU_CONFIG['learning_starts'],\n",
        "        batch_size=DOUBLEDUNK_GPU_CONFIG['batch_size'],\n",
        "        gradient_steps=1,\n",
        "        gamma=DOUBLEDUNK_GPU_CONFIG['gamma'],\n",
        "        train_freq=DOUBLEDUNK_GPU_CONFIG['train_freq'],\n",
        "        target_update_interval=DOUBLEDUNK_GPU_CONFIG['target_update'],\n",
        "        policy_kwargs=dict(\n",
        "            net_arch=[512, 256] if DEVICE_INFO['recommended'] else [256, 128],\n",
        "            activation_fn=torch.nn.ReLU,\n",
        "            normalize_images=True\n",
        "        ),\n",
        "        tensorboard_log=\"./logs_doubledunk\",\n",
        "        verbose=1,\n",
        "        device=DEVICE,\n",
        "    )\n",
        "\n",
        "print(\"✅ Algoritmo DDQN y callbacks configurados\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Entrenamiento con Checkpoints Automáticos\n",
        "\n",
        "**Entrenamiento largo optimizado para GPU con sistema de recuperación**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ENTRENAMIENTO CON SISTEMA DE CHECKPOINTS\n",
        "# ========================================\n",
        "\n",
        "def setup_training_with_checkpoints():\n",
        "    \"\"\"Configura entrenamiento con capacidad de reanudación\"\"\"\n",
        "    \n",
        "    # Verificar si hay checkpoint existente\n",
        "    model_path, state_data = checkpoint_manager.load_checkpoint()\n",
        "    \n",
        "    if model_path and state_data:\n",
        "        # Reanudar desde checkpoint\n",
        "        print(\"🔄 REANUDANDO ENTRENAMIENTO DESDE CHECKPOINT\")\n",
        "        print(f\"   📅 Última sesión: {state_data['timestamp']}\")\n",
        "        print(f\"   📊 Timesteps completados: {state_data['timestep']:,}\")\n",
        "        print(f\"   🎯 Episodios: {len(state_data['episode_rewards'])}\")\n",
        "        \n",
        "        # Crear entorno\n",
        "        env = make_atari_env(\"ALE/DoubleDunk-v5\", n_envs=1, seed=0)\n",
        "        env = VecFrameStack(env, n_stack=4)\n",
        "        env = VecMonitor(env, filename=\"./logs_doubledunk/monitor.csv\")\n",
        "        \n",
        "        # Cargar modelo existente\n",
        "        model = OptimizedDoubleDQN.load(model_path, env=env, device=DEVICE)\n",
        "        \n",
        "        # Configurar callbacks con estado existente\n",
        "        scheduler = crear_eps_scheduler(\n",
        "            DOUBLEDUNK_GPU_CONFIG['eps_inicial'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['eps_min'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['n_ciclos_eps'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['scheduler_degree']\n",
        "        )\n",
        "        \n",
        "        eps_cb = EpsilonSchedulerCallback(\n",
        "            lambda p: scheduler(p), \n",
        "            total_timesteps=DOUBLEDUNK_GPU_CONFIG['total_timesteps']\n",
        "        )\n",
        "        \n",
        "        reward_cb = RewardLoggerCallback(\n",
        "            path_backup=\"./checkpoints_doubledunk/best_model\",\n",
        "            checkpoint_manager=checkpoint_manager,\n",
        "            ventana_para_media=DOUBLEDUNK_GPU_CONFIG['ventana_media']\n",
        "        )\n",
        "        \n",
        "        # Restaurar estado de rewards\n",
        "        reward_cb.episode_rewards = state_data['episode_rewards']\n",
        "        reward_cb.best_score = state_data['metadata'].get('best_score', -np.inf)\n",
        "        \n",
        "        # Calcular timesteps restantes\n",
        "        remaining_timesteps = DOUBLEDUNK_GPU_CONFIG['total_timesteps'] - state_data['timestep']\n",
        "        \n",
        "        return model, env, [reward_cb, eps_cb], remaining_timesteps\n",
        "        \n",
        "    else:\n",
        "        # Entrenamiento desde cero\n",
        "        print(\"🆕 INICIANDO ENTRENAMIENTO DESDE CERO\")\n",
        "        \n",
        "        # Crear entorno\n",
        "        env = make_atari_env(\"ALE/DoubleDunk-v5\", n_envs=1, seed=0)\n",
        "        env = VecFrameStack(env, n_stack=4)\n",
        "        env = VecMonitor(env, filename=\"./logs_doubledunk/monitor.csv\")\n",
        "        \n",
        "        # Crear modelo nuevo\n",
        "        model = create_optimized_gpu_model(env)\n",
        "        \n",
        "        # Configurar logger\n",
        "        new_logger = configure(\"./logs_doubledunk\", [\"csv\", \"tensorboard\"])\n",
        "        model.set_logger(new_logger)\n",
        "        \n",
        "        # Configurar callbacks\n",
        "        scheduler = crear_eps_scheduler(\n",
        "            DOUBLEDUNK_GPU_CONFIG['eps_inicial'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['eps_min'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['n_ciclos_eps'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['scheduler_degree']\n",
        "        )\n",
        "        \n",
        "        eps_cb = EpsilonSchedulerCallback(\n",
        "            lambda p: scheduler(p), \n",
        "            total_timesteps=DOUBLEDUNK_GPU_CONFIG['total_timesteps']\n",
        "        )\n",
        "        \n",
        "        reward_cb = RewardLoggerCallback(\n",
        "            path_backup=\"./checkpoints_doubledunk/best_model\",\n",
        "            checkpoint_manager=checkpoint_manager,\n",
        "            ventana_para_media=DOUBLEDUNK_GPU_CONFIG['ventana_media']\n",
        "        )\n",
        "        \n",
        "        return model, env, [reward_cb, eps_cb], DOUBLEDUNK_GPU_CONFIG['total_timesteps']\n",
        "\n",
        "# Configurar entrenamiento\n",
        "model, env, callbacks, timesteps_to_train = setup_training_with_checkpoints()\n",
        "callback_list = CallbackList(callbacks)\n",
        "\n",
        "print(f\"🎯 Configuración de entrenamiento completada\")\n",
        "print(f\"   🖥️  Dispositivo: {DEVICE_INFO['name']}\")\n",
        "print(f\"   ⏱️  Timesteps a entrenar: {timesteps_to_train:,}\")\n",
        "print(f\"   🎮 Batch size: {DOUBLEDUNK_GPU_CONFIG['batch_size']}\")\n",
        "\n",
        "# EJECUTAR ENTRENAMIENTO\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"🚀 INICIANDO ENTRENAMIENTO DDQN EN {DEVICE_INFO['name'].upper()}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "training_start = time.time()\n",
        "\n",
        "try:\n",
        "    model.learn(\n",
        "        total_timesteps=timesteps_to_train,\n",
        "        log_interval=5,\n",
        "        callback=callback_list,\n",
        "        progress_bar=True\n",
        "    )\n",
        "    \n",
        "    training_end = time.time()\n",
        "    training_duration = training_end - training_start\n",
        "    \n",
        "    print(f\"\\n✅ ENTRENAMIENTO COMPLETADO\")\n",
        "    print(f\"   ⏱️  Duración: {training_duration/3600:.2f} horas\")\n",
        "    print(f\"   🎯 Total episodes: {len(callbacks[0].episode_rewards)}\")\n",
        "    \n",
        "    # Guardar modelo final\n",
        "    final_model_path = \"./DDQN_DoubleDunk_GPU_Final\"\n",
        "    model.save(final_model_path)\n",
        "    print(f\"   💾 Modelo final guardado: {final_model_path}\")\n",
        "    \n",
        "    # Checkpoint final\n",
        "    checkpoint_manager.save_checkpoint(\n",
        "        model, \n",
        "        DOUBLEDUNK_GPU_CONFIG['total_timesteps'], \n",
        "        callbacks[0].episode_rewards,\n",
        "        {\n",
        "            'training_completed': True,\n",
        "            'total_duration_hours': training_duration/3600,\n",
        "            'final_best_score': callbacks[0].best_score\n",
        "        }\n",
        "    )\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(f\"\\n⚠️  Entrenamiento interrumpido por usuario\")\n",
        "    print(f\"   💾 El progreso se ha guardado automáticamente en checkpoints\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error durante entrenamiento: {e}\")\n",
        "    print(f\"   💾 Revisando último checkpoint disponible...\")\n",
        "\n",
        "print(f\"\\n🎯 Progresando a evaluación...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Evaluación Completa y Generación de Resultados\n",
        "\n",
        "**Evaluación académica con todas las evidencias requeridas**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# EVALUACIÓN COMPLETA PARA REPORTE ACADÉMICO\n",
        "# ========================================\n",
        "\n",
        "def comprehensive_evaluation():\n",
        "    \"\"\"Evaluación completa del modelo entrenado\"\"\"\n",
        "    \n",
        "    print(\"📊 INICIANDO EVALUACIÓN COMPLETA\")\n",
        "    eval_start = time.time()\n",
        "    eval_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    \n",
        "    # Crear entorno de evaluación\n",
        "    eval_env = make_atari_env(\"ALE/DoubleDunk-v5\", n_envs=1, seed=42)\n",
        "    eval_env = VecFrameStack(eval_env, n_stack=4)\n",
        "    \n",
        "    # Cargar mejor modelo\n",
        "    try:\n",
        "        best_model = OptimizedDoubleDQN.load(\"./checkpoints_doubledunk/best_model\", device=DEVICE)\n",
        "        print(\"✅ Mejor modelo cargado exitosamente\")\n",
        "    except:\n",
        "        try:\n",
        "            best_model = model  # Usar modelo en memoria si falla la carga\n",
        "            print(\"⚠️  Usando modelo en memoria\")\n",
        "        except:\n",
        "            print(\"❌ No se pudo cargar modelo para evaluación\")\n",
        "            return None\n",
        "    \n",
        "    # 1. EVALUACIÓN REQUERIDA (10 episodios)\n",
        "    print(\"🏆 Evaluación oficial (10 episodios)...\")\n",
        "    mean_10, std_10 = evaluate_policy(best_model, eval_env, n_eval_episodes=10, deterministic=False, render=False)\n",
        "    \n",
        "    # 2. EVALUACIÓN EXTENDIDA (20 episodios)\n",
        "    print(\"📈 Evaluación extendida (20 episodios)...\")\n",
        "    mean_20, std_20 = evaluate_policy(best_model, eval_env, n_eval_episodes=20, deterministic=False, render=False)\n",
        "    \n",
        "    eval_end = time.time()\n",
        "    eval_duration = eval_end - eval_start\n",
        "    \n",
        "    # Obtener estadísticas de entrenamiento\n",
        "    try:\n",
        "        episode_rewards = callbacks[0].episode_rewards\n",
        "        total_episodes = len(episode_rewards)\n",
        "        best_score = callbacks[0].best_score\n",
        "        \n",
        "        if total_episodes > 0:\n",
        "            final_100 = episode_rewards[-100:] if total_episodes >= 100 else episode_rewards\n",
        "            mean_last_100 = np.mean(final_100)\n",
        "            best_episode = np.max(episode_rewards)\n",
        "            worst_episode = np.min(episode_rewards)\n",
        "        else:\n",
        "            mean_last_100 = 0\n",
        "            best_episode = 0\n",
        "            worst_episode = 0\n",
        "    except:\n",
        "        episode_rewards = []\n",
        "        total_episodes = 0\n",
        "        best_score = 0\n",
        "        mean_last_100 = 0\n",
        "        best_episode = 0\n",
        "        worst_episode = 0\n",
        "    \n",
        "    # REPORTE OFICIAL\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"📋 REPORTE OFICIAL - DDQN DOUBLEDUNK GPU COLAB\")\n",
        "    \n",
        "    print(f\"📅 Fecha evaluación: {eval_timestamp}\")\n",
        "    print(f\"⏱️  Tiempo evaluación: {eval_duration:.2f}s\")\n",
        "    print(f\"🖥️  Dispositivo: {DEVICE_INFO['name']} ({DEVICE})\")\n",
        "    print(f\"📊 Episodes entrenados: {total_episodes:,}\")\n",
        "    \n",
        "    \n",
        "    print(f\"🎯 RESULTADOS PRINCIPALES:\")\n",
        "    \n",
        "    print(f\"├─ 🏆 DDQN (10 episodios):    {mean_10:.2f} ± {std_10:.2f}\")\n",
        "    print(f\"└─ 📈 DDQN (20 episodios):    {mean_20:.2f} ± {std_20:.2f}\")\n",
        "    \n",
        "    improvement = mean_20 - (-14.0)\n",
        "    improvement_pct = (improvement / abs(-14.0)) * 100\n",
        "    \n",
        "    print(f\"📊 ANÁLISIS DE MEJORA:\")\n",
        "    print(f\"├─ 📶 Mejora absoluta:        {improvement:+.2f} puntos\")\n",
        "    print(f\"├─ 📈 Mejora porcentual:      {improvement_pct:+.1f}%\")\n",
        "    print(f\"└─ 🏅 Mejor score entrenamiento: {best_score:.2f}\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Guardar resultados estructurados\n",
        "    results = {\n",
        "        'experiment_info': {\n",
        "            'timestamp': eval_timestamp,\n",
        "            'device': str(DEVICE),\n",
        "            'device_name': DEVICE_INFO['name'],\n",
        "            'total_training_episodes': total_episodes\n",
        "        },\n",
        "        'evaluation_results': {\n",
        "            'reinforce_baseline': -14.0,\n",
        "            'ddqn_10_episodes': {'mean': float(mean_10), 'std': float(std_10)},\n",
        "            'ddqn_20_episodes': {'mean': float(mean_20), 'std': float(std_20)},\n",
        "            'improvement_absolute': float(improvement),\n",
        "            'improvement_percentage': float(improvement_pct),\n",
        "            'best_training_score': float(best_score)\n",
        "        },\n",
        "        'training_stats': {\n",
        "            'mean_last_100_episodes': float(mean_last_100),\n",
        "            'best_episode': float(best_episode),\n",
        "            'worst_episode': float(worst_episode)\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Exportar resultados\n",
        "    with open('ddqn_doubledunk_gpu_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    \n",
        "    # CSV para análisis\n",
        "    eval_df = pd.DataFrame({\n",
        "        'Model': ['REINFORCE_baseline', 'DDQN_10eps', 'DDQN_20eps'],\n",
        "        'Mean_Score': [-14.0, mean_10, mean_20],\n",
        "        'Std_Score': [0.0, std_10, std_20],\n",
        "        'Episodes': [10, 10, 20]\n",
        "    })\n",
        "    eval_df.to_csv('ddqn_doubledunk_gpu_evaluation.csv', index=False)\n",
        "    \n",
        "    print(f\"💾 ARCHIVOS GENERADOS:\")\n",
        "    print(f\"├─ ddqn_doubledunk_gpu_results.json\")\n",
        "    print(f\"└─ ddqn_doubledunk_gpu_evaluation.csv\")\n",
        "    \n",
        "    eval_env.close()\n",
        "    return results\n",
        "\n",
        "# Ejecutar evaluación\n",
        "evaluation_results = comprehensive_evaluation()\n",
        "\n",
        "# Mostrar progreso de entrenamiento si está disponible\n",
        "try:\n",
        "    if len(callbacks[0].episode_rewards) > 10:\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        rewards = callbacks[0].episode_rewards\n",
        "        plt.plot(rewards, alpha=0.6, color='blue')\n",
        "        if len(rewards) > 50:\n",
        "            window = 50\n",
        "            moving_avg = pd.Series(rewards).rolling(window=window).mean()\n",
        "            plt.plot(moving_avg, color='red', linewidth=2, label=f'Media móvil ({window})')\n",
        "        plt.axhline(y=-14.0, color='orange', linestyle='--', label='REINFORCE baseline')\n",
        "        plt.xlabel('Episodios')\n",
        "        plt.ylabel('Recompensa')\n",
        "        plt.title('Evolución del Entrenamiento DDQN')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.hist(rewards, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
        "        plt.axvline(x=-14.0, color='orange', linestyle='--', label='REINFORCE baseline')\n",
        "        plt.xlabel('Recompensa')\n",
        "        plt.ylabel('Frecuencia')\n",
        "        plt.title('Distribución de Recompensas')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"📈 Gráficas de entrenamiento generadas\")\n",
        "except:\n",
        "    print(\"⚠️  No se pudieron generar gráficas (entrenamiento muy corto o datos no disponibles)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# FUNCIÓN DE GENERACIÓN DE VIDEOS\n",
        "# ========================================\n",
        "\n",
        "import cv2\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import base64\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def generate_video_best_model(\n",
        "    video_filename=\"ddqn_doubledunk_best_model.mp4\",\n",
        "    num_episodes=3,\n",
        "    max_steps_per_episode=2000,\n",
        "    fps=30,\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Genera un video del mejor modelo jugando DoubleDunk episodios completos\n",
        "    \n",
        "    Args:\n",
        "        video_filename: Nombre del archivo de video a generar\n",
        "        num_episodes: Número de episodios a grabar\n",
        "        max_steps_per_episode: Máximo de pasos por episodio\n",
        "        fps: Frames por segundo del video\n",
        "        seed: Semilla para reproducibilidad\n",
        "    \"\"\"\n",
        "    print(f\"🎬 GENERANDO VIDEO DEL MEJOR MODELO\")\n",
        "    print(f\"=\" * 50)\n",
        "    \n",
        "    # Verificar si existe el mejor modelo\n",
        "    best_model_path = \"./checkpoints_doubledunk/best_model.zip\"\n",
        "    if not os.path.exists(best_model_path):\n",
        "        print(f\"❌ No se encontró el mejor modelo en: {best_model_path}\")\n",
        "        print(\"💡 Asegúrate de que el entrenamiento haya generado un mejor modelo\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Cargar el mejor modelo\n",
        "        print(f\"📂 Cargando mejor modelo desde: {best_model_path}\")\n",
        "        model = DQN.load(best_model_path)\n",
        "        print(f\"✅ Modelo cargado exitosamente\")\n",
        "        \n",
        "        # Crear ambiente para grabación (sin VecVideoRecorder para mejor control)\n",
        "        print(f\"🎮 Configurando ambiente DoubleDunk...\")\n",
        "        env = make_atari_env(\"ALE/DoubleDunk-v5\", n_envs=1, seed=seed)\n",
        "        env = VecFrameStack(env, n_stack=4)\n",
        "        \n",
        "        # Configurar grabación manual con mejor control\n",
        "        video_folder = \"./videos/\"\n",
        "        os.makedirs(video_folder, exist_ok=True)\n",
        "        \n",
        "        print(f\"🎬 Iniciando grabación:\")\n",
        "        print(f\"   📹 Episodios: {num_episodes}\")\n",
        "        print(f\"   ⏱️  Max pasos por episodio: {max_steps_per_episode}\")\n",
        "        print(f\"   🎯 Semilla: {seed}\")\n",
        "        print(f\"   📁 Carpeta: {video_folder}\")\n",
        "        \n",
        "        # Variables para estadísticas y frames\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "        all_frames = []\n",
        "        total_steps = 0\n",
        "        \n",
        "        # Ejecutar episodios y recopilar frames\n",
        "        for episode in range(num_episodes):\n",
        "            print(f\"   🎮 Iniciando episodio {episode + 1}/{num_episodes}...\")\n",
        "            \n",
        "            obs = env.reset()\n",
        "            episode_reward = 0\n",
        "            episode_length = 0\n",
        "            episode_frames = []\n",
        "            \n",
        "            for step in range(max_steps_per_episode):\n",
        "                # Obtener frame antes de la acción\n",
        "                # Para vectorized env, necesitamos obtener el frame del environment interno\n",
        "                try:\n",
        "                    # Intentar obtener frame del environment interno\n",
        "                    if hasattr(env, 'render'):\n",
        "                        frame = env.render()\n",
        "                    else:\n",
        "                        # Para VecEnv, acceder al environment base\n",
        "                        frame = env.envs[0].render()\n",
        "                    \n",
        "                    if frame is not None:\n",
        "                        episode_frames.append(frame)\n",
        "                except:\n",
        "                    # Si no podemos obtener frame, continuamos sin él\n",
        "                    pass\n",
        "                \n",
        "                # Predecir acción usando el modelo\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                \n",
        "                # Ejecutar acción\n",
        "                obs, reward, done, info = env.step(action)\n",
        "                \n",
        "                episode_reward += reward[0]\n",
        "                episode_length += 1\n",
        "                total_steps += 1\n",
        "                \n",
        "                # Si el episodio terminó\n",
        "                if done[0]:\n",
        "                    # Obtener frame final\n",
        "                    try:\n",
        "                        if hasattr(env, 'render'):\n",
        "                            final_frame = env.render()\n",
        "                        else:\n",
        "                            final_frame = env.envs[0].render()\n",
        "                        if final_frame is not None:\n",
        "                            episode_frames.append(final_frame)\n",
        "                    except:\n",
        "                        pass\n",
        "                    break\n",
        "            \n",
        "            # Guardar estadísticas del episodio\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "            all_frames.extend(episode_frames)\n",
        "            \n",
        "            print(f\"   🏁 Episodio {episode + 1}: Reward={episode_reward:.2f}, Steps={episode_length}, Frames={len(episode_frames)}\")\n",
        "        \n",
        "        # Cerrar ambiente\n",
        "        env.close()\n",
        "        \n",
        "        # Estadísticas finales\n",
        "        if episode_rewards:\n",
        "            mean_reward = np.mean(episode_rewards)\n",
        "            std_reward = np.std(episode_rewards)\n",
        "            mean_length = np.mean(episode_lengths)\n",
        "            \n",
        "            print(f\"\\n📊 ESTADÍSTICAS DE GRABACIÓN:\")\n",
        "            print(f\"   🏆 Recompensa promedio: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
        "            print(f\"   📏 Duración promedio: {mean_length:.1f} pasos\")\n",
        "            print(f\"   🎯 Total de pasos: {total_steps}\")\n",
        "            print(f\"   🎬 Total de frames: {len(all_frames)}\")\n",
        "            print(f\"   ⏱️  Episodios completados: {num_episodes}\")\n",
        "        \n",
        "        # Generar video si tenemos frames\n",
        "        if all_frames:\n",
        "            final_video_path = os.path.join(video_folder, video_filename)\n",
        "            \n",
        "            print(f\"\\n🎬 Generando video con {len(all_frames)} frames...\")\n",
        "            \n",
        "            # Usar imageio para crear el video\n",
        "            imageio.mimsave(\n",
        "                final_video_path, \n",
        "                all_frames, \n",
        "                fps=fps,\n",
        "                quality=8,\n",
        "                macro_block_size=1  # Evitar problemas de resolución\n",
        "            )\n",
        "            \n",
        "            print(f\"\\n✅ VIDEO GENERADO EXITOSAMENTE:\")\n",
        "            print(f\"   📁 Ubicación: {final_video_path}\")\n",
        "            print(f\"   📊 Tamaño: {os.path.getsize(final_video_path) / 1024 / 1024:.1f} MB\")\n",
        "            print(f\"   ⏱️  Duración: ~{len(all_frames) / fps:.1f} segundos\")\n",
        "            \n",
        "            return final_video_path\n",
        "        else:\n",
        "            print(f\"❌ No se pudieron capturar frames para el video\")\n",
        "            print(f\"💡 Esto puede deberse a problemas de renderizado en el ambiente\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error generando video: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def generate_video_robust_method(\n",
        "    video_filename=\"ddqn_doubledunk_best_model.mp4\",\n",
        "    num_episodes=3,\n",
        "    max_steps_per_episode=10000,  # Aumentar límite para episodios completos\n",
        "    fps=30,\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Método robusto que garantiza episodios completos de DoubleDunk\n",
        "    Usa el ambiente exacto del entrenamiento para compatibilidad total\n",
        "    \"\"\"\n",
        "    print(f\"🎬 GENERANDO VIDEO - MÉTODO ROBUSTO\")\n",
        "    print(f\"=\" * 50)\n",
        "    \n",
        "    # Verificar si existe el mejor modelo\n",
        "    best_model_path = \"./checkpoints_doubledunk/best_model.zip\"\n",
        "    if not os.path.exists(best_model_path):\n",
        "        print(f\"❌ No se encontró el mejor modelo en: {best_model_path}\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Cargar el mejor modelo\n",
        "        print(f\"📂 Cargando mejor modelo desde: {best_model_path}\")\n",
        "        model = DQN.load(best_model_path)\n",
        "        print(f\"✅ Modelo cargado exitosamente\")\n",
        "        \n",
        "        # Crear ambiente EXACTAMENTE igual al entrenamiento para compatibilidad total\n",
        "        print(f\"🎮 Configurando ambiente EXACTO del entrenamiento...\")\n",
        "        \n",
        "        # Usar make_atari_env como en el entrenamiento pero para un solo env\n",
        "        env_single = make_atari_env(\"ALE/DoubleDunk-v5\", n_envs=1, seed=seed)\n",
        "        env_single = VecFrameStack(env_single, n_stack=4)\n",
        "        \n",
        "        # Para la grabación, también crear un ambiente de renderizado\n",
        "        import gymnasium as gym\n",
        "        render_env = gym.make(\"ALE/DoubleDunk-v5\", render_mode=\"rgb_array\")\n",
        "        \n",
        "        # Aplicar los mismos wrappers que make_atari_env\n",
        "        from stable_baselines3.common.atari_wrappers import (\n",
        "            NoopResetEnv, MaxAndSkipEnv, EpisodicLifeEnv, \n",
        "            FireResetEnv, WarpFrame, ClipRewardEnv\n",
        "        )\n",
        "        \n",
        "        render_env = NoopResetEnv(render_env, noop_max=30)\n",
        "        render_env = MaxAndSkipEnv(render_env, skip=4)\n",
        "        render_env = EpisodicLifeEnv(render_env)\n",
        "        if \"FIRE\" in render_env.unwrapped.get_action_meanings():\n",
        "            render_env = FireResetEnv(render_env)\n",
        "        render_env = WarpFrame(render_env)\n",
        "        render_env = ClipRewardEnv(render_env)\n",
        "        \n",
        "        # Frame stacking manual para sincronizar\n",
        "        from collections import deque\n",
        "        frame_stack = deque(maxlen=4)\n",
        "        \n",
        "        video_folder = \"./videos/\"\n",
        "        os.makedirs(video_folder, exist_ok=True)\n",
        "        \n",
        "        print(f\"🎬 Iniciando grabación con método robusto:\")\n",
        "        print(f\"   📹 Episodios: {num_episodes}\")\n",
        "        print(f\"   ⏱️  Max pasos por episodio: {max_steps_per_episode}\")\n",
        "        print(f\"   🎯 Semilla: {seed}\")\n",
        "        print(f\"   🔄 Usando ambiente exacto del entrenamiento\")\n",
        "        \n",
        "        # Variables para estadísticas y frames\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "        all_frames = []\n",
        "        \n",
        "        # Ejecutar episodios con doble ambiente (predicción + renderizado)\n",
        "        for episode in range(num_episodes):\n",
        "            print(f\"   🎮 Iniciando episodio {episode + 1}/{num_episodes}...\")\n",
        "            \n",
        "            # Reset ambos ambientes con la misma semilla\n",
        "            obs_model = env_single.reset()  # Para el modelo DDQN\n",
        "            obs_render, info_render = render_env.reset(seed=seed + episode)  # Para renderizado\n",
        "            \n",
        "            # Inicializar frame stack para el ambiente de renderizado\n",
        "            frame_stack.clear()\n",
        "            for _ in range(4):\n",
        "                frame_stack.append(obs_render)\n",
        "            \n",
        "            episode_reward = 0\n",
        "            episode_length = 0\n",
        "            episode_frames = []\n",
        "            \n",
        "            # Variables para sincronización\n",
        "            render_done = False\n",
        "            model_done = False\n",
        "            \n",
        "            for step in range(max_steps_per_episode):\n",
        "                # Obtener frame para video desde ambiente de renderizado\n",
        "                if not render_done:\n",
        "                    frame = render_env.render()\n",
        "                    if frame is not None:\n",
        "                        episode_frames.append(frame)\n",
        "                \n",
        "                # Predecir acción usando el ambiente del modelo (exacto al entrenamiento)\n",
        "                if not model_done:\n",
        "                    action, _ = model.predict(obs_model, deterministic=True)\n",
        "                    \n",
        "                    # Ejecutar en ambiente del modelo\n",
        "                    obs_model, reward_model, done_model, info_model = env_single.step(action)\n",
        "                    model_done = done_model[0]\n",
        "                    episode_reward += reward_model[0]\n",
        "                    \n",
        "                    # Ejecutar la misma acción en ambiente de renderizado (sincronizado)\n",
        "                    if not render_done:\n",
        "                        obs_render, reward_render, terminated, truncated, info_render = render_env.step(action[0])\n",
        "                        render_done = terminated or truncated\n",
        "                        frame_stack.append(obs_render)\n",
        "                \n",
        "                episode_length += 1\n",
        "                \n",
        "                # Terminar cuando cualquiera de los dos termine\n",
        "                if model_done or render_done:\n",
        "                    print(f\"     💡 Episodio terminó: Model_done={model_done}, Render_done={render_done}\")\n",
        "                    # Obtener frame final si es posible\n",
        "                    if not render_done:\n",
        "                        try:\n",
        "                            final_frame = render_env.render()\n",
        "                            if final_frame is not None:\n",
        "                                episode_frames.append(final_frame)\n",
        "                        except:\n",
        "                            pass\n",
        "                    break\n",
        "            \n",
        "            # Guardar estadísticas\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "            all_frames.extend(episode_frames)\n",
        "            \n",
        "            print(f\"   🏁 Episodio {episode + 1}: Reward={episode_reward:.2f}, Steps={episode_length}, Frames={len(episode_frames)}\")\n",
        "            \n",
        "            # Verificar si el episodio fue muy corto (posible problema)\n",
        "            if episode_length < 10:\n",
        "                print(f\"     ⚠️  Episodio muy corto - posible problema de sincronización\")\n",
        "        \n",
        "        # Cerrar ambientes\n",
        "        env_single.close()\n",
        "        render_env.close()\n",
        "        \n",
        "        # Estadísticas finales\n",
        "        if episode_rewards:\n",
        "            mean_reward = np.mean(episode_rewards)\n",
        "            std_reward = np.std(episode_rewards)\n",
        "            mean_length = np.mean(episode_lengths)\n",
        "            \n",
        "            print(f\"\\n📊 ESTADÍSTICAS DE GRABACIÓN:\")\n",
        "            print(f\"   🏆 Recompensa promedio: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
        "            print(f\"   📏 Duración promedio: {mean_length:.1f} pasos\")\n",
        "            print(f\"   🎬 Total de frames: {len(all_frames)}\")\n",
        "            print(f\"   ⏱️  Episodios completados: {num_episodes}\")\n",
        "        \n",
        "        # Generar video\n",
        "        if all_frames:\n",
        "            final_video_path = os.path.join(video_folder, video_filename)\n",
        "            \n",
        "            print(f\"\\n🎬 Generando video con {len(all_frames)} frames...\")\n",
        "            \n",
        "            imageio.mimsave(\n",
        "                final_video_path, \n",
        "                all_frames, \n",
        "                fps=fps,\n",
        "                quality=8,\n",
        "                macro_block_size=1\n",
        "            )\n",
        "            \n",
        "            print(f\"\\n✅ VIDEO GENERADO EXITOSAMENTE:\")\n",
        "            print(f\"   📁 Ubicación: {final_video_path}\")\n",
        "            print(f\"   📊 Tamaño: {os.path.getsize(final_video_path) / 1024 / 1024:.1f} MB\")\n",
        "            print(f\"   ⏱️  Duración: ~{len(all_frames) / fps:.1f} segundos\")\n",
        "            \n",
        "            return final_video_path\n",
        "        else:\n",
        "            print(f\"❌ No se pudieron capturar frames\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error en método alternativo: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def diagnose_video_issue(video_path):\n",
        "    \"\"\"\n",
        "    Diagnostica problemas comunes en la generación de videos\n",
        "    \"\"\"\n",
        "    print(f\"🔍 DIAGNÓSTICO DEL VIDEO\")\n",
        "    print(f\"=\" * 30)\n",
        "    \n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"❌ El video no existe: {video_path}\")\n",
        "        return\n",
        "    \n",
        "    # Información básica del archivo\n",
        "    file_size = os.path.getsize(video_path)\n",
        "    print(f\"📁 Archivo: {video_path}\")\n",
        "    print(f\"📊 Tamaño: {file_size / 1024:.1f} KB\")\n",
        "    \n",
        "    # Intentar leer con imageio\n",
        "    try:\n",
        "        reader = imageio.get_reader(video_path)\n",
        "        frame_count = reader.count_frames()\n",
        "        meta = reader.get_meta_data()\n",
        "        fps = meta.get('fps', 30)\n",
        "        duration = frame_count / fps if fps > 0 else 0\n",
        "        \n",
        "        print(f\"🎬 Frames: {frame_count}\")\n",
        "        print(f\"⏱️  FPS: {fps}\")\n",
        "        print(f\"⏰ Duración: {duration:.2f} segundos\")\n",
        "        \n",
        "        # Leer algunos frames para diagnóstico\n",
        "        sample_frames = min(5, frame_count)\n",
        "        print(f\"🖼️  Muestreando {sample_frames} frames...\")\n",
        "        \n",
        "        for i in range(sample_frames):\n",
        "            try:\n",
        "                frame = reader.get_data(i)\n",
        "                print(f\"   Frame {i}: {frame.shape} - {frame.dtype}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   Frame {i}: Error - {e}\")\n",
        "        \n",
        "        reader.close()\n",
        "        \n",
        "        # Diagnóstico de problemas comunes\n",
        "        if duration < 5:\n",
        "            print(f\"⚠️  VIDEO MUY CORTO - Posibles causas:\")\n",
        "            print(f\"   • Episodios terminan muy rápido\")\n",
        "            print(f\"   • Modelo no está funcionando correctamente\")\n",
        "            print(f\"   • Problema de ambiente/wrappers\")\n",
        "        \n",
        "        if frame_count < 100:\n",
        "            print(f\"⚠️  POCOS FRAMES - Posibles causas:\")\n",
        "            print(f\"   • Captura de frames fallando\")\n",
        "            print(f\"   • Renderizado no funciona\")\n",
        "            print(f\"   • Sincronización de ambientes\")\n",
        "        \n",
        "        print(f\"✅ Diagnóstico completado\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error leyendo video: {e}\")\n",
        "        print(f\"💡 El archivo puede estar corrupto o en formato incorrecto\")\n",
        "\n",
        "def display_video_in_notebook(video_path):\n",
        "    \"\"\"\n",
        "    Muestra el video directamente en el notebook de Jupyter/Colab\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"❌ Video no encontrado: {video_path}\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        # Leer y encodear video en base64\n",
        "        with open(video_path, \"rb\") as f:\n",
        "            video_data = f.read()\n",
        "        \n",
        "        video_base64 = base64.b64encode(video_data).decode()\n",
        "        \n",
        "        # Crear HTML para mostrar video\n",
        "        video_html = f\"\"\"\n",
        "        <div style=\"text-align: center; margin: 20px;\">\n",
        "            <h3>🎬 DDQN DoubleDunk - Mejor Modelo</h3>\n",
        "            <video width=\"640\" height=\"480\" controls style=\"border: 2px solid #4CAF50; border-radius: 10px;\">\n",
        "                <source src=\"data:video/mp4;base64,{video_base64}\" type=\"video/mp4\">\n",
        "                Tu navegador no soporta videos HTML5.\n",
        "            </video>\n",
        "            <p style=\"margin-top: 10px; color: #666;\">\n",
        "                📁 Archivo: {os.path.basename(video_path)} | \n",
        "                📊 Tamaño: {os.path.getsize(video_path) / 1024 / 1024:.1f} MB\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        \n",
        "        display(HTML(video_html))\n",
        "        print(f\"✅ Video mostrado en el notebook\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error mostrando video: {e}\")\n",
        "        print(f\"💡 Puedes descargar el video directamente desde: {video_path}\")\n",
        "\n",
        "# Información sobre la generación de videos\n",
        "print(\"🎬 SISTEMA DE GENERACIÓN DE VIDEOS MEJORADO\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Funciones disponibles:\")\n",
        "print(\"   generate_video_robust_method()       # Método principal (ambientes sincronizados)\")\n",
        "print(\"   generate_video_best_model()         # Método de respaldo\")\n",
        "print(\"   display_video_in_notebook()         # Mostrar video en notebook\")\n",
        "print()\n",
        "print(\"✨ MEJORAS IMPLEMENTADAS:\")\n",
        "print(\"   🎯 Episodios completos - No se corta en medio del juego\")\n",
        "print(\"   🔄 Ambientes sincronizados - Modelo + Renderizado en paralelo\")\n",
        "print(\"   📹 Captura frame por frame - Más confiable\")\n",
        "print(\"   🎮 Ambiente exacto - Misma configuración del entrenamiento\")\n",
        "print(\"   ⏱️  Duración precisa - Control total del video\")\n",
        "print(\"   🛡️  Detección de problemas - Diagnóstico automático\")\n",
        "print()\n",
        "print(\"💡 El video se genera automáticamente usando el mejor modelo guardado\")\n",
        "print(\"   durante el entrenamiento (best_model.zip)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# EJECUTAR GENERACIÓN DE VIDEO\n",
        "# ========================================\n",
        "\n",
        "# Configuración de video\n",
        "VIDEO_CONFIG = {\n",
        "    'video_filename': 'ddqn_doubledunk_best_model.mp4',\n",
        "    'num_episodes': 10,           # Número de episodios a grabar\n",
        "    'max_steps_per_episode': 2000,  # Pasos máximos por episodio\n",
        "    'seed': 42                   # Semilla para reproducibilidad\n",
        "}\n",
        "\n",
        "print(\"🎬 CONFIGURACIÓN DE VIDEO:\")\n",
        "print(\"=\" * 30)\n",
        "for key, value in VIDEO_CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "print()\n",
        "\n",
        "# Generar video del mejor modelo - Intentar método alternativo primero\n",
        "print(\"🚀 Iniciando generación de video...\")\n",
        "print(\"💡 Probando método alternativo (mejor control de episodios completos)...\")\n",
        "\n",
        "video_path = generate_video_robust_method(**VIDEO_CONFIG)\n",
        "\n",
        "# Si el método alternativo falla, intentar método original\n",
        "if video_path is None:\n",
        "    print(\"\\n🔄 Método alternativo falló, probando método original...\")\n",
        "    video_path = generate_video_best_model(**VIDEO_CONFIG)\n",
        "\n",
        "if video_path:\n",
        "    print(f\"\\n🎉 ¡Video generado exitosamente!\")\n",
        "    print(f\"📁 Ubicación: {video_path}\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Información adicional para el reporte académico\n",
        "    print(f\"\\n📋 INFORMACIÓN PARA EL REPORTE:\")\n",
        "    print(f\"✅ Video evidencia disponible en: {video_path}\")\n",
        "    print(f\"🎯 Agente entrenado con DDQN + Epsilon Scheduler\")\n",
        "    print(f\"🏆 Modelo usado: Mejor modelo durante entrenamiento\")\n",
        "    print(f\"🎮 Ambiente: ALE/DoubleDunk-v5\")\n",
        "    print(f\"🎲 Semilla: {VIDEO_CONFIG['seed']} (reproducible)\")\n",
        "    print(f\"📊 Episodios grabados: {VIDEO_CONFIG['num_episodes']}\")\n",
        "    \n",
        "else:\n",
        "    print(f\"\\n❌ No se pudo generar el video\")\n",
        "    print(f\"💡 Posibles causas:\")\n",
        "    print(f\"   - El modelo no existe (entrenamiento no completado)\")\n",
        "    print(f\"   - Error de ambiente o dependencias\")\n",
        "    print(f\"   - Falta de espacio en disco\")\n",
        "    print(f\"\\n🔧 Soluciones:\")\n",
        "    print(f\"   1. Verificar que existe: ./checkpoints_doubledunk/best_model.zip\")\n",
        "    print(f\"   2. Ejecutar el entrenamiento antes de generar video\")\n",
        "    print(f\"   3. Revisar los logs de error arriba\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
