{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèÄ DoubleDunk - DDQN Optimizado para MAIA\n",
        "\n",
        "**Reto de Aprendizaje por Refuerzo Profundo**  \n",
        "**Algoritmo:** Double Deep Q-Network (DDQN) con Epsilon Scheduler C√≠clico  \n",
        "**Entorno:** ALE/DoubleDunk-v5 (Atari Basketball)  \n",
        "**Plataforma:** Google Colab con GPU optimizado para entrenamiento largo  \n",
        "\n",
        "\n",
        "\n",
        "## üìã Informaci√≥n del Proyecto\n",
        "\n",
        "- **Curso:** Aprendizaje por Refuerzo Profundo - MAIA\n",
        "- **Problema:** Optimizaci√≥n de agente para juego DoubleDunk\n",
        "- **M√©todo:** DDQN con mejoras espec√≠ficas vs REINFORCE baseline\n",
        "- **Tiempo estimado:** 6-12 horas en GPU Colab (con interrupciones)\n",
        "\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANTE:** Este notebook est√° dise√±ado para entrenamientos largos en GPU. Utiliza checkpoints autom√°ticos para manejar desconexiones de Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Instalaci√≥n de Dependencias (Google Colab GPU)\n",
        "\n",
        "**Optimizado para sesiones GPU largas con gesti√≥n de memoria**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# INSTALACI√ìN OPTIMIZADA PARA GPU COLAB\n",
        "# ========================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "print(\"üöÄ Configurando entorno para DDQN DoubleDunk en GPU...\")\n",
        "\n",
        "# Verificar que estamos en Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"‚úÖ Google Colab detectado\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚ö†Ô∏è  Ejecut√°ndose fuera de Colab\")\n",
        "\n",
        "# Instalaciones principales con verificaci√≥n de errores\n",
        "packages = [\n",
        "    'stable-baselines3[extra]',\n",
        "    'ale-py',\n",
        "    'gymnasium[atari,accept-rom-license]',\n",
        "    'autorom',\n",
        "    'tensorboard',\n",
        "    'opencv-python',\n",
        "    'imageio[ffmpeg]',\n",
        "    'pandas',\n",
        "    'matplotlib',\n",
        "    'seaborn',\n",
        "    'tqdm'\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
        "        print(f\"‚úÖ {package}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Error instalando {package}: {e}\")\n",
        "\n",
        "# Configurar ROMs de Atari\n",
        "try:\n",
        "    subprocess.run(['AutoROM', '--accept-license'], check=True, \n",
        "                  capture_output=True, text=True)\n",
        "    print(\"‚úÖ ROMs de Atari configuradas\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  ROMs ya configuradas o error menor\")\n",
        "\n",
        "# Configurar directorio de trabajo\n",
        "if IN_COLAB:\n",
        "    os.makedirs('/content/ddqn_doubledunk', exist_ok=True)\n",
        "    os.chdir('/content/ddqn_doubledunk')\n",
        "    print(\"üìÅ Directorio de trabajo: /content/ddqn_doubledunk\")\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n completa - Listo para GPU\")\n",
        "print(\"üéØ Iniciando importaciones...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# IMPORTACIONES Y CONFIGURACI√ìN GPU\n",
        "# ========================================\n",
        "\n",
        "# Librer√≠as RL y utilidades\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.logger import configure\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
        "from stable_baselines3.common.vec_env import VecMonitor, VecFrameStack, VecVideoRecorder\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "\n",
        "import gymnasium\n",
        "import ale_py\n",
        "from collections import deque\n",
        "import cv2\n",
        "\n",
        "# Registrar entornos ALE\n",
        "gymnasium.register_envs(ale_py)\n",
        "\n",
        "# Librer√≠as b√°sicas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "import glob\n",
        "import base64\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML, display, clear_output\n",
        "\n",
        "# PyTorch para GPU\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "print(\"üìö Importaciones completadas\")\n",
        "print(f\"üî¢ Stable Baselines3: {stable_baselines3.__version__}\")\n",
        "print(f\"üî• PyTorch: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üñ•Ô∏è Detecci√≥n y Configuraci√≥n de GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# DETECCI√ìN AUTOM√ÅTICA DE GPU COLAB\n",
        "# ========================================\n",
        "\n",
        "import platform\n",
        "import psutil\n",
        "\n",
        "def detect_colab_hardware():\n",
        "    \"\"\"\n",
        "    Detecta y configura el hardware disponible en Google Colab\n",
        "    Optimizado para GPU T4, P100, V100, A100\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üñ•Ô∏è  DETECCI√ìN DE HARDWARE GOOGLE COLAB\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Informaci√≥n del sistema\n",
        "    print(f\"Sistema: {platform.system()} {platform.release()}\")\n",
        "    print(f\"CPU: {platform.processor()}\")\n",
        "    print(f\"RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    \n",
        "    device_info = {}\n",
        "    \n",
        "    # Verificar CUDA (GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        gpu_compute = torch.cuda.get_device_properties(0).major\n",
        "        \n",
        "        device_info = {\n",
        "            'device': device,\n",
        "            'name': gpu_name,\n",
        "            'memory_gb': gpu_memory,\n",
        "            'compute_capability': gpu_compute,\n",
        "            'type': 'GPU_CUDA',\n",
        "            'recommended': True\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ GPU DETECTADA: {gpu_name}\")\n",
        "        print(f\"   üíæ Memoria GPU: {gpu_memory:.1f} GB\")\n",
        "        print(f\"   üîß Compute Capability: {gpu_compute}.x\")\n",
        "        \n",
        "        # Configuraciones espec√≠ficas por GPU\n",
        "        if 'T4' in gpu_name:\n",
        "            print(f\"   üéØ Tesla T4 detectada - Configuraci√≥n optimizada\")\n",
        "            batch_size_factor = 1.0\n",
        "        elif 'P100' in gpu_name:\n",
        "            print(f\"   üöÄ Tesla P100 detectada - Configuraci√≥n de alto rendimiento\")\n",
        "            batch_size_factor = 1.2\n",
        "        elif 'V100' in gpu_name:\n",
        "            print(f\"   üíé Tesla V100 detectada - Configuraci√≥n premium\")\n",
        "            batch_size_factor = 1.5\n",
        "        elif 'A100' in gpu_name:\n",
        "            print(f\"   üåü Tesla A100 detectada - Configuraci√≥n m√°xima\")\n",
        "            batch_size_factor = 2.0\n",
        "        else:\n",
        "            print(f\"   üîß GPU gen√©rica detectada - Configuraci√≥n est√°ndar\")\n",
        "            batch_size_factor = 1.0\n",
        "            \n",
        "        device_info['batch_size_factor'] = batch_size_factor\n",
        "        \n",
        "    else:\n",
        "        # Fallback a CPU\n",
        "        device = torch.device('cpu')\n",
        "        device_info = {\n",
        "            'device': device,\n",
        "            'name': 'CPU',\n",
        "            'type': 'CPU',\n",
        "            'recommended': False,\n",
        "            'batch_size_factor': 0.5\n",
        "        }\n",
        "        print(f\"‚ö†Ô∏è  SOLO CPU DISPONIBLE\")\n",
        "        print(f\"   ‚ùå No se detect√≥ GPU - El entrenamiento ser√° MUY lento\")\n",
        "        print(f\"   üí° Aseg√∫rate de activar GPU en Colab: Runtime > Change runtime type > GPU\")\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "    print(f\"DISPOSITIVO FINAL: {device_info['name']} ({device_info['device']})\")\n",
        "    \n",
        "    if device_info['recommended']:\n",
        "        print(f\"‚úÖ Configuraci√≥n √≥ptima para entrenamiento largo\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  ADVERTENCIA: Sin GPU el entrenamiento puede tomar d√≠as\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return device_info['device'], device_info\n",
        "\n",
        "# Detectar hardware\n",
        "DEVICE, DEVICE_INFO = detect_colab_hardware()\n",
        "\n",
        "# Configuraciones de PyTorch para GPU\n",
        "if DEVICE.type == 'cuda':\n",
        "    print(\"üöÄ Aplicando optimizaciones CUDA...\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    # Limpiar cach√© de GPU\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"   üìä Memoria GPU inicial: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"üíª Configurando para CPU...\")\n",
        "    torch.set_num_threads(4)  # Limitar threads en Colab\n",
        "\n",
        "print(f\"\\nüéØ Sistema configurado para: {DEVICE_INFO['name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Sistema de Checkpoints Inteligente\n",
        "\n",
        "**Gesti√≥n autom√°tica de checkpoints para entrenamientos largos con interrupciones**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# SISTEMA DE CHECKPOINTS INTELIGENTE\n",
        "# ========================================\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"\n",
        "    Gestor de checkpoints optimizado para entrenamientos largos en Colab\n",
        "    Maneja desconexiones autom√°ticamente y preserva todo el estado\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, checkpoint_dir=\"./checkpoints\", backup_every=50000):\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.backup_every = backup_every\n",
        "        self.session_start = datetime.now()\n",
        "        \n",
        "        # Crear directorios\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "        os.makedirs(f\"{checkpoint_dir}/models\", exist_ok=True)\n",
        "        os.makedirs(f\"{checkpoint_dir}/training_state\", exist_ok=True)\n",
        "        \n",
        "        print(f\"üíæ Checkpoint Manager inicializado\")\n",
        "        print(f\"   üìÅ Directorio: {checkpoint_dir}\")\n",
        "        print(f\"   ‚è∞ Backup cada: {backup_every:,} timesteps\")\n",
        "    \n",
        "    def _convert_to_json_serializable(self, obj):\n",
        "        \"\"\"\n",
        "        Convierte objetos a tipos serializables por JSON\n",
        "        \"\"\"\n",
        "        if isinstance(obj, dict):\n",
        "            return {key: self._convert_to_json_serializable(value) for key, value in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [self._convert_to_json_serializable(item) for item in obj]\n",
        "        elif hasattr(obj, 'item'):  # numpy scalars\n",
        "            return obj.item()\n",
        "        elif hasattr(obj, 'tolist'):  # numpy arrays\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, (np.int32, np.int64, np.float32, np.float64)):\n",
        "            return float(obj) if 'float' in str(type(obj)) else int(obj)\n",
        "        else:\n",
        "            return obj\n",
        "    \n",
        "    def save_checkpoint(self, model, timestep, episode_rewards, metadata=None):\n",
        "        \"\"\"\n",
        "        Guarda checkpoint completo del estado del entrenamiento\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convertir episode_rewards a float est√°ndar para JSON serialization\n",
        "            safe_episode_rewards = [float(reward) for reward in episode_rewards]\n",
        "            \n",
        "            # Convertir metadata recursivamente\n",
        "            safe_metadata = self._convert_to_json_serializable(metadata or {})\n",
        "            \n",
        "            checkpoint_data = {\n",
        "                'timestep': int(timestep),\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'session_duration': str(datetime.now() - self.session_start),\n",
        "                'episode_rewards': safe_episode_rewards,\n",
        "                'device': str(DEVICE),\n",
        "                'metadata': safe_metadata\n",
        "            }\n",
        "            \n",
        "            # Guardar modelo\n",
        "            model_path = f\"{self.checkpoint_dir}/models/ddqn_checkpoint_{timestep}.zip\"\n",
        "            model.save(model_path)\n",
        "            \n",
        "            # Guardar estado de entrenamiento\n",
        "            state_path = f\"{self.checkpoint_dir}/training_state/state_{timestep}.json\"\n",
        "            with open(state_path, 'w') as f:\n",
        "                json.dump(checkpoint_data, f, indent=2)\n",
        "            \n",
        "            print(f\"üíæ Checkpoint guardado: timestep {timestep:,}\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error guardando checkpoint: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def list_checkpoints(self):\n",
        "        \"\"\"Lista todos los checkpoints disponibles\"\"\"\n",
        "        checkpoints = []\n",
        "        model_files = glob.glob(f\"{self.checkpoint_dir}/models/ddqn_checkpoint_*.zip\")\n",
        "        \n",
        "        for model_file in model_files:\n",
        "            try:\n",
        "                timestep = int(model_file.split('_')[-1].split('.')[0])\n",
        "                state_file = f\"{self.checkpoint_dir}/training_state/state_{timestep}.json\"\n",
        "                \n",
        "                if os.path.exists(state_file):\n",
        "                    with open(state_file, 'r') as f:\n",
        "                        state_data = json.load(f)\n",
        "                    \n",
        "                    checkpoints.append({\n",
        "                        'timestep': timestep,\n",
        "                        'model_path': model_file,\n",
        "                        'state_path': state_file,\n",
        "                        'timestamp': state_data.get('timestamp', 'Unknown'),\n",
        "                        'episodes': len(state_data.get('episode_rewards', [])),\n",
        "                        'last_reward': state_data.get('episode_rewards', [0])[-1] if state_data.get('episode_rewards') else 0\n",
        "                    })\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        return sorted(checkpoints, key=lambda x: x['timestep'])\n",
        "    \n",
        "    def get_latest_checkpoint(self):\n",
        "        \"\"\"Obtiene el checkpoint m√°s reciente\"\"\"\n",
        "        checkpoints = self.list_checkpoints()\n",
        "        return checkpoints[-1] if checkpoints else None\n",
        "    \n",
        "    def load_checkpoint(self, timestep=None):\n",
        "        \"\"\"Carga un checkpoint espec√≠fico o el m√°s reciente\"\"\"\n",
        "        if timestep is None:\n",
        "            checkpoint = self.get_latest_checkpoint()\n",
        "        else:\n",
        "            checkpoints = self.list_checkpoints()\n",
        "            checkpoint = next((c for c in checkpoints if c['timestep'] == timestep), None)\n",
        "        \n",
        "        if checkpoint is None:\n",
        "            return None, None\n",
        "        \n",
        "        try:\n",
        "            # Cargar estado\n",
        "            with open(checkpoint['state_path'], 'r') as f:\n",
        "                state_data = json.load(f)\n",
        "            \n",
        "            print(f\"üìÇ Cargando checkpoint: timestep {checkpoint['timestep']:,}\")\n",
        "            print(f\"   üìÖ Fecha: {checkpoint['timestamp']}\")\n",
        "            print(f\"   üìä Episodios: {checkpoint['episodes']}\")\n",
        "            print(f\"   üéØ √öltima recompensa: {checkpoint['last_reward']:.2f}\")\n",
        "            \n",
        "            return checkpoint['model_path'], state_data\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error cargando checkpoint: {e}\")\n",
        "            return None, None\n",
        "\n",
        "# Inicializar gestor de checkpoints\n",
        "checkpoint_manager = CheckpointManager(\n",
        "    checkpoint_dir=\"./checkpoints_doubledunk\",\n",
        "    backup_every=50000\n",
        ")\n",
        "\n",
        "# Verificar checkpoints existentes\n",
        "existing_checkpoints = checkpoint_manager.list_checkpoints()\n",
        "if existing_checkpoints:\n",
        "    print(f\"\\nüìã Checkpoints existentes encontrados: {len(existing_checkpoints)}\")\n",
        "    for cp in existing_checkpoints[-3:]:  # Mostrar los 3 m√°s recientes\n",
        "        print(f\"   ‚è∞ {cp['timestep']:,} steps - {cp['timestamp'][:19]} - Reward: {cp['last_reward']:.2f}\")\n",
        "else:\n",
        "    print(f\"\\nüìã No se encontraron checkpoints - Entrenamiento desde cero\")\n",
        "\n",
        "print(f\"\\n‚úÖ Sistema de checkpoints configurado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Algoritmo DDQN Optimizado + Callbacks + Trainer\n",
        "\n",
        "**Implementaci√≥n completa con sistema de checkpoints integrado**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# IMPLEMENTACI√ìN COMPLETA DDQN + SISTEMA CHECKPOINT\n",
        "# ========================================\n",
        "\n",
        "# Callbacks optimizados\n",
        "class RewardLoggerCallback(BaseCallback):\n",
        "    def __init__(self, path_backup: str = None, checkpoint_manager=None, usar_media: bool = True, ventana_para_media: int = 30):\n",
        "        super().__init__(verbose=True)\n",
        "        self.episode_rewards = []\n",
        "        self.best_score = -np.inf\n",
        "        self.path_backup = path_backup\n",
        "        self.checkpoint_manager = checkpoint_manager\n",
        "        self.usar_media = usar_media\n",
        "        self.ventana_para_media = ventana_para_media\n",
        "        self.last_checkpoint_step = 0\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        infos = self.locals.get(\"infos\", [])\n",
        "        if not infos:\n",
        "            return True\n",
        "\n",
        "        for info in infos:\n",
        "            ep = info.get(\"episode\")\n",
        "            if ep is None:\n",
        "                continue\n",
        "            r = ep.get(\"r\")\n",
        "            self.episode_rewards.append(r)\n",
        "\n",
        "            # Checkpoint autom√°tico cada 50k steps\n",
        "            if (self.checkpoint_manager and \n",
        "                self.num_timesteps - self.last_checkpoint_step >= self.checkpoint_manager.backup_every):\n",
        "                self.checkpoint_manager.save_checkpoint(\n",
        "                    self.model, self.num_timesteps, self.episode_rewards,\n",
        "                    {'best_score': self.best_score, 'total_episodes': len(self.episode_rewards)}\n",
        "                )\n",
        "                self.last_checkpoint_step = self.num_timesteps\n",
        "\n",
        "            # Evaluar mejor modelo\n",
        "            if self.usar_media:\n",
        "                if len(self.episode_rewards) >= self.ventana_para_media:\n",
        "                    score = float(np.mean(self.episode_rewards[-self.ventana_para_media:]))\n",
        "                else:\n",
        "                    score = float(np.mean(self.episode_rewards))\n",
        "            else:\n",
        "                score = float(r)\n",
        "\n",
        "            if score > self.best_score:\n",
        "                self.best_score = score\n",
        "                if self.path_backup:\n",
        "                    dirname = os.path.dirname(self.path_backup)\n",
        "                    if dirname and not os.path.exists(dirname):\n",
        "                        os.makedirs(dirname, exist_ok=True)\n",
        "                    try:\n",
        "                        self.model.save(self.path_backup)\n",
        "                        print(f\"üèÜ Nuevo mejor score {score:.2f} -> Guardado en: {self.path_backup}.zip\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ùå Error guardando mejor modelo: {e}\")\n",
        "        return True\n",
        "\n",
        "class EpsilonSchedulerCallback(BaseCallback):\n",
        "    def __init__(self, schedule_fn, total_timesteps: int, verbose: int = 1):\n",
        "        super().__init__(verbose)\n",
        "        self.schedule_fn = schedule_fn\n",
        "        self.total_timesteps = int(total_timesteps)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        t = min(self.num_timesteps, self.total_timesteps)\n",
        "        progress = t / float(self.total_timesteps)\n",
        "        new_eps = float(self.schedule_fn(progress))\n",
        "        self.model.exploration_rate = new_eps\n",
        "\n",
        "        if ((self.num_timesteps - 1) % 10_000 == 0):\n",
        "            try:\n",
        "                self.logger.record(\"train/epsilon\", new_eps)\n",
        "                lr = float(self.model.policy.optimizer.param_groups[0][\"lr\"])\n",
        "                self.logger.record(\"train/learning_rate\", lr)\n",
        "                if self.verbose:\n",
        "                    print(f\"üìä Step {self.num_timesteps:,} | Œµ={new_eps:.4f} | LR={lr:.6f}\")\n",
        "            except Exception:\n",
        "                pass\n",
        "        return True\n",
        "\n",
        "# Epsilon scheduler\n",
        "def crear_eps_scheduler(val_inicial: float, val_min: float, n_ciclos: int, degree: int):\n",
        "    def scheduler(progress: float) -> float:\n",
        "        envelope = (1.0 - progress**degree)\n",
        "        cos_term = 0.5 * (1.0 + np.cos(2 * np.pi * n_ciclos * progress))\n",
        "        val = val_inicial * envelope * cos_term\n",
        "        return float(max(val, val_min))\n",
        "    return scheduler\n",
        "\n",
        "# DDQN implementaci√≥n\n",
        "class OptimizedDoubleDQN(DQN):\n",
        "    def train(self, gradient_steps: int, batch_size: int = 32) -> None:\n",
        "        self.policy.set_training_mode(True)\n",
        "        self._update_learning_rate(self.policy.optimizer)\n",
        "        \n",
        "        losses = []\n",
        "        for _ in range(gradient_steps):\n",
        "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n",
        "            obs = replay_data.observations\n",
        "            next_obs = replay_data.next_observations\n",
        "            actions = replay_data.actions\n",
        "            rewards = replay_data.rewards\n",
        "            dones = replay_data.dones\n",
        "\n",
        "            if rewards.dim() == 1:\n",
        "                rewards = rewards.unsqueeze(1)\n",
        "            if dones.dim() == 1:\n",
        "                dones = dones.unsqueeze(1)\n",
        "            if actions.dim() == 1:\n",
        "                actions = actions.unsqueeze(1)\n",
        "\n",
        "            device = self.device\n",
        "            obs = obs.to(device)\n",
        "            next_obs = next_obs.to(device)\n",
        "            actions = actions.to(device)\n",
        "            rewards = rewards.to(device).float()\n",
        "            dones = dones.to(device).float()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                q_next_online = self.q_net(next_obs)\n",
        "                next_actions_online = q_next_online.argmax(dim=1, keepdim=True)\n",
        "                q_next_target = self.q_net_target(next_obs)\n",
        "                q_next_target_selected = torch.gather(q_next_target, dim=1, index=next_actions_online)\n",
        "                target_q_values = rewards + (1.0 - dones) * (self.gamma * q_next_target_selected)\n",
        "\n",
        "            q_values_all = self.q_net(obs)\n",
        "            current_q_values = torch.gather(q_values_all, dim=1, index=actions.long())\n",
        "            \n",
        "            loss = F.huber_loss(current_q_values, target_q_values, delta=1.0)\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "            self.policy.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm * 0.5)\n",
        "            self.policy.optimizer.step()\n",
        "\n",
        "        self._n_updates += gradient_steps\n",
        "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
        "        self.logger.record(\"train/loss\", np.mean(losses))\n",
        "\n",
        "# Configuraci√≥n optimizada para GPU\n",
        "DOUBLEDUNK_GPU_CONFIG = {\n",
        "    'total_timesteps': 5_000_000,\n",
        "    'n_ciclos_eps': 12,\n",
        "    'eps_inicial': 0.95,\n",
        "    'eps_min': 0.01,\n",
        "    'scheduler_degree': 1.5,\n",
        "    'ventana_media': 30,\n",
        "    'learning_rate': 2.5e-4 if DEVICE_INFO['recommended'] else 1e-4,\n",
        "    'buffer_size': 200_000,\n",
        "    'batch_size': int(32 * DEVICE_INFO.get('batch_size_factor', 1.0)),\n",
        "    'target_update': 8_000,\n",
        "    'train_freq': 4,\n",
        "    'learning_starts': 20_000,\n",
        "    'gamma': 0.995,\n",
        "}\n",
        "\n",
        "print(\"‚öôÔ∏è Configuraci√≥n GPU optimizada:\")\n",
        "for key, value in DOUBLEDUNK_GPU_CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "def create_optimized_gpu_model(env):\n",
        "    return OptimizedDoubleDQN(\n",
        "        \"CnnPolicy\",\n",
        "        env,\n",
        "        learning_rate=DOUBLEDUNK_GPU_CONFIG['learning_rate'],\n",
        "        buffer_size=DOUBLEDUNK_GPU_CONFIG['buffer_size'],\n",
        "        learning_starts=DOUBLEDUNK_GPU_CONFIG['learning_starts'],\n",
        "        batch_size=DOUBLEDUNK_GPU_CONFIG['batch_size'],\n",
        "        gradient_steps=1,\n",
        "        gamma=DOUBLEDUNK_GPU_CONFIG['gamma'],\n",
        "        train_freq=DOUBLEDUNK_GPU_CONFIG['train_freq'],\n",
        "        target_update_interval=DOUBLEDUNK_GPU_CONFIG['target_update'],\n",
        "        policy_kwargs=dict(\n",
        "            net_arch=[512, 256] if DEVICE_INFO['recommended'] else [256, 128],\n",
        "            activation_fn=torch.nn.ReLU,\n",
        "            normalize_images=True\n",
        "        ),\n",
        "        tensorboard_log=\"./logs_doubledunk\",\n",
        "        verbose=1,\n",
        "        device=DEVICE,\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Algoritmo DDQN y callbacks configurados\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Entrenamiento con Checkpoints Autom√°ticos\n",
        "\n",
        "**Entrenamiento largo optimizado para GPU con sistema de recuperaci√≥n**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ENTRENAMIENTO CON SISTEMA DE CHECKPOINTS\n",
        "# ========================================\n",
        "\n",
        "def setup_training_with_checkpoints():\n",
        "    \"\"\"Configura entrenamiento con capacidad de reanudaci√≥n\"\"\"\n",
        "    \n",
        "    # Verificar si hay checkpoint existente\n",
        "    model_path, state_data = checkpoint_manager.load_checkpoint()\n",
        "    \n",
        "    if model_path and state_data:\n",
        "        # Reanudar desde checkpoint\n",
        "        print(\"üîÑ REANUDANDO ENTRENAMIENTO DESDE CHECKPOINT\")\n",
        "        print(f\"   üìÖ √öltima sesi√≥n: {state_data['timestamp']}\")\n",
        "        print(f\"   üìä Timesteps completados: {state_data['timestep']:,}\")\n",
        "        print(f\"   üéØ Episodios: {len(state_data['episode_rewards'])}\")\n",
        "        \n",
        "        # Crear entorno\n",
        "        env = make_atari_env(\"ALE/DoubleDunk-v5\", n_envs=1, seed=0)\n",
        "        env = VecFrameStack(env, n_stack=4)\n",
        "        env = VecMonitor(env, filename=\"./logs_doubledunk/monitor.csv\")\n",
        "        \n",
        "        # Cargar modelo existente\n",
        "        model = OptimizedDoubleDQN.load(model_path, env=env, device=DEVICE)\n",
        "        \n",
        "        # Configurar callbacks con estado existente\n",
        "        scheduler = crear_eps_scheduler(\n",
        "            DOUBLEDUNK_GPU_CONFIG['eps_inicial'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['eps_min'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['n_ciclos_eps'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['scheduler_degree']\n",
        "        )\n",
        "        \n",
        "        eps_cb = EpsilonSchedulerCallback(\n",
        "            lambda p: scheduler(p), \n",
        "            total_timesteps=DOUBLEDUNK_GPU_CONFIG['total_timesteps']\n",
        "        )\n",
        "        \n",
        "        reward_cb = RewardLoggerCallback(\n",
        "            path_backup=\"./checkpoints_doubledunk/best_model\",\n",
        "            checkpoint_manager=checkpoint_manager,\n",
        "            ventana_para_media=DOUBLEDUNK_GPU_CONFIG['ventana_media']\n",
        "        )\n",
        "        \n",
        "        # Restaurar estado de rewards\n",
        "        reward_cb.episode_rewards = state_data['episode_rewards']\n",
        "        reward_cb.best_score = state_data['metadata'].get('best_score', -np.inf)\n",
        "        \n",
        "        # Calcular timesteps restantes\n",
        "        remaining_timesteps = DOUBLEDUNK_GPU_CONFIG['total_timesteps'] - state_data['timestep']\n",
        "        \n",
        "        return model, env, [reward_cb, eps_cb], remaining_timesteps\n",
        "        \n",
        "    else:\n",
        "        # Entrenamiento desde cero\n",
        "        print(\"üÜï INICIANDO ENTRENAMIENTO DESDE CERO\")\n",
        "        \n",
        "        # Crear entorno\n",
        "        env = make_atari_env(\"ALE/DoubleDunk-v5\", n_envs=1, seed=0)\n",
        "        env = VecFrameStack(env, n_stack=4)\n",
        "        env = VecMonitor(env, filename=\"./logs_doubledunk/monitor.csv\")\n",
        "        \n",
        "        # Crear modelo nuevo\n",
        "        model = create_optimized_gpu_model(env)\n",
        "        \n",
        "        # Configurar logger\n",
        "        new_logger = configure(\"./logs_doubledunk\", [\"csv\", \"tensorboard\"])\n",
        "        model.set_logger(new_logger)\n",
        "        \n",
        "        # Configurar callbacks\n",
        "        scheduler = crear_eps_scheduler(\n",
        "            DOUBLEDUNK_GPU_CONFIG['eps_inicial'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['eps_min'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['n_ciclos_eps'], \n",
        "            DOUBLEDUNK_GPU_CONFIG['scheduler_degree']\n",
        "        )\n",
        "        \n",
        "        eps_cb = EpsilonSchedulerCallback(\n",
        "            lambda p: scheduler(p), \n",
        "            total_timesteps=DOUBLEDUNK_GPU_CONFIG['total_timesteps']\n",
        "        )\n",
        "        \n",
        "        reward_cb = RewardLoggerCallback(\n",
        "            path_backup=\"./checkpoints_doubledunk/best_model\",\n",
        "            checkpoint_manager=checkpoint_manager,\n",
        "            ventana_para_media=DOUBLEDUNK_GPU_CONFIG['ventana_media']\n",
        "        )\n",
        "        \n",
        "        return model, env, [reward_cb, eps_cb], DOUBLEDUNK_GPU_CONFIG['total_timesteps']\n",
        "\n",
        "# Configurar entrenamiento\n",
        "model, env, callbacks, timesteps_to_train = setup_training_with_checkpoints()\n",
        "callback_list = CallbackList(callbacks)\n",
        "\n",
        "print(f\"üéØ Configuraci√≥n de entrenamiento completada\")\n",
        "print(f\"   üñ•Ô∏è  Dispositivo: {DEVICE_INFO['name']}\")\n",
        "print(f\"   ‚è±Ô∏è  Timesteps a entrenar: {timesteps_to_train:,}\")\n",
        "print(f\"   üéÆ Batch size: {DOUBLEDUNK_GPU_CONFIG['batch_size']}\")\n",
        "\n",
        "# EJECUTAR ENTRENAMIENTO\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üöÄ INICIANDO ENTRENAMIENTO DDQN EN {DEVICE_INFO['name'].upper()}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "training_start = time.time()\n",
        "\n",
        "try:\n",
        "    model.learn(\n",
        "        total_timesteps=timesteps_to_train,\n",
        "        log_interval=5,\n",
        "        callback=callback_list,\n",
        "        progress_bar=True\n",
        "    )\n",
        "    \n",
        "    training_end = time.time()\n",
        "    training_duration = training_end - training_start\n",
        "    \n",
        "    print(f\"\\n‚úÖ ENTRENAMIENTO COMPLETADO\")\n",
        "    print(f\"   ‚è±Ô∏è  Duraci√≥n: {training_duration/3600:.2f} horas\")\n",
        "    print(f\"   üéØ Total episodes: {len(callbacks[0].episode_rewards)}\")\n",
        "    \n",
        "    # Guardar modelo final\n",
        "    final_model_path = \"./DDQN_DoubleDunk_GPU_Final\"\n",
        "    model.save(final_model_path)\n",
        "    print(f\"   üíæ Modelo final guardado: {final_model_path}\")\n",
        "    \n",
        "    # Checkpoint final\n",
        "    checkpoint_manager.save_checkpoint(\n",
        "        model, \n",
        "        DOUBLEDUNK_GPU_CONFIG['total_timesteps'], \n",
        "        callbacks[0].episode_rewards,\n",
        "        {\n",
        "            'training_completed': True,\n",
        "            'total_duration_hours': training_duration/3600,\n",
        "            'final_best_score': callbacks[0].best_score\n",
        "        }\n",
        "    )\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(f\"\\n‚ö†Ô∏è  Entrenamiento interrumpido por usuario\")\n",
        "    print(f\"   üíæ El progreso se ha guardado autom√°ticamente en checkpoints\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error durante entrenamiento: {e}\")\n",
        "    print(f\"   üíæ Revisando √∫ltimo checkpoint disponible...\")\n",
        "\n",
        "print(f\"\\nüéØ Progresando a evaluaci√≥n...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Evaluaci√≥n Completa y Generaci√≥n de Resultados\n",
        "\n",
        "**Evaluaci√≥n acad√©mica con todas las evidencias requeridas**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# EVALUACI√ìN COMPLETA PARA REPORTE ACAD√âMICO\n",
        "# ========================================\n",
        "\n",
        "def comprehensive_evaluation():\n",
        "    \"\"\"Evaluaci√≥n completa del modelo entrenado\"\"\"\n",
        "    \n",
        "    print(\"üìä INICIANDO EVALUACI√ìN COMPLETA\")\n",
        "    eval_start = time.time()\n",
        "    eval_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    \n",
        "    # Crear entorno de evaluaci√≥n\n",
        "    eval_env = make_atari_env(\"ALE/DoubleDunk-v5\", n_envs=1, seed=42)\n",
        "    eval_env = VecFrameStack(eval_env, n_stack=4)\n",
        "    \n",
        "    # Cargar mejor modelo\n",
        "    try:\n",
        "        best_model = OptimizedDoubleDQN.load(\"./checkpoints_doubledunk/best_model\", device=DEVICE)\n",
        "        print(\"‚úÖ Mejor modelo cargado exitosamente\")\n",
        "    except:\n",
        "        try:\n",
        "            best_model = model  # Usar modelo en memoria si falla la carga\n",
        "            print(\"‚ö†Ô∏è  Usando modelo en memoria\")\n",
        "        except:\n",
        "            print(\"‚ùå No se pudo cargar modelo para evaluaci√≥n\")\n",
        "            return None\n",
        "    \n",
        "    # 1. EVALUACI√ìN REQUERIDA (10 episodios)\n",
        "    print(\"üèÜ Evaluaci√≥n oficial (10 episodios)...\")\n",
        "    mean_10, std_10 = evaluate_policy(best_model, eval_env, n_eval_episodes=10, deterministic=False, render=False)\n",
        "    \n",
        "    # 2. EVALUACI√ìN EXTENDIDA (20 episodios)\n",
        "    print(\"üìà Evaluaci√≥n extendida (20 episodios)...\")\n",
        "    mean_20, std_20 = evaluate_policy(best_model, eval_env, n_eval_episodes=20, deterministic=False, render=False)\n",
        "    \n",
        "    eval_end = time.time()\n",
        "    eval_duration = eval_end - eval_start\n",
        "    \n",
        "    # Obtener estad√≠sticas de entrenamiento\n",
        "    try:\n",
        "        episode_rewards = callbacks[0].episode_rewards\n",
        "        total_episodes = len(episode_rewards)\n",
        "        best_score = callbacks[0].best_score\n",
        "        \n",
        "        if total_episodes > 0:\n",
        "            final_100 = episode_rewards[-100:] if total_episodes >= 100 else episode_rewards\n",
        "            mean_last_100 = np.mean(final_100)\n",
        "            best_episode = np.max(episode_rewards)\n",
        "            worst_episode = np.min(episode_rewards)\n",
        "        else:\n",
        "            mean_last_100 = 0\n",
        "            best_episode = 0\n",
        "            worst_episode = 0\n",
        "    except:\n",
        "        episode_rewards = []\n",
        "        total_episodes = 0\n",
        "        best_score = 0\n",
        "        mean_last_100 = 0\n",
        "        best_episode = 0\n",
        "        worst_episode = 0\n",
        "    \n",
        "    # REPORTE OFICIAL\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"üìã REPORTE OFICIAL - DDQN DOUBLEDUNK GPU COLAB\")\n",
        "    \n",
        "    print(f\"üìÖ Fecha evaluaci√≥n: {eval_timestamp}\")\n",
        "    print(f\"‚è±Ô∏è  Tiempo evaluaci√≥n: {eval_duration:.2f}s\")\n",
        "    print(f\"üñ•Ô∏è  Dispositivo: {DEVICE_INFO['name']} ({DEVICE})\")\n",
        "    print(f\"üìä Episodes entrenados: {total_episodes:,}\")\n",
        "    \n",
        "    \n",
        "    print(f\"üéØ RESULTADOS PRINCIPALES:\")\n",
        "    \n",
        "    print(f\"‚îú‚îÄ üèÜ DDQN (10 episodios):    {mean_10:.2f} ¬± {std_10:.2f}\")\n",
        "    print(f\"‚îî‚îÄ üìà DDQN (20 episodios):    {mean_20:.2f} ¬± {std_20:.2f}\")\n",
        "    \n",
        "    improvement = mean_20 - (-14.0)\n",
        "    improvement_pct = (improvement / abs(-14.0)) * 100\n",
        "    \n",
        "    print(f\"üìä AN√ÅLISIS DE MEJORA:\")\n",
        "    print(f\"‚îú‚îÄ üì∂ Mejora absoluta:        {improvement:+.2f} puntos\")\n",
        "    print(f\"‚îú‚îÄ üìà Mejora porcentual:      {improvement_pct:+.1f}%\")\n",
        "    print(f\"‚îî‚îÄ üèÖ Mejor score entrenamiento: {best_score:.2f}\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Guardar resultados estructurados\n",
        "    results = {\n",
        "        'experiment_info': {\n",
        "            'timestamp': eval_timestamp,\n",
        "            'device': str(DEVICE),\n",
        "            'device_name': DEVICE_INFO['name'],\n",
        "            'total_training_episodes': total_episodes\n",
        "        },\n",
        "        'evaluation_results': {\n",
        "            'reinforce_baseline': -14.0,\n",
        "            'ddqn_10_episodes': {'mean': float(mean_10), 'std': float(std_10)},\n",
        "            'ddqn_20_episodes': {'mean': float(mean_20), 'std': float(std_20)},\n",
        "            'improvement_absolute': float(improvement),\n",
        "            'improvement_percentage': float(improvement_pct),\n",
        "            'best_training_score': float(best_score)\n",
        "        },\n",
        "        'training_stats': {\n",
        "            'mean_last_100_episodes': float(mean_last_100),\n",
        "            'best_episode': float(best_episode),\n",
        "            'worst_episode': float(worst_episode)\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Exportar resultados\n",
        "    with open('ddqn_doubledunk_gpu_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    \n",
        "    # CSV para an√°lisis\n",
        "    eval_df = pd.DataFrame({\n",
        "        'Model': ['REINFORCE_baseline', 'DDQN_10eps', 'DDQN_20eps'],\n",
        "        'Mean_Score': [-14.0, mean_10, mean_20],\n",
        "        'Std_Score': [0.0, std_10, std_20],\n",
        "        'Episodes': [10, 10, 20]\n",
        "    })\n",
        "    eval_df.to_csv('ddqn_doubledunk_gpu_evaluation.csv', index=False)\n",
        "    \n",
        "    print(f\"üíæ ARCHIVOS GENERADOS:\")\n",
        "    print(f\"‚îú‚îÄ ddqn_doubledunk_gpu_results.json\")\n",
        "    print(f\"‚îî‚îÄ ddqn_doubledunk_gpu_evaluation.csv\")\n",
        "    \n",
        "    eval_env.close()\n",
        "    return results\n",
        "\n",
        "# Ejecutar evaluaci√≥n\n",
        "evaluation_results = comprehensive_evaluation()\n",
        "\n",
        "# Mostrar progreso de entrenamiento si est√° disponible\n",
        "try:\n",
        "    if len(callbacks[0].episode_rewards) > 10:\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        rewards = callbacks[0].episode_rewards\n",
        "        plt.plot(rewards, alpha=0.6, color='blue')\n",
        "        if len(rewards) > 50:\n",
        "            window = 50\n",
        "            moving_avg = pd.Series(rewards).rolling(window=window).mean()\n",
        "            plt.plot(moving_avg, color='red', linewidth=2, label=f'Media m√≥vil ({window})')\n",
        "        plt.axhline(y=-14.0, color='orange', linestyle='--', label='REINFORCE baseline')\n",
        "        plt.xlabel('Episodios')\n",
        "        plt.ylabel('Recompensa')\n",
        "        plt.title('Evoluci√≥n del Entrenamiento DDQN')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.hist(rewards, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
        "        plt.axvline(x=-14.0, color='orange', linestyle='--', label='REINFORCE baseline')\n",
        "        plt.xlabel('Recompensa')\n",
        "        plt.ylabel('Frecuencia')\n",
        "        plt.title('Distribuci√≥n de Recompensas')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"üìà Gr√°ficas de entrenamiento generadas\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  No se pudieron generar gr√°ficas (entrenamiento muy corto o datos no disponibles)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# FUNCI√ìN DE GENERACI√ìN DE VIDEOS\n",
        "# ========================================\n",
        "\n",
        "import cv2\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import base64\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def generate_video_best_model(\n",
        "    video_filename=\"ddqn_doubledunk_best_model.mp4\",\n",
        "    num_episodes=3,\n",
        "    max_steps_per_episode=2000,\n",
        "    fps=30,\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Genera un video del mejor modelo jugando DoubleDunk episodios completos\n",
        "    \n",
        "    Args:\n",
        "        video_filename: Nombre del archivo de video a generar\n",
        "        num_episodes: N√∫mero de episodios a grabar\n",
        "        max_steps_per_episode: M√°ximo de pasos por episodio\n",
        "        fps: Frames por segundo del video\n",
        "        seed: Semilla para reproducibilidad\n",
        "    \"\"\"\n",
        "    print(f\"üé¨ GENERANDO VIDEO DEL MEJOR MODELO\")\n",
        "    print(f\"=\" * 50)\n",
        "    \n",
        "    # Verificar si existe el mejor modelo\n",
        "    best_model_path = \"./checkpoints_doubledunk/best_model.zip\"\n",
        "    if not os.path.exists(best_model_path):\n",
        "        print(f\"‚ùå No se encontr√≥ el mejor modelo en: {best_model_path}\")\n",
        "        print(\"üí° Aseg√∫rate de que el entrenamiento haya generado un mejor modelo\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Cargar el mejor modelo\n",
        "        print(f\"üìÇ Cargando mejor modelo desde: {best_model_path}\")\n",
        "        model = DQN.load(best_model_path)\n",
        "        print(f\"‚úÖ Modelo cargado exitosamente\")\n",
        "        \n",
        "        # Crear ambiente para grabaci√≥n (sin VecVideoRecorder para mejor control)\n",
        "        print(f\"üéÆ Configurando ambiente DoubleDunk...\")\n",
        "        env = make_atari_env(\"ALE/DoubleDunk-v5\", n_envs=1, seed=seed)\n",
        "        env = VecFrameStack(env, n_stack=4)\n",
        "        \n",
        "        # Configurar grabaci√≥n manual con mejor control\n",
        "        video_folder = \"./videos/\"\n",
        "        os.makedirs(video_folder, exist_ok=True)\n",
        "        \n",
        "        print(f\"üé¨ Iniciando grabaci√≥n:\")\n",
        "        print(f\"   üìπ Episodios: {num_episodes}\")\n",
        "        print(f\"   ‚è±Ô∏è  Max pasos por episodio: {max_steps_per_episode}\")\n",
        "        print(f\"   üéØ Semilla: {seed}\")\n",
        "        print(f\"   üìÅ Carpeta: {video_folder}\")\n",
        "        \n",
        "        # Variables para estad√≠sticas y frames\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "        all_frames = []\n",
        "        total_steps = 0\n",
        "        \n",
        "        # Ejecutar episodios y recopilar frames\n",
        "        for episode in range(num_episodes):\n",
        "            print(f\"   üéÆ Iniciando episodio {episode + 1}/{num_episodes}...\")\n",
        "            \n",
        "            obs = env.reset()\n",
        "            episode_reward = 0\n",
        "            episode_length = 0\n",
        "            episode_frames = []\n",
        "            \n",
        "            for step in range(max_steps_per_episode):\n",
        "                # Obtener frame antes de la acci√≥n\n",
        "                # Para vectorized env, necesitamos obtener el frame del environment interno\n",
        "                try:\n",
        "                    # Intentar obtener frame del environment interno\n",
        "                    if hasattr(env, 'render'):\n",
        "                        frame = env.render()\n",
        "                    else:\n",
        "                        # Para VecEnv, acceder al environment base\n",
        "                        frame = env.envs[0].render()\n",
        "                    \n",
        "                    if frame is not None:\n",
        "                        episode_frames.append(frame)\n",
        "                except:\n",
        "                    # Si no podemos obtener frame, continuamos sin √©l\n",
        "                    pass\n",
        "                \n",
        "                # Predecir acci√≥n usando el modelo\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                \n",
        "                # Ejecutar acci√≥n\n",
        "                obs, reward, done, info = env.step(action)\n",
        "                \n",
        "                episode_reward += reward[0]\n",
        "                episode_length += 1\n",
        "                total_steps += 1\n",
        "                \n",
        "                # Si el episodio termin√≥\n",
        "                if done[0]:\n",
        "                    # Obtener frame final\n",
        "                    try:\n",
        "                        if hasattr(env, 'render'):\n",
        "                            final_frame = env.render()\n",
        "                        else:\n",
        "                            final_frame = env.envs[0].render()\n",
        "                        if final_frame is not None:\n",
        "                            episode_frames.append(final_frame)\n",
        "                    except:\n",
        "                        pass\n",
        "                    break\n",
        "            \n",
        "            # Guardar estad√≠sticas del episodio\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "            all_frames.extend(episode_frames)\n",
        "            \n",
        "            print(f\"   üèÅ Episodio {episode + 1}: Reward={episode_reward:.2f}, Steps={episode_length}, Frames={len(episode_frames)}\")\n",
        "        \n",
        "        # Cerrar ambiente\n",
        "        env.close()\n",
        "        \n",
        "        # Estad√≠sticas finales\n",
        "        if episode_rewards:\n",
        "            mean_reward = np.mean(episode_rewards)\n",
        "            std_reward = np.std(episode_rewards)\n",
        "            mean_length = np.mean(episode_lengths)\n",
        "            \n",
        "            print(f\"\\nüìä ESTAD√çSTICAS DE GRABACI√ìN:\")\n",
        "            print(f\"   üèÜ Recompensa promedio: {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
        "            print(f\"   üìè Duraci√≥n promedio: {mean_length:.1f} pasos\")\n",
        "            print(f\"   üéØ Total de pasos: {total_steps}\")\n",
        "            print(f\"   üé¨ Total de frames: {len(all_frames)}\")\n",
        "            print(f\"   ‚è±Ô∏è  Episodios completados: {num_episodes}\")\n",
        "        \n",
        "        # Generar video si tenemos frames\n",
        "        if all_frames:\n",
        "            final_video_path = os.path.join(video_folder, video_filename)\n",
        "            \n",
        "            print(f\"\\nüé¨ Generando video con {len(all_frames)} frames...\")\n",
        "            \n",
        "            # Usar imageio para crear el video\n",
        "            imageio.mimsave(\n",
        "                final_video_path, \n",
        "                all_frames, \n",
        "                fps=fps,\n",
        "                quality=8,\n",
        "                macro_block_size=1  # Evitar problemas de resoluci√≥n\n",
        "            )\n",
        "            \n",
        "            print(f\"\\n‚úÖ VIDEO GENERADO EXITOSAMENTE:\")\n",
        "            print(f\"   üìÅ Ubicaci√≥n: {final_video_path}\")\n",
        "            print(f\"   üìä Tama√±o: {os.path.getsize(final_video_path) / 1024 / 1024:.1f} MB\")\n",
        "            print(f\"   ‚è±Ô∏è  Duraci√≥n: ~{len(all_frames) / fps:.1f} segundos\")\n",
        "            \n",
        "            return final_video_path\n",
        "        else:\n",
        "            print(f\"‚ùå No se pudieron capturar frames para el video\")\n",
        "            print(f\"üí° Esto puede deberse a problemas de renderizado en el ambiente\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generando video: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def generate_video_robust_method(\n",
        "    video_filename=\"ddqn_doubledunk_best_model.mp4\",\n",
        "    num_episodes=3,\n",
        "    max_steps_per_episode=10000,  # Aumentar l√≠mite para episodios completos\n",
        "    fps=30,\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"\n",
        "    M√©todo robusto que garantiza episodios completos de DoubleDunk\n",
        "    Usa el ambiente exacto del entrenamiento para compatibilidad total\n",
        "    \"\"\"\n",
        "    print(f\"üé¨ GENERANDO VIDEO - M√âTODO ROBUSTO\")\n",
        "    print(f\"=\" * 50)\n",
        "    \n",
        "    # Verificar si existe el mejor modelo\n",
        "    best_model_path = \"./checkpoints_doubledunk/best_model.zip\"\n",
        "    if not os.path.exists(best_model_path):\n",
        "        print(f\"‚ùå No se encontr√≥ el mejor modelo en: {best_model_path}\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Cargar el mejor modelo\n",
        "        print(f\"üìÇ Cargando mejor modelo desde: {best_model_path}\")\n",
        "        model = DQN.load(best_model_path)\n",
        "        print(f\"‚úÖ Modelo cargado exitosamente\")\n",
        "        \n",
        "        # Crear ambiente EXACTAMENTE igual al entrenamiento para compatibilidad total\n",
        "        print(f\"üéÆ Configurando ambiente EXACTO del entrenamiento...\")\n",
        "        \n",
        "        # Usar make_atari_env como en el entrenamiento pero para un solo env\n",
        "        env_single = make_atari_env(\"ALE/DoubleDunk-v5\", n_envs=1, seed=seed)\n",
        "        env_single = VecFrameStack(env_single, n_stack=4)\n",
        "        \n",
        "        # Para la grabaci√≥n, tambi√©n crear un ambiente de renderizado\n",
        "        import gymnasium as gym\n",
        "        render_env = gym.make(\"ALE/DoubleDunk-v5\", render_mode=\"rgb_array\")\n",
        "        \n",
        "        # Aplicar los mismos wrappers que make_atari_env\n",
        "        from stable_baselines3.common.atari_wrappers import (\n",
        "            NoopResetEnv, MaxAndSkipEnv, EpisodicLifeEnv, \n",
        "            FireResetEnv, WarpFrame, ClipRewardEnv\n",
        "        )\n",
        "        \n",
        "        render_env = NoopResetEnv(render_env, noop_max=30)\n",
        "        render_env = MaxAndSkipEnv(render_env, skip=4)\n",
        "        render_env = EpisodicLifeEnv(render_env)\n",
        "        if \"FIRE\" in render_env.unwrapped.get_action_meanings():\n",
        "            render_env = FireResetEnv(render_env)\n",
        "        render_env = WarpFrame(render_env)\n",
        "        render_env = ClipRewardEnv(render_env)\n",
        "        \n",
        "        # Frame stacking manual para sincronizar\n",
        "        from collections import deque\n",
        "        frame_stack = deque(maxlen=4)\n",
        "        \n",
        "        video_folder = \"./videos/\"\n",
        "        os.makedirs(video_folder, exist_ok=True)\n",
        "        \n",
        "        print(f\"üé¨ Iniciando grabaci√≥n con m√©todo robusto:\")\n",
        "        print(f\"   üìπ Episodios: {num_episodes}\")\n",
        "        print(f\"   ‚è±Ô∏è  Max pasos por episodio: {max_steps_per_episode}\")\n",
        "        print(f\"   üéØ Semilla: {seed}\")\n",
        "        print(f\"   üîÑ Usando ambiente exacto del entrenamiento\")\n",
        "        \n",
        "        # Variables para estad√≠sticas y frames\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "        all_frames = []\n",
        "        \n",
        "        # Ejecutar episodios con doble ambiente (predicci√≥n + renderizado)\n",
        "        for episode in range(num_episodes):\n",
        "            print(f\"   üéÆ Iniciando episodio {episode + 1}/{num_episodes}...\")\n",
        "            \n",
        "            # Reset ambos ambientes con la misma semilla\n",
        "            obs_model = env_single.reset()  # Para el modelo DDQN\n",
        "            obs_render, info_render = render_env.reset(seed=seed + episode)  # Para renderizado\n",
        "            \n",
        "            # Inicializar frame stack para el ambiente de renderizado\n",
        "            frame_stack.clear()\n",
        "            for _ in range(4):\n",
        "                frame_stack.append(obs_render)\n",
        "            \n",
        "            episode_reward = 0\n",
        "            episode_length = 0\n",
        "            episode_frames = []\n",
        "            \n",
        "            # Variables para sincronizaci√≥n\n",
        "            render_done = False\n",
        "            model_done = False\n",
        "            \n",
        "            for step in range(max_steps_per_episode):\n",
        "                # Obtener frame para video desde ambiente de renderizado\n",
        "                if not render_done:\n",
        "                    frame = render_env.render()\n",
        "                    if frame is not None:\n",
        "                        episode_frames.append(frame)\n",
        "                \n",
        "                # Predecir acci√≥n usando el ambiente del modelo (exacto al entrenamiento)\n",
        "                if not model_done:\n",
        "                    action, _ = model.predict(obs_model, deterministic=True)\n",
        "                    \n",
        "                    # Ejecutar en ambiente del modelo\n",
        "                    obs_model, reward_model, done_model, info_model = env_single.step(action)\n",
        "                    model_done = done_model[0]\n",
        "                    episode_reward += reward_model[0]\n",
        "                    \n",
        "                    # Ejecutar la misma acci√≥n en ambiente de renderizado (sincronizado)\n",
        "                    if not render_done:\n",
        "                        obs_render, reward_render, terminated, truncated, info_render = render_env.step(action[0])\n",
        "                        render_done = terminated or truncated\n",
        "                        frame_stack.append(obs_render)\n",
        "                \n",
        "                episode_length += 1\n",
        "                \n",
        "                # Terminar cuando cualquiera de los dos termine\n",
        "                if model_done or render_done:\n",
        "                    print(f\"     üí° Episodio termin√≥: Model_done={model_done}, Render_done={render_done}\")\n",
        "                    # Obtener frame final si es posible\n",
        "                    if not render_done:\n",
        "                        try:\n",
        "                            final_frame = render_env.render()\n",
        "                            if final_frame is not None:\n",
        "                                episode_frames.append(final_frame)\n",
        "                        except:\n",
        "                            pass\n",
        "                    break\n",
        "            \n",
        "            # Guardar estad√≠sticas\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "            all_frames.extend(episode_frames)\n",
        "            \n",
        "            print(f\"   üèÅ Episodio {episode + 1}: Reward={episode_reward:.2f}, Steps={episode_length}, Frames={len(episode_frames)}\")\n",
        "            \n",
        "            # Verificar si el episodio fue muy corto (posible problema)\n",
        "            if episode_length < 10:\n",
        "                print(f\"     ‚ö†Ô∏è  Episodio muy corto - posible problema de sincronizaci√≥n\")\n",
        "        \n",
        "        # Cerrar ambientes\n",
        "        env_single.close()\n",
        "        render_env.close()\n",
        "        \n",
        "        # Estad√≠sticas finales\n",
        "        if episode_rewards:\n",
        "            mean_reward = np.mean(episode_rewards)\n",
        "            std_reward = np.std(episode_rewards)\n",
        "            mean_length = np.mean(episode_lengths)\n",
        "            \n",
        "            print(f\"\\nüìä ESTAD√çSTICAS DE GRABACI√ìN:\")\n",
        "            print(f\"   üèÜ Recompensa promedio: {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
        "            print(f\"   üìè Duraci√≥n promedio: {mean_length:.1f} pasos\")\n",
        "            print(f\"   üé¨ Total de frames: {len(all_frames)}\")\n",
        "            print(f\"   ‚è±Ô∏è  Episodios completados: {num_episodes}\")\n",
        "        \n",
        "        # Generar video\n",
        "        if all_frames:\n",
        "            final_video_path = os.path.join(video_folder, video_filename)\n",
        "            \n",
        "            print(f\"\\nüé¨ Generando video con {len(all_frames)} frames...\")\n",
        "            \n",
        "            imageio.mimsave(\n",
        "                final_video_path, \n",
        "                all_frames, \n",
        "                fps=fps,\n",
        "                quality=8,\n",
        "                macro_block_size=1\n",
        "            )\n",
        "            \n",
        "            print(f\"\\n‚úÖ VIDEO GENERADO EXITOSAMENTE:\")\n",
        "            print(f\"   üìÅ Ubicaci√≥n: {final_video_path}\")\n",
        "            print(f\"   üìä Tama√±o: {os.path.getsize(final_video_path) / 1024 / 1024:.1f} MB\")\n",
        "            print(f\"   ‚è±Ô∏è  Duraci√≥n: ~{len(all_frames) / fps:.1f} segundos\")\n",
        "            \n",
        "            return final_video_path\n",
        "        else:\n",
        "            print(f\"‚ùå No se pudieron capturar frames\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en m√©todo alternativo: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def diagnose_video_issue(video_path):\n",
        "    \"\"\"\n",
        "    Diagnostica problemas comunes en la generaci√≥n de videos\n",
        "    \"\"\"\n",
        "    print(f\"üîç DIAGN√ìSTICO DEL VIDEO\")\n",
        "    print(f\"=\" * 30)\n",
        "    \n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"‚ùå El video no existe: {video_path}\")\n",
        "        return\n",
        "    \n",
        "    # Informaci√≥n b√°sica del archivo\n",
        "    file_size = os.path.getsize(video_path)\n",
        "    print(f\"üìÅ Archivo: {video_path}\")\n",
        "    print(f\"üìä Tama√±o: {file_size / 1024:.1f} KB\")\n",
        "    \n",
        "    # Intentar leer con imageio\n",
        "    try:\n",
        "        reader = imageio.get_reader(video_path)\n",
        "        frame_count = reader.count_frames()\n",
        "        meta = reader.get_meta_data()\n",
        "        fps = meta.get('fps', 30)\n",
        "        duration = frame_count / fps if fps > 0 else 0\n",
        "        \n",
        "        print(f\"üé¨ Frames: {frame_count}\")\n",
        "        print(f\"‚è±Ô∏è  FPS: {fps}\")\n",
        "        print(f\"‚è∞ Duraci√≥n: {duration:.2f} segundos\")\n",
        "        \n",
        "        # Leer algunos frames para diagn√≥stico\n",
        "        sample_frames = min(5, frame_count)\n",
        "        print(f\"üñºÔ∏è  Muestreando {sample_frames} frames...\")\n",
        "        \n",
        "        for i in range(sample_frames):\n",
        "            try:\n",
        "                frame = reader.get_data(i)\n",
        "                print(f\"   Frame {i}: {frame.shape} - {frame.dtype}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   Frame {i}: Error - {e}\")\n",
        "        \n",
        "        reader.close()\n",
        "        \n",
        "        # Diagn√≥stico de problemas comunes\n",
        "        if duration < 5:\n",
        "            print(f\"‚ö†Ô∏è  VIDEO MUY CORTO - Posibles causas:\")\n",
        "            print(f\"   ‚Ä¢ Episodios terminan muy r√°pido\")\n",
        "            print(f\"   ‚Ä¢ Modelo no est√° funcionando correctamente\")\n",
        "            print(f\"   ‚Ä¢ Problema de ambiente/wrappers\")\n",
        "        \n",
        "        if frame_count < 100:\n",
        "            print(f\"‚ö†Ô∏è  POCOS FRAMES - Posibles causas:\")\n",
        "            print(f\"   ‚Ä¢ Captura de frames fallando\")\n",
        "            print(f\"   ‚Ä¢ Renderizado no funciona\")\n",
        "            print(f\"   ‚Ä¢ Sincronizaci√≥n de ambientes\")\n",
        "        \n",
        "        print(f\"‚úÖ Diagn√≥stico completado\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error leyendo video: {e}\")\n",
        "        print(f\"üí° El archivo puede estar corrupto o en formato incorrecto\")\n",
        "\n",
        "def display_video_in_notebook(video_path):\n",
        "    \"\"\"\n",
        "    Muestra el video directamente en el notebook de Jupyter/Colab\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"‚ùå Video no encontrado: {video_path}\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        # Leer y encodear video en base64\n",
        "        with open(video_path, \"rb\") as f:\n",
        "            video_data = f.read()\n",
        "        \n",
        "        video_base64 = base64.b64encode(video_data).decode()\n",
        "        \n",
        "        # Crear HTML para mostrar video\n",
        "        video_html = f\"\"\"\n",
        "        <div style=\"text-align: center; margin: 20px;\">\n",
        "            <h3>üé¨ DDQN DoubleDunk - Mejor Modelo</h3>\n",
        "            <video width=\"640\" height=\"480\" controls style=\"border: 2px solid #4CAF50; border-radius: 10px;\">\n",
        "                <source src=\"data:video/mp4;base64,{video_base64}\" type=\"video/mp4\">\n",
        "                Tu navegador no soporta videos HTML5.\n",
        "            </video>\n",
        "            <p style=\"margin-top: 10px; color: #666;\">\n",
        "                üìÅ Archivo: {os.path.basename(video_path)} | \n",
        "                üìä Tama√±o: {os.path.getsize(video_path) / 1024 / 1024:.1f} MB\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        \n",
        "        display(HTML(video_html))\n",
        "        print(f\"‚úÖ Video mostrado en el notebook\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error mostrando video: {e}\")\n",
        "        print(f\"üí° Puedes descargar el video directamente desde: {video_path}\")\n",
        "\n",
        "# Informaci√≥n sobre la generaci√≥n de videos\n",
        "print(\"üé¨ SISTEMA DE GENERACI√ìN DE VIDEOS MEJORADO\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Funciones disponibles:\")\n",
        "print(\"   generate_video_robust_method()       # M√©todo principal (ambientes sincronizados)\")\n",
        "print(\"   generate_video_best_model()         # M√©todo de respaldo\")\n",
        "print(\"   display_video_in_notebook()         # Mostrar video en notebook\")\n",
        "print()\n",
        "print(\"‚ú® MEJORAS IMPLEMENTADAS:\")\n",
        "print(\"   üéØ Episodios completos - No se corta en medio del juego\")\n",
        "print(\"   üîÑ Ambientes sincronizados - Modelo + Renderizado en paralelo\")\n",
        "print(\"   üìπ Captura frame por frame - M√°s confiable\")\n",
        "print(\"   üéÆ Ambiente exacto - Misma configuraci√≥n del entrenamiento\")\n",
        "print(\"   ‚è±Ô∏è  Duraci√≥n precisa - Control total del video\")\n",
        "print(\"   üõ°Ô∏è  Detecci√≥n de problemas - Diagn√≥stico autom√°tico\")\n",
        "print()\n",
        "print(\"üí° El video se genera autom√°ticamente usando el mejor modelo guardado\")\n",
        "print(\"   durante el entrenamiento (best_model.zip)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# EJECUTAR GENERACI√ìN DE VIDEO\n",
        "# ========================================\n",
        "\n",
        "# Configuraci√≥n de video\n",
        "VIDEO_CONFIG = {\n",
        "    'video_filename': 'ddqn_doubledunk_best_model.mp4',\n",
        "    'num_episodes': 10,           # N√∫mero de episodios a grabar\n",
        "    'max_steps_per_episode': 2000,  # Pasos m√°ximos por episodio\n",
        "    'seed': 42                   # Semilla para reproducibilidad\n",
        "}\n",
        "\n",
        "print(\"üé¨ CONFIGURACI√ìN DE VIDEO:\")\n",
        "print(\"=\" * 30)\n",
        "for key, value in VIDEO_CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "print()\n",
        "\n",
        "# Generar video del mejor modelo - Intentar m√©todo alternativo primero\n",
        "print(\"üöÄ Iniciando generaci√≥n de video...\")\n",
        "print(\"üí° Probando m√©todo alternativo (mejor control de episodios completos)...\")\n",
        "\n",
        "video_path = generate_video_robust_method(**VIDEO_CONFIG)\n",
        "\n",
        "# Si el m√©todo alternativo falla, intentar m√©todo original\n",
        "if video_path is None:\n",
        "    print(\"\\nüîÑ M√©todo alternativo fall√≥, probando m√©todo original...\")\n",
        "    video_path = generate_video_best_model(**VIDEO_CONFIG)\n",
        "\n",
        "if video_path:\n",
        "    print(f\"\\nüéâ ¬°Video generado exitosamente!\")\n",
        "    print(f\"üìÅ Ubicaci√≥n: {video_path}\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Informaci√≥n adicional para el reporte acad√©mico\n",
        "    print(f\"\\nüìã INFORMACI√ìN PARA EL REPORTE:\")\n",
        "    print(f\"‚úÖ Video evidencia disponible en: {video_path}\")\n",
        "    print(f\"üéØ Agente entrenado con DDQN + Epsilon Scheduler\")\n",
        "    print(f\"üèÜ Modelo usado: Mejor modelo durante entrenamiento\")\n",
        "    print(f\"üéÆ Ambiente: ALE/DoubleDunk-v5\")\n",
        "    print(f\"üé≤ Semilla: {VIDEO_CONFIG['seed']} (reproducible)\")\n",
        "    print(f\"üìä Episodios grabados: {VIDEO_CONFIG['num_episodes']}\")\n",
        "    \n",
        "else:\n",
        "    print(f\"\\n‚ùå No se pudo generar el video\")\n",
        "    print(f\"üí° Posibles causas:\")\n",
        "    print(f\"   - El modelo no existe (entrenamiento no completado)\")\n",
        "    print(f\"   - Error de ambiente o dependencias\")\n",
        "    print(f\"   - Falta de espacio en disco\")\n",
        "    print(f\"\\nüîß Soluciones:\")\n",
        "    print(f\"   1. Verificar que existe: ./checkpoints_doubledunk/best_model.zip\")\n",
        "    print(f\"   2. Ejecutar el entrenamiento antes de generar video\")\n",
        "    print(f\"   3. Revisar los logs de error arriba\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
