{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9091ae",
   "metadata": {},
   "source": [
    "# BeamRider con REINFORCE (GPU: CUDA o MPS)\n",
    "\n",
    "Entrenamos un agente de Aprendizaje por Refuerzo con el algoritmo REINFORCE (con baseline) para el entorno Atari `ALE/BeamRider-v5` usando Gymnasium + ALE.\n",
    "\n",
    "- Algoritmo: REINFORCE con baseline (actor-crítico simple), entropía y clipping de gradiente.\n",
    "- Observaciones: preprocesamiento Atari (84x84, escala de grises) + apilado de 4 frames.\n",
    "- Acción: espacio `Discrete(9)` propio de BeamRider.\n",
    "- Hardware: GPU recomendado (CUDA en NVIDIA o MPS en Apple Silicon), entrenamiento prolongado; se guardan checkpoints para reanudar.\n",
    "\n",
    "Referencia del entorno: [Documentación Farama ALE - BeamRider](https://ale.farama.org/environments/beam_rider/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5421a708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': '3.13.5', 'platform': 'macOS-15.5-arm64-arm-64bit-Mach-O'}\n"
     ]
    }
   ],
   "source": [
    "# Instalación en Colab (descomentar si es necesario)\n",
    "#!pip install -q gymnasium[atari]==0.29.1 ale-py==0.10.1 torch==2.2.2 torchvision==0.17.2\n",
    "#!pip install -q tqdm==4.66.4 numpy==1.26.4 matplotlib==3.8.4 imageio==2.34.1 imageio-ffmpeg==0.4.9\n",
    "\n",
    "import sys, platform\n",
    "print({'python': sys.version.split()[0], 'platform': platform.platform()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d41da415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import warnings\n",
    "import imageio\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Registrar entornos ALE como en la plantilla\n",
    "gymnasium.register_envs(ale_py)\n",
    "\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Preferir CUDA, luego MPS (Apple Silicon), sino CPU\n",
    "device = (\n",
    "    torch.device('cuda') if torch.cuda.is_available() else\n",
    "    (torch.device('mps') if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else torch.device('cpu'))\n",
    ")\n",
    "print('Using device:', device)\n",
    "\n",
    "# Asegurar determinismo parcial\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "452cd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabación de video opcional de un episodio\n",
    "\n",
    "def record_video(model: nn.Module, filename: str = 'videos/beamrider_reinforce.mp4', fps: int = 30, seed: int = SEED+2024):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    env = make_env(seed=seed, render_mode='rgb_array')\n",
    "    frames = []\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    done = False\n",
    "    with torch.no_grad():\n",
    "        for t in range(cfg.max_steps_per_episode):\n",
    "            x = obs_to_tensor(obs)\n",
    "            logits, _ = model(x)\n",
    "            action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "            obs, r, terminated, truncated, info = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                frame = env.render()\n",
    "                frames.append(frame)\n",
    "                break\n",
    "    env.close()\n",
    "    imageio.mimwrite(filename, frames, fps=fps, quality=8)\n",
    "    print('Video guardado en:', filename)\n",
    "\n",
    "# Descomenta para grabar video de un episodio\n",
    "# record_video(agent, filename='videos/beamrider_reinforce.mp4', fps=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8b52285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrameStack mínimo compatible (sin dependencias externas)\n",
    "class SimpleFrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k: int = 4):\n",
    "        super().__init__(env)\n",
    "        self.k = k\n",
    "        self.frames = deque(maxlen=k)\n",
    "        obs_space = env.observation_space\n",
    "        assert len(obs_space.shape) == 2 or len(obs_space.shape) == 3\n",
    "        h, w = obs_space.shape[0], obs_space.shape[1]\n",
    "        # Observación resultante: (H, W, K) en channel-last (para mantener compatibilidad)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(h, w, k), dtype=np.uint8)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.frames.clear()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_ob(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_ob(), reward, terminated, truncated, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        return np.stack(list(self.frames), axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d20fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear entorno Atari con preprocesamiento y frame stacking (sin AtariPreprocessing)\n",
    "\n",
    "\n",
    "def make_env(seed: int = SEED, render_mode=None):\n",
    "    env = gym.make('ALE/BeamRider-v5', render_mode=render_mode)\n",
    "    # Convertir a escala de grises y 84x84 con OpenCV para compatibilidad amplia\n",
    "    class GrayResizeWrapper(gym.ObservationWrapper):\n",
    "        def __init__(self, env):\n",
    "            super().__init__(env)\n",
    "            h = 84\n",
    "            w = 84\n",
    "            self.observation_space = gym.spaces.Box(low=0, high=255, shape=(h, w), dtype=np.uint8)\n",
    "        def observation(self, obs):\n",
    "            # obs: uint8 (H, W, 3) RGB\n",
    "            gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "            resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "            return resized\n",
    "    env = GrayResizeWrapper(env)\n",
    "    env = SimpleFrameStack(env, 4)\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    return env\n",
    "\n",
    "# Conversión de observación a tensor float32 CHW en [0,1]\n",
    "def obs_to_tensor(obs) -> torch.Tensor:\n",
    "    # obs viene como (84,84,4), uint8 (channel-last). Convertimos a (4,84,84)\n",
    "    if isinstance(obs, np.ndarray):\n",
    "        arr = obs\n",
    "    else:\n",
    "        arr = np.array(obs)  # LazyFrames\n",
    "    if arr.ndim == 3 and arr.shape[-1] == 4:\n",
    "        arr = np.transpose(arr, (2, 0, 1))\n",
    "    elif arr.ndim == 2:\n",
    "        arr = np.stack([arr]*4, axis=0)\n",
    "    tensor = torch.from_numpy(arr).float() / 255.0\n",
    "    return tensor.unsqueeze(0).to(device)  # batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82384a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabación de video opcional de un episodio\n",
    "\n",
    "def record_video(model: nn.Module, filename: str = 'videos/beamrider_reinforce.mp4', fps: int = 30, seed: int = SEED+2024):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    env = make_env(seed=seed, render_mode='rgb_array')\n",
    "    frames = []\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    done = False\n",
    "    with torch.no_grad():\n",
    "        for t in range(cfg.max_steps_per_episode):\n",
    "            x = obs_to_tensor(obs)\n",
    "            logits, _ = model(x)\n",
    "            action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "            obs, r, terminated, truncated, info = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                frame = env.render()\n",
    "                frames.append(frame)\n",
    "                break\n",
    "    env.close()\n",
    "    imageio.mimwrite(filename, frames, fps=fps, quality=8)\n",
    "    print('Video guardado en:', filename)\n",
    "\n",
    "# Descomenta para grabar video de un episodio\n",
    "# record_video(agent, filename='videos/beamrider_reinforce.mp4', fps=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "607f1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red Actor-Crítico CNN para Atari (REINFORCE con baseline)\n",
    "class AtariActorCritic(nn.Module):\n",
    "    def __init__(self, in_channels: int, n_actions: int):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512), nn.ReLU(),\n",
    "        )\n",
    "        self.policy_head = nn.Linear(512, n_actions)\n",
    "        self.value_head = nn.Linear(512, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = self.features(x)\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z)\n",
    "        logits = self.policy_head(z)\n",
    "        value = self.value_head(z)\n",
    "        return logits, value.squeeze(-1)\n",
    "\n",
    "    def act(self, x: torch.Tensor):\n",
    "        logits, value = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action.item(), log_prob, entropy, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3acef704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(total_episodes=3000, max_steps_per_episode=10000, gamma=0.99, learning_rate=0.00025, entropy_coef=0.01, value_coef=0.5, grad_clip_norm=0.5, checkpoint_dir='checkpoints_beamrider', checkpoint_every_episodes=50, eval_every_episodes=100, eval_episodes=10)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    total_episodes: int = 3000\n",
    "    max_steps_per_episode: int = 10000\n",
    "    gamma: float = 0.99\n",
    "    learning_rate: float = 2.5e-4\n",
    "    entropy_coef: float = 0.01\n",
    "    value_coef: float = 0.5\n",
    "    grad_clip_norm: float = 0.5\n",
    "    checkpoint_dir: str = 'checkpoints_beamrider'\n",
    "    checkpoint_every_episodes: int = 50\n",
    "    eval_every_episodes: int = 100\n",
    "    eval_episodes: int = 10\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79078365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidades: retorno descontado, guardar/cargar checkpoints, evaluación\n",
    "\n",
    "def compute_returns(rewards: List[float], gamma: float) -> torch.Tensor:\n",
    "    G = 0.0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.append(G)\n",
    "    returns.reverse()\n",
    "    return torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "def save_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer, episode: int, path: str):\n",
    "    torch.save({'episode': episode,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()}, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer, path: str):\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    return ckpt.get('episode', 0)\n",
    "\n",
    "\n",
    "def evaluate(agent: nn.Module, episodes: int = 10, render: bool = False) -> float:\n",
    "    env = make_env(seed=SEED + 999, render_mode='human' if render else None)\n",
    "    agent.eval()\n",
    "    rewards = []\n",
    "    with torch.no_grad():\n",
    "        for ep in range(episodes):\n",
    "            obs, info = env.reset(seed=SEED + 999 + ep)\n",
    "            done = False\n",
    "            total_r = 0.0\n",
    "            for t in range(cfg.max_steps_per_episode):\n",
    "                x = obs_to_tensor(obs)\n",
    "                logits, _ = agent(x)\n",
    "                action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
    "                obs, r, terminated, truncated, info = env.step(action)\n",
    "                total_r += float(r)\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            rewards.append(total_r)\n",
    "    env.close()\n",
    "    agent.train()\n",
    "    return float(np.mean(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74a9fbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando checkpoint desde checkpoints_beamrider/reinforce_beamrider.pt\n",
      "Ep 100 | Reward entrenamiento: 352.0 | Eval-10 media: 96.8\n",
      "Ep 200 | Reward entrenamiento: 220.0 | Eval-10 media: 96.8\n",
      "Ep 300 | Reward entrenamiento: 440.0 | Eval-10 media: 4.4\n",
      "Ep 400 | Reward entrenamiento: 804.0 | Eval-10 media: 684.4\n",
      "Ep 500 | Reward entrenamiento: 264.0 | Eval-10 media: 560.4\n",
      "Ep 600 | Reward entrenamiento: 440.0 | Eval-10 media: 70.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m loss = policy_loss + cfg.value_coef * value_loss - cfg.entropy_coef * entropy_bonus\n\u001b[32m     50\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m nn.utils.clip_grad_norm_(agent.parameters(), cfg.grad_clip_norm)\n\u001b[32m     53\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversidaddelosAndes/MAIA/aprendizaje_refuerzo_profundo/reto_1/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Entrenamiento REINFORCE con baseline\n",
    "\n",
    "env = make_env(seed=SEED, render_mode=None)\n",
    "n_actions = env.action_space.n\n",
    "in_channels = 4\n",
    "\n",
    "agent = AtariActorCritic(in_channels=in_channels, n_actions=n_actions).to(device)\n",
    "optimizer = Adam(agent.parameters(), lr=cfg.learning_rate)\n",
    "\n",
    "start_episode = 0\n",
    "ckpt_path = os.path.join(cfg.checkpoint_dir, 'reinforce_beamrider.pt')\n",
    "if os.path.exists(ckpt_path):\n",
    "    print('Cargando checkpoint desde', ckpt_path)\n",
    "    start_episode = load_checkpoint(agent, optimizer, ckpt_path)\n",
    "\n",
    "best_eval = -float('inf')\n",
    "\n",
    "for ep in range(start_episode, cfg.total_episodes):\n",
    "    obs, info = env.reset(seed=SEED + ep)\n",
    "    log_probs, entropies, values, rewards = [], [], [], []\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for t in range(cfg.max_steps_per_episode):\n",
    "        x = obs_to_tensor(obs)\n",
    "        action, log_prob, entropy, value = agent.act(x)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        log_probs.append(log_prob)\n",
    "        entropies.append(entropy)\n",
    "        values.append(value)\n",
    "        rewards.append(float(reward))\n",
    "        total_reward += float(reward)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # REINFORCE con baseline: advantage = G_t - V(s_t)\n",
    "    returns = compute_returns(rewards, cfg.gamma)\n",
    "    values_t = torch.stack(values)\n",
    "    log_probs_t = torch.stack(log_probs)\n",
    "    entropies_t = torch.stack(entropies)\n",
    "\n",
    "    advantages = returns - values_t\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    policy_loss = -(log_probs_t * advantages.detach()).mean()\n",
    "    value_loss = F.mse_loss(values_t, returns)\n",
    "    entropy_bonus = entropies_t.mean()\n",
    "\n",
    "    loss = policy_loss + cfg.value_coef * value_loss - cfg.entropy_coef * entropy_bonus\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(agent.parameters(), cfg.grad_clip_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    if (ep + 1) % cfg.checkpoint_every_episodes == 0:\n",
    "        save_checkpoint(agent, optimizer, ep + 1, ckpt_path)\n",
    "\n",
    "    if (ep + 1) % cfg.eval_every_episodes == 0:\n",
    "        avg_eval = evaluate(agent, episodes=cfg.eval_episodes, render=False)\n",
    "        print(f'Ep {ep+1} | Reward entrenamiento: {total_reward:.1f} | Eval-10 media: {avg_eval:.1f}')\n",
    "        if avg_eval > best_eval:\n",
    "            best_eval = avg_eval\n",
    "            torch.save({'model': agent.state_dict(), 'avg_eval': best_eval}, os.path.join(cfg.checkpoint_dir, 'best.pt'))\n",
    "\n",
    "env.close()\n",
    "print('Entrenamiento finalizado.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6239620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video guardado en: videos/beamrider_reinforce.mp4\n"
     ]
    }
   ],
   "source": [
    "# Grabación de video opcional de un episodio\n",
    "\n",
    "def record_video(model: nn.Module, filename: str = 'videos/beamrider_reinforce.mp4', fps: int = 30, seed: int = SEED+2024):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    env = make_env(seed=seed, render_mode='rgb_array')\n",
    "    frames = []\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    done = False\n",
    "    with torch.no_grad():\n",
    "        for t in range(cfg.max_steps_per_episode):\n",
    "            x = obs_to_tensor(obs)\n",
    "            logits, _ = model(x)\n",
    "            action = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "            obs, r, terminated, truncated, info = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                frame = env.render()\n",
    "                frames.append(frame)\n",
    "                break\n",
    "    env.close()\n",
    "    imageio.mimwrite(filename, frames, fps=fps, quality=8)\n",
    "    print('Video guardado en:', filename)\n",
    "\n",
    "# Descomenta para grabar video de un episodio\n",
    "record_video(agent, filename='videos/beamrider_reinforce.mp4', fps=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28e430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando mejor modelo desde checkpoints_beamrider/best.pt\n"
     ]
    }
   ],
   "source": [
    "# Cargar mejor modelo (si existe) y evaluación en 10 episodios\n",
    "best_path = os.path.join(cfg.checkpoint_dir, 'best.pt')\n",
    "if os.path.exists(best_path):\n",
    "    print('Cargando mejor modelo desde', best_path)\n",
    "    ckpt = torch.load(best_path, map_location=device)\n",
    "    agent.load_state_dict(ckpt['model'])\n",
    "\n",
    "avg10 = evaluate(agent, episodes=10, render=False)\n",
    "print(f'Recompensa promedio en 10 episodios: {avg10:.2f}')\n",
    "\n",
    "# Grabación adicional con best.pt si existe\n",
    "if os.path.exists(best_path):\n",
    "    # agente temporal para evitar sobre-escribir 'agent' si estás entrenando\n",
    "    env_tmp = make_env(seed=SEED+3030)\n",
    "    n_actions_best = env_tmp.action_space.n\n",
    "    env_tmp.close()\n",
    "    best_agent = AtariActorCritic(in_channels=4, n_actions=n_actions_best).to(device)\n",
    "    ckpt_best = torch.load(best_path, map_location=device)\n",
    "    best_agent.load_state_dict(ckpt_best['model'])\n",
    "    os.makedirs('videos', exist_ok=True)\n",
    "    record_video(best_agent, filename='videos/beamrider_best.mp4', fps=30, seed=SEED+3030)\n",
    "else:\n",
    "    print('No se encontró best.pt; aún no hay mejor modelo guardado')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
